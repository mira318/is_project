{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-04-03T01:16:39.252599Z",
     "iopub.status.busy": "2023-04-03T01:16:39.251774Z",
     "iopub.status.idle": "2023-04-03T01:16:40.723914Z",
     "shell.execute_reply": "2023-04-03T01:16:40.722702Z",
     "shell.execute_reply.started": "2023-04-03T01:16:39.252564Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'summarus'...\n",
      "remote: Enumerating objects: 1484, done.\u001b[K\n",
      "remote: Counting objects: 100% (118/118), done.\u001b[K\n",
      "remote: Compressing objects: 100% (39/39), done.\u001b[K\n",
      "remote: Total 1484 (delta 99), reused 96 (delta 79), pack-reused 1366\u001b[K\n",
      "Receiving objects: 100% (1484/1484), 504.99 KiB | 3.16 MiB/s, done.\n",
      "Resolving deltas: 100% (988/988), done.\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/IlyaGusev/summarus.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-03T01:16:40.726984Z",
     "iopub.status.busy": "2023-04-03T01:16:40.726564Z",
     "iopub.status.idle": "2023-04-03T01:16:42.264341Z",
     "shell.execute_reply": "2023-04-03T01:16:42.262931Z",
     "shell.execute_reply.started": "2023-04-03T01:16:40.726942Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'ru_articles_summarization'...\n",
      "remote: Enumerating objects: 10, done.\u001b[K\n",
      "remote: Counting objects: 100% (10/10), done.\u001b[K\n",
      "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
      "remote: Total 10 (delta 1), reused 0 (delta 0), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (10/10), 23.09 KiB | 482.00 KiB/s, done.\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/wdariaw/ru_articles_summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-03T01:16:42.267103Z",
     "iopub.status.busy": "2023-04-03T01:16:42.266712Z",
     "iopub.status.idle": "2023-04-03T01:16:42.282111Z",
     "shell.execute_reply": "2023-04-03T01:16:42.280881Z",
     "shell.execute_reply.started": "2023-04-03T01:16:42.267058Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ichuviliaeva/is_project/ru_articles_summarization\n"
     ]
    }
   ],
   "source": [
    "%cd ru_articles_summarization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-03T01:16:42.288095Z",
     "iopub.status.busy": "2023-04-03T01:16:42.286708Z",
     "iopub.status.idle": "2023-04-03T01:16:43.656102Z",
     "shell.execute_reply": "2023-04-03T01:16:43.654502Z",
     "shell.execute_reply.started": "2023-04-03T01:16:42.288054Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate.py  lid.176.ftz.1  README.md         ru_articles_summarization.ipynb\r\n",
      "lid.176.ftz  \u001b[0m\u001b[01;34m__pycache__\u001b[0m/   requirements.txt\r\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-03T01:20:08.735843Z",
     "iopub.status.busy": "2023-04-03T01:20:08.735404Z",
     "iopub.status.idle": "2023-04-03T01:20:11.708834Z",
     "shell.execute_reply": "2023-04-03T01:20:11.707761Z",
     "shell.execute_reply.started": "2023-04-03T01:20:08.735777Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from datasets import Dataset, DatasetDict\n",
    "import razdel\n",
    "import fasttext\n",
    "\n",
    "from evaluate import print_metrics, postprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-03T01:20:11.711007Z",
     "iopub.status.busy": "2023-04-03T01:20:11.710336Z",
     "iopub.status.idle": "2023-04-03T01:20:12.695490Z",
     "shell.execute_reply": "2023-04-03T01:20:12.694032Z",
     "shell.execute_reply.started": "2023-04-03T01:20:11.710975Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "articles_part_0.csv  articles_part_5.csv  predictions.txt  val_data.json\r\n",
      "articles_part_1.csv  articles_part_6.csv  query_list.txt   visited_set.txt\r\n",
      "articles_part_2.csv  articles_part_7.csv  targets.txt      vocab.bpe\r\n",
      "articles_part_3.csv  articles_part_8.csv  test_data.json\r\n",
      "articles_part_4.csv  encoder.json         train_data.json\r\n"
     ]
    }
   ],
   "source": [
    "%ls /DATA/ichuviliaeva/project_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-03T01:20:12.698880Z",
     "iopub.status.busy": "2023-04-03T01:20:12.698414Z",
     "iopub.status.idle": "2023-04-03T01:24:39.604254Z",
     "shell.execute_reply": "2023-04-03T01:24:39.603015Z",
     "shell.execute_reply.started": "2023-04-03T01:20:12.698803Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-04-17 09:14:31--  https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.ftz\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 108.156.22.45, 108.156.22.14, 108.156.22.118, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|108.156.22.45|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 938013 (916K) [binary/octet-stream]\n",
      "Saving to: ‘lid.176.ftz’\n",
      "\n",
      "lid.176.ftz         100%[===================>] 916.03K  --.-KB/s    in 0.1s    \n",
      "\n",
      "2023-04-17 09:14:32 (6.03 MB/s) - ‘lid.176.ftz’ saved [938013/938013]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91fa2536858c411788fa6d8cf29fa04c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc21b867c0bc476cbee96139c3a98c20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d25affaa1264123a5ffaf9084375d1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "444428be801346dfa2884c3fb2247876",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7effcac21f4c4f389265d9628395164c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d8121787e78494193f54d83a882f1b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a256108ef2b8439190e7ef98505907d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f02f582330f405fb61a95d27b9a2ff2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a46be6038e740b99393d3bc49b72e80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Состав и калорийность\\nПлоды айвы достаточно н...</td>\n",
       "      <td>Айву, которую часто называют ложным яблоком вс...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>В последние годы при госпитализации больных с ...</td>\n",
       "      <td>Исследовалась частота ассоциации заболеваний щ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Введение. Одной из серьезных проблем, стоящих ...</td>\n",
       "      <td>Цель исследования - провести анализ динамики с...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Актуальность проблемы. На современном этапе пр...</td>\n",
       "      <td>В статье проанализированы данные о впервые выя...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Сохранение здоровья детей в Российской Федерац...</td>\n",
       "      <td>В статье представлены краткие данные о распрос...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             article  \\\n",
       "0  Состав и калорийность\\nПлоды айвы достаточно н...   \n",
       "1  В последние годы при госпитализации больных с ...   \n",
       "2  Введение. Одной из серьезных проблем, стоящих ...   \n",
       "3  Актуальность проблемы. На современном этапе пр...   \n",
       "4  Сохранение здоровья детей в Российской Федерац...   \n",
       "\n",
       "                                            abstract  \n",
       "0  Айву, которую часто называют ложным яблоком вс...  \n",
       "1  Исследовалась частота ассоциации заболеваний щ...  \n",
       "2  Цель исследования - провести анализ динамики с...  \n",
       "3  В статье проанализированы данные о впервые выя...  \n",
       "4  В статье представлены краткие данные о распрос...  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "! wget https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.ftz\n",
    "lang_detector = fasttext.load_model('lid.176.ftz')\n",
    "\n",
    "data_path = '/DATA/ichuviliaeva/project_data'\n",
    "data = []\n",
    "for file_name in os.listdir(data_path):\n",
    "    if 'articles' not in file_name:\n",
    "        continue\n",
    "    data_part = pd.read_csv(os.path.join(data_path, file_name))\n",
    "\n",
    "    # Checking that article and abstract are written in russian\n",
    "    for article, abstract in tqdm(data_part[['article', 'abstract']].to_numpy()):\n",
    "        if lang_detector.predict(article.replace('\\n', ' '), k=1)[0][0] == '__label__en' or \\\n",
    "            lang_detector.predict(abstract.replace('\\n', ' '), k=1)[0][0] == '__label__en':\n",
    "            continue\n",
    "        data.append([article, abstract])\n",
    "\n",
    "data = pd.DataFrame(data, columns=['article', 'abstract'])\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-03T01:24:39.609188Z",
     "iopub.status.busy": "2023-04-03T01:24:39.608401Z",
     "iopub.status.idle": "2023-04-03T01:24:39.622969Z",
     "shell.execute_reply": "2023-04-03T01:24:39.621827Z",
     "shell.execute_reply.started": "2023-04-03T01:24:39.609144Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Состав и калорийность\\nПлоды айвы достаточно н...</td>\n",
       "      <td>Айву, которую часто называют ложным яблоком вс...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>В последние годы при госпитализации больных с ...</td>\n",
       "      <td>Исследовалась частота ассоциации заболеваний щ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Введение. Одной из серьезных проблем, стоящих ...</td>\n",
       "      <td>Цель исследования - провести анализ динамики с...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Актуальность проблемы. На современном этапе пр...</td>\n",
       "      <td>В статье проанализированы данные о впервые выя...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Сохранение здоровья детей в Российской Федерац...</td>\n",
       "      <td>В статье представлены краткие данные о распрос...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38670</th>\n",
       "      <td>Одним из значимых симптомов в структуре дефект...</td>\n",
       "      <td>Работа посвящена рассмотрению проблемы коррекц...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38671</th>\n",
       "      <td>Актуальность обращения к методу сенсорной инте...</td>\n",
       "      <td>Представлены результаты экспериментального исс...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38672</th>\n",
       "      <td>По определению, принятому в научной литературе...</td>\n",
       "      <td>В статье рассматриваются условия успешного обу...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38673</th>\n",
       "      <td>В настоящее время в логопедии понятие «звуко-с...</td>\n",
       "      <td>В статье рассматривается актуальная проблема д...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38674</th>\n",
       "      <td>Эволюция представлений о формировании речи реб...</td>\n",
       "      <td>Статья посвящена лонгитюдному изучению формиро...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38675 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 article  \\\n",
       "0      Состав и калорийность\\nПлоды айвы достаточно н...   \n",
       "1      В последние годы при госпитализации больных с ...   \n",
       "2      Введение. Одной из серьезных проблем, стоящих ...   \n",
       "3      Актуальность проблемы. На современном этапе пр...   \n",
       "4      Сохранение здоровья детей в Российской Федерац...   \n",
       "...                                                  ...   \n",
       "38670  Одним из значимых симптомов в структуре дефект...   \n",
       "38671  Актуальность обращения к методу сенсорной инте...   \n",
       "38672  По определению, принятому в научной литературе...   \n",
       "38673  В настоящее время в логопедии понятие «звуко-с...   \n",
       "38674  Эволюция представлений о формировании речи реб...   \n",
       "\n",
       "                                                abstract  \n",
       "0      Айву, которую часто называют ложным яблоком вс...  \n",
       "1      Исследовалась частота ассоциации заболеваний щ...  \n",
       "2      Цель исследования - провести анализ динамики с...  \n",
       "3      В статье проанализированы данные о впервые выя...  \n",
       "4      В статье представлены краткие данные о распрос...  \n",
       "...                                                  ...  \n",
       "38670  Работа посвящена рассмотрению проблемы коррекц...  \n",
       "38671  Представлены результаты экспериментального исс...  \n",
       "38672  В статье рассматриваются условия успешного обу...  \n",
       "38673  В статье рассматривается актуальная проблема д...  \n",
       "38674  Статья посвящена лонгитюдному изучению формиро...  \n",
       "\n",
       "[38675 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-03T01:24:39.625107Z",
     "iopub.status.busy": "2023-04-03T01:24:39.624703Z",
     "iopub.status.idle": "2023-04-03T01:24:39.645666Z",
     "shell.execute_reply": "2023-04-03T01:24:39.644706Z",
     "shell.execute_reply.started": "2023-04-03T01:24:39.625068Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "test, val = train_test_split(test, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-03T01:24:39.648310Z",
     "iopub.status.busy": "2023-04-03T01:24:39.647861Z",
     "iopub.status.idle": "2023-04-03T01:24:39.662032Z",
     "shell.execute_reply": "2023-04-03T01:24:39.660774Z",
     "shell.execute_reply.started": "2023-04-03T01:24:39.648271Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36246</th>\n",
       "      <td>прочность, удар, разрушение, частица, измельче...</td>\n",
       "      <td>Среди технологического оборудования для измель...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33141</th>\n",
       "      <td>Актуальность научного исследования. Проблема в...</td>\n",
       "      <td>Проведено изучение морфологических особенносте...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23860</th>\n",
       "      <td>Современные направления развития высшего образ...</td>\n",
       "      <td>В работе представлены результаты проведенного ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8364</th>\n",
       "      <td>Вопрос о праве требует понятийно-философского ...</td>\n",
       "      <td>Мы находимся на переломном этапе истории, когд...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13153</th>\n",
       "      <td>Введение\\nздоровых детей раннего возраста при ...</td>\n",
       "      <td>Целью исследования являлась разработка подходо...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6920</th>\n",
       "      <td>I I роисходящие в мире преобразования в эконом...</td>\n",
       "      <td>Рассматривается сущность, специфика и структур...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2592</th>\n",
       "      <td>В современном обществе все более ясно осознает...</td>\n",
       "      <td>В статье приводятся результаты исследования ин...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11425</th>\n",
       "      <td>Основным видом органического вяжущего, применя...</td>\n",
       "      <td>Проведен анализ проблем, связанных с производс...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17884</th>\n",
       "      <td>Специфика деятельности таких больших библиотеч...</td>\n",
       "      <td>В статье раскрывается структура информационног...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15241</th>\n",
       "      <td>В настоящее время понятие доказывания находит ...</td>\n",
       "      <td>Настоящая статья посвящена одному из наиболее ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3810 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 article  \\\n",
       "36246  прочность, удар, разрушение, частица, измельче...   \n",
       "33141  Актуальность научного исследования. Проблема в...   \n",
       "23860  Современные направления развития высшего образ...   \n",
       "8364   Вопрос о праве требует понятийно-философского ...   \n",
       "13153  Введение\\nздоровых детей раннего возраста при ...   \n",
       "...                                                  ...   \n",
       "6920   I I роисходящие в мире преобразования в эконом...   \n",
       "2592   В современном обществе все более ясно осознает...   \n",
       "11425  Основным видом органического вяжущего, применя...   \n",
       "17884  Специфика деятельности таких больших библиотеч...   \n",
       "15241  В настоящее время понятие доказывания находит ...   \n",
       "\n",
       "                                                abstract  \n",
       "36246  Среди технологического оборудования для измель...  \n",
       "33141  Проведено изучение морфологических особенносте...  \n",
       "23860  В работе представлены результаты проведенного ...  \n",
       "8364   Мы находимся на переломном этапе истории, когд...  \n",
       "13153  Целью исследования являлась разработка подходо...  \n",
       "...                                                  ...  \n",
       "6920   Рассматривается сущность, специфика и структур...  \n",
       "2592   В статье приводятся результаты исследования ин...  \n",
       "11425  Проведен анализ проблем, связанных с производс...  \n",
       "17884  В статье раскрывается структура информационног...  \n",
       "15241  Настоящая статья посвящена одному из наиболее ...  \n",
       "\n",
       "[3810 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30940\n"
     ]
    }
   ],
   "source": [
    "print(len(train['abstract']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20633\n"
     ]
    }
   ],
   "source": [
    "print(len(train.iloc[8]['article']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing story 0 of 30476; 0.00 percent done\n",
      "Writing story 1000 of 30476; 3.28 percent done\n",
      "Writing story 2000 of 30476; 6.56 percent done\n",
      "Writing story 3000 of 30476; 9.84 percent done\n",
      "Writing story 4000 of 30476; 13.13 percent done\n",
      "Writing story 5000 of 30476; 16.41 percent done\n",
      "Writing story 6000 of 30476; 19.69 percent done\n",
      "Writing story 7000 of 30476; 22.97 percent done\n",
      "Writing story 8000 of 30476; 26.25 percent done\n",
      "Writing story 9000 of 30476; 29.53 percent done\n",
      "Writing story 10000 of 30476; 32.81 percent done\n",
      "Writing story 11000 of 30476; 36.09 percent done\n",
      "Writing story 12000 of 30476; 39.38 percent done\n",
      "Writing story 13000 of 30476; 42.66 percent done\n",
      "Writing story 14000 of 30476; 45.94 percent done\n",
      "Writing story 15000 of 30476; 49.22 percent done\n",
      "Writing story 16000 of 30476; 52.50 percent done\n",
      "Writing story 17000 of 30476; 55.78 percent done\n",
      "Writing story 18000 of 30476; 59.06 percent done\n",
      "Writing story 19000 of 30476; 62.34 percent done\n",
      "Writing story 20000 of 30476; 65.63 percent done\n",
      "Writing story 21000 of 30476; 68.91 percent done\n",
      "Writing story 22000 of 30476; 72.19 percent done\n",
      "Writing story 23000 of 30476; 75.47 percent done\n",
      "Writing story 24000 of 30476; 78.75 percent done\n",
      "Writing story 25000 of 30476; 82.03 percent done\n",
      "Writing story 26000 of 30476; 85.31 percent done\n",
      "Writing story 27000 of 30476; 88.59 percent done\n",
      "Writing story 28000 of 30476; 91.88 percent done\n",
      "Writing story 29000 of 30476; 95.16 percent done\n",
      "Writing story 30000 of 30476; 98.44 percent done\n",
      "Writing story 0 of 3810; 0.00 percent done\n",
      "Writing story 1000 of 3810; 26.25 percent done\n",
      "Writing story 2000 of 3810; 52.49 percent done\n",
      "Writing story 3000 of 3810; 78.74 percent done\n",
      "Writing story 0 of 3810; 0.00 percent done\n",
      "Writing story 1000 of 3810; 26.25 percent done\n",
      "Writing story 2000 of 3810; 52.49 percent done\n",
      "Writing story 3000 of 3810; 78.74 percent done\n",
      "Finished writing files\n"
     ]
    }
   ],
   "source": [
    "# prepearing for BART\n",
    "start_prefix = '/DATA/ichuviliaeva/project_data/'\n",
    "datasets = [train, val, test]\n",
    "for i, out_prefix in enumerate(['train', 'val', 'test']):\n",
    "    num_stories = len(datasets[i]['article'])\n",
    "    \n",
    "    with open(start_prefix + out_prefix + '.source', 'wt') as source_file, open(start_prefix + out_prefix + '.target', 'wt') as target_file:\n",
    "        for idx, s in enumerate(datasets[i]['article']):\n",
    "            if idx % 1000 == 0:\n",
    "                print(\"Writing story %i of %i; %.2f percent done\" % (idx, num_stories, float(idx)*100.0/float(num_stories)))\n",
    "\n",
    "            source_file.write(datasets[i].iloc[idx]['article'].replace('\\n', ' ') + '\\n')\n",
    "            target_file.write(datasets[i].iloc[idx]['abstract'].replace('\\n', ' ') + '\\n')\n",
    "\n",
    "print(\"Finished writing files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'fairseq' already exists and is not an empty directory.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/facebookresearch/fairseq.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to bpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "commonsense_qa\t\t\t__pycache__\r\n",
      "config\t\t\t\tREADME.custom_classification.md\r\n",
      "fb_multilingual\t\t\tREADME.glue.md\r\n",
      "multiprocessing_bpe_encoder.py\tREADME.md\r\n",
      "preprocess_GLUE_tasks.sh\tREADME.pretraining.md\r\n",
      "preprocess_RACE.py\t\tREADME.race.md\r\n",
      "preprocess_RACE.sh\t\twsc\r\n"
     ]
    }
   ],
   "source": [
    "!ls fairseq/examples/roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ichuviliaeva/is_project/fairseq\n"
     ]
    }
   ],
   "source": [
    "%cd fairseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fairseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-04-17 07:05:08--  https://www.dropbox.com/s/b2auu9dhrm2wj0p/gazeta_mbart_checkpoint_600_160.tar.gz\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 162.125.70.18, 2620:100:6026:18::a27d:4612\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|162.125.70.18|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: /s/raw/b2auu9dhrm2wj0p/gazeta_mbart_checkpoint_600_160.tar.gz [following]\n",
      "--2023-04-17 07:05:09--  https://www.dropbox.com/s/raw/b2auu9dhrm2wj0p/gazeta_mbart_checkpoint_600_160.tar.gz\n",
      "Reusing existing connection to www.dropbox.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://uc10fcd470b1d08525e3d3154740.dl.dropboxusercontent.com/cd/0/inline/B6Vz0iY2vOlUW9H1V1DJaLSBjgGdSitDsTpWNt_Pfv6Bl6PdCEzL0d-ocQ-IV0UFReZQANnemFfDN9dUvq41uF_mLMZlkyoSa4G1VyXp4p1AdEfqMEQzfft5l0E7NU2Z01rtmVe_lmVOxet5ysfIEGKeUcSwadwLDQgEUgsdBn3V6Q/file# [following]\n",
      "--2023-04-17 07:05:09--  https://uc10fcd470b1d08525e3d3154740.dl.dropboxusercontent.com/cd/0/inline/B6Vz0iY2vOlUW9H1V1DJaLSBjgGdSitDsTpWNt_Pfv6Bl6PdCEzL0d-ocQ-IV0UFReZQANnemFfDN9dUvq41uF_mLMZlkyoSa4G1VyXp4p1AdEfqMEQzfft5l0E7NU2Z01rtmVe_lmVOxet5ysfIEGKeUcSwadwLDQgEUgsdBn3V6Q/file\n",
      "Resolving uc10fcd470b1d08525e3d3154740.dl.dropboxusercontent.com (uc10fcd470b1d08525e3d3154740.dl.dropboxusercontent.com)... 162.125.70.15, 2620:100:6026:15::a27d:460f\n",
      "Connecting to uc10fcd470b1d08525e3d3154740.dl.dropboxusercontent.com (uc10fcd470b1d08525e3d3154740.dl.dropboxusercontent.com)|162.125.70.15|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: /cd/0/inline2/B6W904-YldUXvvCZjSzjs-vY4U5n9erBHdTrIzuKJq6fdZaIiJ2WUA_emSJVrSKRVFLByS-8dI3IlnL7EZyOx7QKddZOIIRxuNzM8igQusEo68WkTef8zadIv0hUOgcMdyfJdDiyfcNshjC-hUfglUC-PIKdQB6fi3a8Zb3YJUUTdBlmRSzG_0KZxV1a1lhJ4KaJD9gA54FEh3sHYyfp_d7c5zakn3CRk1miG_IBmtFe-dcJZmiiifnI2CJphCKu7SKI_dkVQ8UwzKEPG5LAzhIk8YhkypmgVE6up7__cUnq-dlyWv2SZRA-KA63IuWsFwuMqB5XAGzE8ExdNZBU_MW0QNwVZYdC866NohU51-VAo50eKwsdJ6x9ywawG-HgoK5p025ZyDDiauyOcvQ2sl6JAD92LIjG7T83iBf0DZwR4g/file [following]\n",
      "--2023-04-17 07:05:10--  https://uc10fcd470b1d08525e3d3154740.dl.dropboxusercontent.com/cd/0/inline2/B6W904-YldUXvvCZjSzjs-vY4U5n9erBHdTrIzuKJq6fdZaIiJ2WUA_emSJVrSKRVFLByS-8dI3IlnL7EZyOx7QKddZOIIRxuNzM8igQusEo68WkTef8zadIv0hUOgcMdyfJdDiyfcNshjC-hUfglUC-PIKdQB6fi3a8Zb3YJUUTdBlmRSzG_0KZxV1a1lhJ4KaJD9gA54FEh3sHYyfp_d7c5zakn3CRk1miG_IBmtFe-dcJZmiiifnI2CJphCKu7SKI_dkVQ8UwzKEPG5LAzhIk8YhkypmgVE6up7__cUnq-dlyWv2SZRA-KA63IuWsFwuMqB5XAGzE8ExdNZBU_MW0QNwVZYdC866NohU51-VAo50eKwsdJ6x9ywawG-HgoK5p025ZyDDiauyOcvQ2sl6JAD92LIjG7T83iBf0DZwR4g/file\n",
      "Reusing existing connection to uc10fcd470b1d08525e3d3154740.dl.dropboxusercontent.com:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 6097678592 (5.7G) [application/octet-stream]\n",
      "Saving to: ‘/DATA/ichuviliaeva/project_data/gazeta_mbart_checkpoint_600_160.tar.gz’\n",
      "\n",
      "/DATA/ichuviliaeva/ 100%[===================>]   5.68G  19.8MB/s    in 4m 53s  \n",
      "\n",
      "2023-04-17 07:10:04 (19.9 MB/s) - ‘/DATA/ichuviliaeva/project_data/gazeta_mbart_checkpoint_600_160.tar.gz’ saved [6097678592/6097678592]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://www.dropbox.com/s/b2auu9dhrm2wj0p/gazeta_mbart_checkpoint_600_160.tar.gz \\\n",
    "    -O /DATA/ichuviliaeva/project_data/gazeta_mbart_checkpoint_600_160.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint12.pt\r\n"
     ]
    }
   ],
   "source": [
    "!tar -xzvf /DATA/ichuviliaeva/project_data/gazeta_mbart_checkpoint_600_160.tar.gz -C /DATA/ichuviliaeva/project_data/gazeta_bart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-04-17 07:38:28--  https://www.dropbox.com/s/rqnwjuvp91vhni5/gazeta_data_mbart_600_160.tar.gz\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 162.125.70.18, 2620:100:6026:18::a27d:4612\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|162.125.70.18|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: /s/raw/rqnwjuvp91vhni5/gazeta_data_mbart_600_160.tar.gz [following]\n",
      "--2023-04-17 07:38:29--  https://www.dropbox.com/s/raw/rqnwjuvp91vhni5/gazeta_data_mbart_600_160.tar.gz\n",
      "Reusing existing connection to www.dropbox.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://uc95ee0a2f5befbeea3111755c6d.dl.dropboxusercontent.com/cd/0/inline/B6UOP7ZEF__jAVOsLo31b2nfiYLIH_AWRxvxMxmnYOFjXCvx3nsegMJPtXOXaOlDKNGz0rtaanusz4z2iSOqlygm2yHFmrVEKpu85HRGUodduFfC6C3NjJKWaBgfTdS1Kek11Z7k3SaBvUy0B1g6Qcfs1LHMFuGTADevyF94AkNlYw/file# [following]\n",
      "--2023-04-17 07:38:29--  https://uc95ee0a2f5befbeea3111755c6d.dl.dropboxusercontent.com/cd/0/inline/B6UOP7ZEF__jAVOsLo31b2nfiYLIH_AWRxvxMxmnYOFjXCvx3nsegMJPtXOXaOlDKNGz0rtaanusz4z2iSOqlygm2yHFmrVEKpu85HRGUodduFfC6C3NjJKWaBgfTdS1Kek11Z7k3SaBvUy0B1g6Qcfs1LHMFuGTADevyF94AkNlYw/file\n",
      "Resolving uc95ee0a2f5befbeea3111755c6d.dl.dropboxusercontent.com (uc95ee0a2f5befbeea3111755c6d.dl.dropboxusercontent.com)... 162.125.70.15, 2620:100:6026:15::a27d:460f\n",
      "Connecting to uc95ee0a2f5befbeea3111755c6d.dl.dropboxusercontent.com (uc95ee0a2f5befbeea3111755c6d.dl.dropboxusercontent.com)|162.125.70.15|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: /cd/0/inline2/B6UWEC0KrZKlgw2pDRbUKPL47K9boEh-J58oThlh3Y3CFIFhr5uAjkACDXe-4RHMMv_BywU5GgH_PlONgXv8k-1QcYAVEn3npuqH7VD__fTb49nNKpf0YKXSX2THAPZhBoXrLFuTs9lC9uWUN3EFA6aXpj2FmOn8_DvgZBocCjkU_3_enbVMJhCzh5VnilsIjR7tmftGL5Zv8ufeBa99aBgEh5z3kxTRpdCrj12uum9VAZ9mlkeq3sV1qkCfAGolh2PipU6giC3lOa9n3YiMLqwO5D5l6UxzYalIqkKEY177t_AeJbCbsjia2hPg7kOb2fK4ivLQcuvmJBr3GDfhNokxvGbWrnVTXbyiNNTG61Rl9nmtfbHPCuxTjl6UHwzTL1yJKeCOfz_lf1hvxcr_CTDl1eHzxpNjhuJbuk7ZUcVFhA/file [following]\n",
      "--2023-04-17 07:38:30--  https://uc95ee0a2f5befbeea3111755c6d.dl.dropboxusercontent.com/cd/0/inline2/B6UWEC0KrZKlgw2pDRbUKPL47K9boEh-J58oThlh3Y3CFIFhr5uAjkACDXe-4RHMMv_BywU5GgH_PlONgXv8k-1QcYAVEn3npuqH7VD__fTb49nNKpf0YKXSX2THAPZhBoXrLFuTs9lC9uWUN3EFA6aXpj2FmOn8_DvgZBocCjkU_3_enbVMJhCzh5VnilsIjR7tmftGL5Zv8ufeBa99aBgEh5z3kxTRpdCrj12uum9VAZ9mlkeq3sV1qkCfAGolh2PipU6giC3lOa9n3YiMLqwO5D5l6UxzYalIqkKEY177t_AeJbCbsjia2hPg7kOb2fK4ivLQcuvmJBr3GDfhNokxvGbWrnVTXbyiNNTG61Rl9nmtfbHPCuxTjl6UHwzTL1yJKeCOfz_lf1hvxcr_CTDl1eHzxpNjhuJbuk7ZUcVFhA/file\n",
      "Reusing existing connection to uc95ee0a2f5befbeea3111755c6d.dl.dropboxusercontent.com:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 86003148 (82M) [application/octet-stream]\n",
      "Saving to: ‘/DATA/ichuviliaeva/project_data/gazeta_data_mbart_600_160.tar.gz’\n",
      "\n",
      "/DATA/ichuviliaeva/ 100%[===================>]  82.02M  18.2MB/s    in 4.7s    \n",
      "\n",
      "2023-04-17 07:38:35 (17.6 MB/s) - ‘/DATA/ichuviliaeva/project_data/gazeta_data_mbart_600_160.tar.gz’ saved [86003148/86003148]\n",
      "\n",
      "--2023-04-17 07:38:35--  https://dl.fbaipublicfiles.com/fairseq/models/mbart/mbart.CC25.tar.gz\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 108.156.22.45, 108.156.22.14, 108.156.22.118, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|108.156.22.45|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5618825662 (5.2G) [application/x-tar]\n",
      "Saving to: ‘/DATA/ichuviliaeva/project_data/mbart.CC25.tar.gz’\n",
      "\n",
      "/DATA/ichuviliaeva/ 100%[===================>]   5.23G  26.8MB/s    in 4m 40s  \n",
      "\n",
      "2023-04-17 07:43:16 (19.1 MB/s) - ‘/DATA/ichuviliaeva/project_data/mbart.CC25.tar.gz’ saved [5618825662/5618825662]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://www.dropbox.com/s/rqnwjuvp91vhni5/gazeta_data_mbart_600_160.tar.gz \\\n",
    "    -O /DATA/ichuviliaeva/project_data/gazeta_data_mbart_600_160.tar.gz\n",
    "!wget https://dl.fbaipublicfiles.com/fairseq/models/mbart/mbart.CC25.tar.gz \\\n",
    "    -O /DATA/ichuviliaeva/project_data/mbart.CC25.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data-bin/\n",
      "data-bin/dict.source.txt\n",
      "data-bin/dict.target.txt\n",
      "data-bin/preprocess.log\n",
      "data-bin/test.source-target.source.bin\n",
      "data-bin/test.source-target.source.idx\n",
      "data-bin/test.source-target.target.bin\n",
      "data-bin/test.source-target.target.idx\n",
      "data-bin/train.source-target.source.bin\n",
      "data-bin/valid.source-target.source.bin\n",
      "data-bin/valid.source-target.source.idx\n",
      "data-bin/valid.source-target.target.bin\n",
      "data-bin/valid.source-target.target.idx\n",
      "data-bin/train.source-target.source.idx\n",
      "data-bin/train.source-target.target.bin\n",
      "data-bin/train.source-target.target.idx\n",
      "mbart.cc25/\n",
      "mbart.cc25/model.pt\n",
      "mbart.cc25/sentence.bpe.model\n",
      "mbart.cc25/dict.txt\n"
     ]
    }
   ],
   "source": [
    "!tar -xzvf /DATA/ichuviliaeva/project_data/gazeta_data_mbart_600_160.tar.gz -C /DATA/ichuviliaeva/project_data/gazeta_bart\n",
    "!tar -xzvf /DATA/ichuviliaeva/project_data/mbart.CC25.tar.gz -C /DATA/ichuviliaeva/project_data/gazeta_bart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fairseq-preprocess \\\r\n",
      "  --source-lang \"source\" \\\r\n",
      "  --target-lang \"target\" \\\r\n",
      "  --trainpref \"/DATA/ichuviliaeva/project_data/train.bpe\" \\\r\n",
      "  --validpref \"/DATA/ichuviliaeva/project_data/val.bpe\" \\\r\n",
      "  --destdir \"/DATA/ichuviliaeva/project_data/bin/\" \\\r\n",
      "  --workers 60 \\\r\n",
      "  --srcdict \"/DATA/ichuviliaeva/project_data/gazeta_bart/data-bin/dict.source.txt\" \\\r\n",
      "  --tgtdict \"/DATA/ichuviliaeva/project_data/gazeta_bart/data-bin/dict.target.txt\";"
     ]
    }
   ],
   "source": [
    "!cat fairseq/binarise.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 10:40:23 | INFO | fairseq_cli.preprocess | Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer=None, lr_scheduler='fixed', scoring='bleu', task='translation', source_lang='source', target_lang='target', trainpref='/DATA/ichuviliaeva/project_data/train.bpe', validpref='/DATA/ichuviliaeva/project_data/val.bpe', testpref=None, align_suffix=None, destdir='/DATA/ichuviliaeva/project_data/bin/', thresholdtgt=0, thresholdsrc=0, tgtdict='/DATA/ichuviliaeva/project_data/gazeta_bart/data-bin/dict.target.txt', srcdict='/DATA/ichuviliaeva/project_data/gazeta_bart/data-bin/dict.source.txt', nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=False, padding_factor=8, workers=60, dict_only=False)\n",
      "2023-04-17 10:40:25 | INFO | fairseq_cli.preprocess | [source] Dictionary: 250001 types\n",
      "2023-04-17 10:44:41 | INFO | fairseq_cli.preprocess | [source] /DATA/ichuviliaeva/project_data/train.bpe.source: 30940 sents, 597356549 tokens, 67.9% replaced (by <unk>)\n",
      "2023-04-17 10:44:41 | INFO | fairseq_cli.preprocess | [source] Dictionary: 250001 types\n",
      "2023-04-17 10:45:17 | INFO | fairseq_cli.preprocess | [source] /DATA/ichuviliaeva/project_data/val.bpe.source: 3868 sents, 74288249 tokens, 67.9% replaced (by <unk>)\n",
      "2023-04-17 10:45:17 | INFO | fairseq_cli.preprocess | [target] Dictionary: 250001 types\n",
      "2023-04-17 10:45:34 | INFO | fairseq_cli.preprocess | [target] /DATA/ichuviliaeva/project_data/train.bpe.target: 30940 sents, 25887679 tokens, 69.1% replaced (by <unk>)\n",
      "2023-04-17 10:45:34 | INFO | fairseq_cli.preprocess | [target] Dictionary: 250001 types\n",
      "2023-04-17 10:45:43 | INFO | fairseq_cli.preprocess | [target] /DATA/ichuviliaeva/project_data/val.bpe.target: 3868 sents, 3205940 tokens, 69.0% replaced (by <unk>)\n",
      "2023-04-17 10:45:43 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to /DATA/ichuviliaeva/project_data/bin/\n"
     ]
    }
   ],
   "source": [
    "!bash fairseq/binarise.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BART_PATH=\"$1\"\r\n",
      "\r\n",
      "rm -rf \"/DATA/ichuviliaeva/project_data/bin/\"\r\n",
      "fairseq-preprocess \\\r\n",
      "    --source-lang \"source\" \\\r\n",
      "    --target-lang \"target\" \\\r\n",
      "    --trainpref \"/DATA/ichuviliaeva/project_data/train.bpe\" \\\r\n",
      "    --validpref \"/DATA/ichuviliaeva/project_data/val.bpe\" \\\r\n",
      "    --testpref \"/DATA/ichuviliaeva/project_data/test.bpe\" \\\r\n",
      "    --destdir \"/DATA/ichuviliaeva/project_data/bin/\" \\\r\n",
      "    --workers 60 \\\r\n",
      "    --srcdict \"${BART_PATH}/dict.source.txt\" \\\r\n",
      "    --tgtdict \"${BART_PATH}/dict.target.txt\";\r\n"
     ]
    }
   ],
   "source": [
    "!cat summarus/external/bart_scripts/preprocess.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 10:45:49 | INFO | fairseq_cli.preprocess | Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer=None, lr_scheduler='fixed', scoring='bleu', task='translation', source_lang='source', target_lang='target', trainpref='/DATA/ichuviliaeva/project_data/train.bpe', validpref='/DATA/ichuviliaeva/project_data/val.bpe', testpref='/DATA/ichuviliaeva/project_data/test.bpe', align_suffix=None, destdir='/DATA/ichuviliaeva/project_data/bin/', thresholdtgt=0, thresholdsrc=0, tgtdict='/DATA/ichuviliaeva/project_data/gazeta_bart/data-bin/dict.target.txt', srcdict='/DATA/ichuviliaeva/project_data/gazeta_bart/data-bin/dict.source.txt', nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=False, padding_factor=8, workers=60, dict_only=False)\n",
      "2023-04-17 10:45:50 | INFO | fairseq_cli.preprocess | [source] Dictionary: 250001 types\n",
      "2023-04-17 10:49:49 | INFO | fairseq_cli.preprocess | [source] /DATA/ichuviliaeva/project_data/train.bpe.source: 30940 sents, 597356549 tokens, 67.9% replaced (by <unk>)\n",
      "2023-04-17 10:49:49 | INFO | fairseq_cli.preprocess | [source] Dictionary: 250001 types\n",
      "2023-04-17 10:50:24 | INFO | fairseq_cli.preprocess | [source] /DATA/ichuviliaeva/project_data/val.bpe.source: 3868 sents, 74288249 tokens, 67.9% replaced (by <unk>)\n",
      "2023-04-17 10:50:24 | INFO | fairseq_cli.preprocess | [source] Dictionary: 250001 types\n",
      "2023-04-17 10:50:59 | INFO | fairseq_cli.preprocess | [source] /DATA/ichuviliaeva/project_data/test.bpe.source: 3867 sents, 74748125 tokens, 67.9% replaced (by <unk>)\n",
      "2023-04-17 10:50:59 | INFO | fairseq_cli.preprocess | [target] Dictionary: 250001 types\n",
      "2023-04-17 10:51:15 | INFO | fairseq_cli.preprocess | [target] /DATA/ichuviliaeva/project_data/train.bpe.target: 30940 sents, 25887679 tokens, 69.1% replaced (by <unk>)\n",
      "2023-04-17 10:51:15 | INFO | fairseq_cli.preprocess | [target] Dictionary: 250001 types\n",
      "2023-04-17 10:51:24 | INFO | fairseq_cli.preprocess | [target] /DATA/ichuviliaeva/project_data/val.bpe.target: 3868 sents, 3205940 tokens, 69.0% replaced (by <unk>)\n",
      "2023-04-17 10:51:24 | INFO | fairseq_cli.preprocess | [target] Dictionary: 250001 types\n",
      "2023-04-17 10:51:32 | INFO | fairseq_cli.preprocess | [target] /DATA/ichuviliaeva/project_data/test.bpe.target: 3867 sents, 3210194 tokens, 69.0% replaced (by <unk>)\n",
      "2023-04-17 10:51:32 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to /DATA/ichuviliaeva/project_data/bin/\n"
     ]
    }
   ],
   "source": [
    "!bash summarus/external/bart_scripts/preprocess.sh \\\n",
    "/DATA/ichuviliaeva/project_data/gazeta_bart/data-bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MBART_PATH=\"$1\"\r\n",
      "DATA_BIN_PATH=\"$2\"\r\n",
      "\r\n",
      "MAX_UPDATE=80000\r\n",
      "WARMUP_UPDATES=500\r\n",
      "LR=3e-05\r\n",
      "MAX_TOKENS=605\r\n",
      "UPDATE_FREQ=2\r\n",
      "LANGS=ar_AR,cs_CZ,de_DE,en_XX,es_XX,et_EE,fi_FI,fr_XX,gu_IN,hi_IN,it_IT,ja_XX,kk_KZ,ko_KR,lt_LT,lv_LV,my_MM,ne_NP,nl_XX,ro_RO,ru_RU,si_LK,tr_TR,vi_VN,zh_CN\r\n",
      "\r\n",
      "CUDA_VISIBLE_DEVICES=2 fairseq-train \"$DATA_BIN_PATH\" \\\r\n",
      "  --encoder-normalize-before --decoder-normalize-before \\\r\n",
      "  --arch bart_large --layernorm-embedding \\\r\n",
      "  --task translation_from_pretrained_bart \\\r\n",
      "  --source-lang source --target-lang target \\\r\n",
      "  --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\r\n",
      "  --optimizer adam --adam-eps 1e-06 --adam-betas '(0.9, 0.98)' \\\r\n",
      "  --lr-scheduler polynomial_decay --lr \"$LR\" --warmup-updates \"$WARMUP_UPDATES\" \\\r\n",
      "  --dropout 0.1 --attention-dropout 0.1 --weight-decay 0.01 --clip-norm 0.1 \\\r\n",
      "  --total-num-update \"$MAX_UPDATE\" \\\r\n",
      "  --max-tokens \"$MAX_TOKENS\" \\\r\n",
      "  --required-batch-size-multiple 1 \\\r\n",
      "  --update-freq \"$UPDATE_FREQ\" \\\r\n",
      "  --save-interval 1 --save-interval-updates 5000 \\\r\n",
      "  --keep-interval-updates 3 --no-epoch-checkpoints \\\r\n",
      "  --seed 42 --log-format simple --log-interval 100 \\\r\n",
      "  --restore-file \"${MBART_PATH}\" \\\r\n",
      "  --reset-optimizer --reset-meters --reset-dataloader --reset-lr-scheduler \\\r\n",
      "  --langs $LANGS \\\r\n",
      "  --find-unused-parameters \\\r\n",
      "  --memory-efficient-fp16 \\\r\n",
      "  --skip-invalid-size-inputs-valid-test \\\r\n",
      "  --find-unused-parameters \\\r\n",
      "  --no-save-optimizer-state\r\n"
     ]
    }
   ],
   "source": [
    "!cat summarus/external/bart_scripts/train.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 10:51:39 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': 'simple', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 42, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 605, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 605, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.1, 'sentence_avg': False, 'update_freq': [2], 'lr': [3e-05], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': '/DATA/ichuviliaeva/project_data/gazeta_bart/checkpoint12.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': True, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 5000, 'keep_interval_updates': 3, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': True, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format='simple', log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=42, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=True, memory_efficient_fp16=True, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='polynomial_decay', scoring='bleu', task='translation_from_pretrained_bart', num_workers=1, skip_invalid_size_inputs_valid_test=True, max_tokens=605, batch_size=None, required_batch_size_multiple=1, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=605, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=True, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='bart_large', max_epoch=0, max_update=0, stop_time_hours=0, clip_norm=0.1, sentence_avg=False, update_freq=[2], lr=[3e-05], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, debug_param_names=False, save_dir='checkpoints', restore_file='/DATA/ichuviliaeva/project_data/gazeta_bart/checkpoint12.pt', continue_once=None, finetune_from_model=None, reset_dataloader=True, reset_lr_scheduler=True, reset_meters=True, reset_optimizer=True, optimizer_overrides='{}', save_interval=1, save_interval_updates=5000, keep_interval_updates=3, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=True, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, data='/DATA/ichuviliaeva/project_data/bin', source_lang='source', target_lang='target', load_alignments=False, left_pad_source=True, left_pad_target=False, upsample_primary=-1, truncate_source=False, num_batch_buckets=0, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_print_samples=False, langs='ar_AR,cs_CZ,de_DE,en_XX,es_XX,et_EE,fi_FI,fr_XX,gu_IN,hi_IN,it_IT,ja_XX,kk_KZ,ko_KR,lt_LT,lv_LV,my_MM,ne_NP,nl_XX,ro_RO,ru_RU,si_LK,tr_TR,vi_VN,zh_CN', prepend_bos=False, label_smoothing=0.1, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.98)', adam_eps=1e-06, weight_decay=0.01, use_old_adam=False, fp16_adam_stats=False, warmup_updates=500, force_anneal=None, end_learning_rate=0.0, power=1.0, total_num_update='80000', pad=1, eos=2, unk=3, encoder_normalize_before=True, decoder_normalize_before=True, layernorm_embedding=True, dropout=0.1, attention_dropout=0.1, no_seed_provided=False, encoder_embed_path=None, encoder_embed_dim=1024, encoder_ffn_embed_dim=4096, encoder_layers=12, encoder_attention_heads=16, encoder_learned_pos=True, decoder_embed_path=None, decoder_embed_dim=1024, decoder_ffn_embed_dim=4096, decoder_layers=12, decoder_attention_heads=16, decoder_learned_pos=True, relu_dropout=0.0, max_target_positions=1024, max_source_positions=1024, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, share_decoder_input_output_embed=True, share_all_embeddings=True, decoder_output_dim=1024, decoder_input_dim=1024, no_scale_embedding=True, activation_fn='gelu', pooler_activation_fn='tanh', pooler_dropout=0.0, _name='bart_large'), 'task': Namespace(no_progress_bar=False, log_interval=100, log_format='simple', log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=42, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=True, memory_efficient_fp16=True, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='polynomial_decay', scoring='bleu', task='translation_from_pretrained_bart', num_workers=1, skip_invalid_size_inputs_valid_test=True, max_tokens=605, batch_size=None, required_batch_size_multiple=1, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=605, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=True, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='bart_large', max_epoch=0, max_update=0, stop_time_hours=0, clip_norm=0.1, sentence_avg=False, update_freq=[2], lr=[3e-05], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, debug_param_names=False, save_dir='checkpoints', restore_file='/DATA/ichuviliaeva/project_data/gazeta_bart/checkpoint12.pt', continue_once=None, finetune_from_model=None, reset_dataloader=True, reset_lr_scheduler=True, reset_meters=True, reset_optimizer=True, optimizer_overrides='{}', save_interval=1, save_interval_updates=5000, keep_interval_updates=3, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=True, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, data='/DATA/ichuviliaeva/project_data/bin', source_lang='source', target_lang='target', load_alignments=False, left_pad_source=True, left_pad_target=False, upsample_primary=-1, truncate_source=False, num_batch_buckets=0, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_print_samples=False, langs='ar_AR,cs_CZ,de_DE,en_XX,es_XX,et_EE,fi_FI,fr_XX,gu_IN,hi_IN,it_IT,ja_XX,kk_KZ,ko_KR,lt_LT,lv_LV,my_MM,ne_NP,nl_XX,ro_RO,ru_RU,si_LK,tr_TR,vi_VN,zh_CN', prepend_bos=False, label_smoothing=0.1, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.98)', adam_eps=1e-06, weight_decay=0.01, use_old_adam=False, fp16_adam_stats=False, warmup_updates=500, force_anneal=None, end_learning_rate=0.0, power=1.0, total_num_update='80000', pad=1, eos=2, unk=3, encoder_normalize_before=True, decoder_normalize_before=True, layernorm_embedding=True, dropout=0.1, attention_dropout=0.1, no_seed_provided=False, encoder_embed_path=None, encoder_embed_dim=1024, encoder_ffn_embed_dim=4096, encoder_layers=12, encoder_attention_heads=16, encoder_learned_pos=True, decoder_embed_path=None, decoder_embed_dim=1024, decoder_ffn_embed_dim=4096, decoder_layers=12, decoder_attention_heads=16, decoder_learned_pos=True, relu_dropout=0.0, max_target_positions=1024, max_source_positions=1024, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, share_decoder_input_output_embed=True, share_all_embeddings=True, decoder_output_dim=1024, decoder_input_dim=1024, no_scale_embedding=True, activation_fn='gelu', pooler_activation_fn='tanh', pooler_dropout=0.0, _name='translation_from_pretrained_bart'), 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [3e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 500, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 80000.0, 'lr': [3e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 10:51:40 | INFO | fairseq.tasks.translation | [source] dictionary: 250001 types\n",
      "2023-04-17 10:51:40 | INFO | fairseq.tasks.translation | [target] dictionary: 250001 types\n",
      "2023-04-17 10:51:58 | INFO | fairseq_cli.train | BARTModel(\n",
      "  (encoder): TransformerEncoderBase(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): Embedding(250027, 1024, padding_idx=1)\n",
      "    (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)\n",
      "    (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (decoder): TransformerDecoderBase(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): Embedding(250027, 1024, padding_idx=1)\n",
      "    (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)\n",
      "    (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (output_projection): Linear(in_features=1024, out_features=250027, bias=False)\n",
      "  )\n",
      "  (classification_heads): ModuleDict()\n",
      ")\n",
      "2023-04-17 10:51:58 | INFO | fairseq_cli.train | task: TranslationFromPretrainedBARTTask\n",
      "2023-04-17 10:51:58 | INFO | fairseq_cli.train | model: BARTModel\n",
      "2023-04-17 10:51:58 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion\n",
      "2023-04-17 10:51:58 | INFO | fairseq_cli.train | num. shared model params: 610,851,840 (num. trained: 610,851,840)\n",
      "2023-04-17 10:51:58 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
      "2023-04-17 10:51:58 | INFO | fairseq.data.data_utils | loaded 3,868 examples from: /DATA/ichuviliaeva/project_data/bin/valid.source-target.source\n",
      "2023-04-17 10:51:58 | INFO | fairseq.data.data_utils | loaded 3,868 examples from: /DATA/ichuviliaeva/project_data/bin/valid.source-target.target\n",
      "2023-04-17 10:51:58 | INFO | fairseq.tasks.translation | /DATA/ichuviliaeva/project_data/bin valid source-target 3868 examples\n",
      "2023-04-17 10:52:00 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight\n",
      "2023-04-17 10:52:00 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight\n",
      "2023-04-17 10:52:00 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2023-04-17 10:52:00 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = NVIDIA TITAN RTX                        \n",
      "2023-04-17 10:52:00 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2023-04-17 10:52:00 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
      "2023-04-17 10:52:00 | INFO | fairseq_cli.train | max tokens per device = 605 and max sentences per device = None\n",
      "2023-04-17 10:52:00 | INFO | fairseq.trainer | Preparing to load checkpoint /DATA/ichuviliaeva/project_data/gazeta_bart/checkpoint12.pt\n",
      "2023-04-17 10:52:09 | INFO | fairseq.trainer | Loaded checkpoint /DATA/ichuviliaeva/project_data/gazeta_bart/checkpoint12.pt (epoch 12 @ 0 updates)\n",
      "2023-04-17 10:52:09 | INFO | fairseq.trainer | loading train data for epoch 1\n",
      "2023-04-17 10:52:11 | INFO | fairseq.data.data_utils | loaded 30,940 examples from: /DATA/ichuviliaeva/project_data/bin/train.source-target.source\n",
      "2023-04-17 10:52:11 | INFO | fairseq.data.data_utils | loaded 30,940 examples from: /DATA/ichuviliaeva/project_data/bin/train.source-target.target\n",
      "2023-04-17 10:52:11 | INFO | fairseq.tasks.translation | /DATA/ichuviliaeva/project_data/bin train source-target 30940 examples\n",
      "2023-04-17 10:52:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 10:52:11 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n",
      "2023-04-17 10:52:11 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n",
      "2023-04-17 10:52:11 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n",
      "2023-04-17 10:52:11 | WARNING | fairseq.tasks.fairseq_task | 30,875 samples have invalid sizes and will be skipped, max_positions=(605, 605), first few sample ids=[7047, 11385, 26965, 466, 8736, 429, 14357, 17034, 7284, 24793]\n",
      "2023-04-17 10:52:11 | INFO | fairseq_cli.train | begin dry-run validation on \"valid\" subset\n",
      "2023-04-17 10:52:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 10:52:11 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n",
      "2023-04-17 10:52:11 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n",
      "2023-04-17 10:52:11 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n",
      "2023-04-17 10:52:11 | WARNING | fairseq.tasks.fairseq_task | 3,863 samples have invalid sizes and will be skipped, max_positions=(605, 605), first few sample ids=[368, 3125, 3422, 1793, 3118, 488, 116, 2693, 1272, 555]\n",
      "2023-04-17 10:52:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 10:52:11 | INFO | fairseq.trainer | begin training epoch 1\n",
      "2023-04-17 10:52:11 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "/home/ichuviliaeva/is_project/fairseq/fairseq/utils.py:374: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library\n",
      "  warnings.warn(\n",
      "2023-04-17 10:52:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0\n",
      "2023-04-17 10:52:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0\n",
      "2023-04-17 10:52:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
      "2023-04-17 10:52:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0\n",
      "2023-04-17 10:52:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 10:52:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0\n",
      "2023-04-17 10:52:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0\n",
      "2023-04-17 10:52:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.5\n",
      "/home/ichuviliaeva/miniconda3/envs/is_project/lib/python3.9/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "2023-04-17 10:52:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.25\n",
      "2023-04-17 10:52:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 10:52:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 10:52:25 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 17.884 | nll_loss 16.369 | ppl 84621.3 | wps 6414.3 | wpb 290.8 | bsz 1 | num_updates 23\n",
      "2023-04-17 10:52:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 23 updates\n",
      "2023-04-17 10:52:25 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_best.pt\n",
      "2023-04-17 10:52:39 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_best.pt\n",
      "2023-04-17 10:52:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 1 @ 23 updates, score 17.884) (writing took 22.636226251954213 seconds)\n",
      "2023-04-17 10:52:48 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
      "2023-04-17 10:52:48 | INFO | train | epoch 001 | loss 17.686 | nll_loss 16.372 | ppl 84796.6 | wps 470.5 | ups 0.7 | wpb 665.9 | bsz 2 | num_updates 23 | lr 1.38e-06 | gnorm 286.942 | clip 100 | loss_scale 0.25 | train_wall 13 | gb_free 13.9 | wall 48\n",
      "2023-04-17 10:52:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 10:52:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 10:52:48 | INFO | fairseq.trainer | begin training epoch 2\n",
      "2023-04-17 10:52:48 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 10:53:00 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 10:53:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 10:53:00 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 16.747 | nll_loss 15.099 | ppl 35096 | wps 6470 | wpb 290.8 | bsz 1 | num_updates 55 | best_loss 16.747\n",
      "2023-04-17 10:53:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 55 updates\n",
      "2023-04-17 10:53:00 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_best.pt\n",
      "2023-04-17 10:53:16 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_best.pt\n",
      "2023-04-17 10:53:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 2 @ 55 updates, score 16.747) (writing took 24.433882548939437 seconds)\n",
      "2023-04-17 10:53:25 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
      "2023-04-17 10:53:25 | INFO | train | epoch 002 | loss 16.978 | nll_loss 15.61 | ppl 50023.8 | wps 589.4 | ups 0.87 | wpb 678.5 | bsz 2 | num_updates 55 | lr 3.3e-06 | gnorm 238.299 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 85\n",
      "2023-04-17 10:53:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 10:53:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 10:53:25 | INFO | fairseq.trainer | begin training epoch 3\n",
      "2023-04-17 10:53:25 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 10:53:37 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 10:53:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 10:53:37 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 13.123 | nll_loss 10.987 | ppl 2029.21 | wps 6355.2 | wpb 290.8 | bsz 1 | num_updates 87 | best_loss 13.123\n",
      "2023-04-17 10:53:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 87 updates\n",
      "2023-04-17 10:53:37 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_best.pt\n",
      "2023-04-17 10:53:53 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_best.pt\n",
      "2023-04-17 10:54:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 3 @ 87 updates, score 13.123) (writing took 23.89454177301377 seconds)\n",
      "2023-04-17 10:54:01 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
      "2023-04-17 10:54:01 | INFO | train | epoch 003 | loss 14.352 | nll_loss 12.729 | ppl 6788.23 | wps 591.4 | ups 0.87 | wpb 678.5 | bsz 2 | num_updates 87 | lr 5.22e-06 | gnorm 192.531 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 122\n",
      "2023-04-17 10:54:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 10:54:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 10:54:01 | INFO | fairseq.trainer | begin training epoch 4\n",
      "2023-04-17 10:54:01 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 10:54:07 | INFO | train_inner | epoch 004:     13 / 32 loss=15.478, nll_loss=13.949, ppl=15815.2, wps=616.5, ups=0.9, wpb=684.6, bsz=2, num_updates=100, lr=6e-06, gnorm=228.974, clip=100, loss_scale=0.25, train_wall=43, gb_free=13.9, wall=127\n",
      "2023-04-17 10:54:14 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 10:54:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 10:54:14 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 6.7 | nll_loss 4.54 | ppl 23.26 | wps 6309.1 | wpb 290.8 | bsz 1 | num_updates 119 | best_loss 6.7\n",
      "2023-04-17 10:54:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 119 updates\n",
      "2023-04-17 10:54:14 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_best.pt\n",
      "2023-04-17 10:54:28 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_best.pt\n",
      "2023-04-17 10:54:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 4 @ 119 updates, score 6.7) (writing took 22.459336561965756 seconds)\n",
      "2023-04-17 10:54:36 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
      "2023-04-17 10:54:36 | INFO | train | epoch 004 | loss 9.421 | nll_loss 7.358 | ppl 164.03 | wps 617.9 | ups 0.91 | wpb 678.5 | bsz 2 | num_updates 119 | lr 7.14e-06 | gnorm 160.286 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 157\n",
      "2023-04-17 10:54:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 10:54:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 10:54:36 | INFO | fairseq.trainer | begin training epoch 5\n",
      "2023-04-17 10:54:36 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 10:54:49 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 10:54:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 10:54:49 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 5.624 | nll_loss 3.813 | ppl 14.05 | wps 6539.6 | wpb 290.8 | bsz 1 | num_updates 151 | best_loss 5.624\n",
      "2023-04-17 10:54:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 151 updates\n",
      "2023-04-17 10:54:49 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_best.pt\n",
      "2023-04-17 10:55:04 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_best.pt\n",
      "2023-04-17 10:55:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 5 @ 151 updates, score 5.624) (writing took 22.60528888006229 seconds)\n",
      "2023-04-17 10:55:12 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 10:55:12 | INFO | train | epoch 005 | loss 5.335 | nll_loss 3.223 | ppl 9.34 | wps 613.7 | ups 0.9 | wpb 678.5 | bsz 2 | num_updates 151 | lr 9.06e-06 | gnorm 37.139 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 192\n",
      "2023-04-17 10:55:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 10:55:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 10:55:12 | INFO | fairseq.trainer | begin training epoch 6\n",
      "2023-04-17 10:55:12 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 10:55:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 10:55:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 10:55:25 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 4.938 | nll_loss 3.109 | ppl 8.63 | wps 6992 | wpb 290.8 | bsz 1 | num_updates 183 | best_loss 4.938\n",
      "2023-04-17 10:55:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 183 updates\n",
      "2023-04-17 10:55:25 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_best.pt\n",
      "2023-04-17 10:55:39 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_best.pt\n",
      "2023-04-17 10:55:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 6 @ 183 updates, score 4.938) (writing took 22.07255195209291 seconds)\n",
      "2023-04-17 10:55:47 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)\n",
      "2023-04-17 10:55:47 | INFO | train | epoch 006 | loss 4.731 | nll_loss 2.716 | ppl 6.57 | wps 616.4 | ups 0.91 | wpb 678.5 | bsz 2 | num_updates 183 | lr 1.098e-05 | gnorm 25.236 | clip 100 | loss_scale 0.25 | train_wall 13 | gb_free 13.9 | wall 227\n",
      "2023-04-17 10:55:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 10:55:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 10:55:47 | INFO | fairseq.trainer | begin training epoch 7\n",
      "2023-04-17 10:55:47 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 10:55:54 | INFO | train_inner | epoch 007:     17 / 32 loss=5.493, nll_loss=3.433, ppl=10.8, wps=626.2, ups=0.93, wpb=670.3, bsz=2, num_updates=200, lr=1.2e-05, gnorm=47.351, clip=100, loss_scale=0.25, train_wall=38, gb_free=13.9, wall=234\n",
      "2023-04-17 10:55:59 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 10:55:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 10:55:59 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 4.72 | nll_loss 2.813 | ppl 7.03 | wps 6895.9 | wpb 290.8 | bsz 1 | num_updates 215 | best_loss 4.72\n",
      "2023-04-17 10:55:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 215 updates\n",
      "2023-04-17 10:55:59 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_best.pt\n",
      "2023-04-17 10:56:11 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_best.pt\n",
      "2023-04-17 10:56:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 7 @ 215 updates, score 4.72) (writing took 19.325140586937778 seconds)\n",
      "2023-04-17 10:56:19 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)\n",
      "2023-04-17 10:56:19 | INFO | train | epoch 007 | loss 4.579 | nll_loss 2.575 | ppl 5.96 | wps 689.7 | ups 1.02 | wpb 678.5 | bsz 2 | num_updates 215 | lr 1.29e-05 | gnorm 7.096 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 259\n",
      "2023-04-17 10:56:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 10:56:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 10:56:19 | INFO | fairseq.trainer | begin training epoch 8\n",
      "2023-04-17 10:56:19 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 10:56:32 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 10:56:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 10:56:32 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 4.837 | nll_loss 3.048 | ppl 8.27 | wps 6223.3 | wpb 290.8 | bsz 1 | num_updates 247 | best_loss 4.72\n",
      "2023-04-17 10:56:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 247 updates\n",
      "2023-04-17 10:56:32 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 10:56:47 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 10:56:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 8 @ 247 updates, score 4.837) (writing took 14.803539103013463 seconds)\n",
      "2023-04-17 10:56:47 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)\n",
      "2023-04-17 10:56:47 | INFO | train | epoch 008 | loss 4.505 | nll_loss 2.515 | ppl 5.72 | wps 767.4 | ups 1.13 | wpb 678.5 | bsz 2 | num_updates 247 | lr 1.482e-05 | gnorm 5.64 | clip 100 | loss_scale 0.25 | train_wall 13 | gb_free 13.9 | wall 287\n",
      "2023-04-17 10:56:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 10:56:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 10:56:47 | INFO | fairseq.trainer | begin training epoch 9\n",
      "2023-04-17 10:56:47 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 10:56:59 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 10:56:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 10:57:00 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 4.655 | nll_loss 2.78 | ppl 6.87 | wps 6120.1 | wpb 290.8 | bsz 1 | num_updates 279 | best_loss 4.655\n",
      "2023-04-17 10:57:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 279 updates\n",
      "2023-04-17 10:57:00 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_best.pt\n",
      "2023-04-17 10:57:13 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_best.pt\n",
      "2023-04-17 10:57:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 9 @ 279 updates, score 4.655) (writing took 21.313258577953093 seconds)\n",
      "2023-04-17 10:57:21 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)\n",
      "2023-04-17 10:57:21 | INFO | train | epoch 009 | loss 4.47 | nll_loss 2.486 | ppl 5.6 | wps 635.4 | ups 0.94 | wpb 678.5 | bsz 2 | num_updates 279 | lr 1.674e-05 | gnorm 7.035 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 321\n",
      "2023-04-17 10:57:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 10:57:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 10:57:21 | INFO | fairseq.trainer | begin training epoch 10\n",
      "2023-04-17 10:57:21 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 10:57:29 | INFO | train_inner | epoch 010:     21 / 32 loss=4.489, nll_loss=2.501, ppl=5.66, wps=712.7, ups=1.04, wpb=683, bsz=2, num_updates=300, lr=1.8e-05, gnorm=6.307, clip=100, loss_scale=0.25, train_wall=39, gb_free=13.9, wall=330\n",
      "2023-04-17 10:57:34 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 10:57:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 10:57:34 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 4.578 | nll_loss 2.641 | ppl 6.24 | wps 6210 | wpb 290.8 | bsz 1 | num_updates 311 | best_loss 4.578\n",
      "2023-04-17 10:57:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 311 updates\n",
      "2023-04-17 10:57:34 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_best.pt\n",
      "2023-04-17 10:57:47 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_best.pt\n",
      "2023-04-17 10:57:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 10 @ 311 updates, score 4.578) (writing took 21.57714779500384 seconds)\n",
      "2023-04-17 10:57:55 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)\n",
      "2023-04-17 10:57:55 | INFO | train | epoch 010 | loss 4.434 | nll_loss 2.456 | ppl 5.49 | wps 631.6 | ups 0.93 | wpb 678.5 | bsz 2 | num_updates 311 | lr 1.866e-05 | gnorm 4.919 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 356\n",
      "2023-04-17 10:57:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 10:57:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 10:57:55 | INFO | fairseq.trainer | begin training epoch 11\n",
      "2023-04-17 10:57:55 | INFO | fairseq_cli.train | Start iterating over samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 10:58:08 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 10:58:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 10:58:09 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 4.556 | nll_loss 2.635 | ppl 6.21 | wps 6135.9 | wpb 290.8 | bsz 1 | num_updates 343 | best_loss 4.556\n",
      "2023-04-17 10:58:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 343 updates\n",
      "2023-04-17 10:58:09 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_best.pt\n",
      "2023-04-17 10:58:22 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_best.pt\n",
      "2023-04-17 10:58:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 11 @ 343 updates, score 4.556) (writing took 21.62903234793339 seconds)\n",
      "2023-04-17 10:58:30 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)\n",
      "2023-04-17 10:58:30 | INFO | train | epoch 011 | loss 4.4 | nll_loss 2.423 | ppl 5.36 | wps 623.8 | ups 0.92 | wpb 678.5 | bsz 2 | num_updates 343 | lr 2.058e-05 | gnorm 6.78 | clip 100 | loss_scale 0.25 | train_wall 13 | gb_free 13.9 | wall 390\n",
      "2023-04-17 10:58:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 10:58:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 10:58:30 | INFO | fairseq.trainer | begin training epoch 12\n",
      "2023-04-17 10:58:30 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 10:58:43 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 10:58:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 10:58:43 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 4.469 | nll_loss 2.505 | ppl 5.68 | wps 6427.1 | wpb 290.8 | bsz 1 | num_updates 375 | best_loss 4.469\n",
      "2023-04-17 10:58:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 375 updates\n",
      "2023-04-17 10:58:43 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_best.pt\n",
      "2023-04-17 10:58:57 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_best.pt\n",
      "2023-04-17 10:59:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 12 @ 375 updates, score 4.469) (writing took 22.68222642794717 seconds)\n",
      "2023-04-17 10:59:06 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)\n",
      "2023-04-17 10:59:06 | INFO | train | epoch 012 | loss 4.38 | nll_loss 2.397 | ppl 5.27 | wps 607.8 | ups 0.9 | wpb 678.5 | bsz 2 | num_updates 375 | lr 2.25e-05 | gnorm 6.796 | clip 100 | loss_scale 0.25 | train_wall 13 | gb_free 13.9 | wall 426\n",
      "2023-04-17 10:59:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 10:59:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 10:59:06 | INFO | fairseq.trainer | begin training epoch 13\n",
      "2023-04-17 10:59:06 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 10:59:16 | INFO | train_inner | epoch 013:     25 / 32 loss=4.38, nll_loss=2.399, ppl=5.27, wps=628.4, ups=0.94, wpb=669.4, bsz=2, num_updates=400, lr=2.4e-05, gnorm=6.559, clip=100, loss_scale=0.25, train_wall=39, gb_free=13.9, wall=436\n",
      "2023-04-17 10:59:19 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 10:59:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 10:59:19 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 4.447 | nll_loss 2.504 | ppl 5.67 | wps 6195.2 | wpb 290.8 | bsz 1 | num_updates 407 | best_loss 4.447\n",
      "2023-04-17 10:59:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 407 updates\n",
      "2023-04-17 10:59:19 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_best.pt\n",
      "2023-04-17 10:59:33 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_best.pt\n",
      "2023-04-17 10:59:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 13 @ 407 updates, score 4.447) (writing took 22.68098325491883 seconds)\n",
      "2023-04-17 10:59:41 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)\n",
      "2023-04-17 10:59:41 | INFO | train | epoch 013 | loss 4.342 | nll_loss 2.361 | ppl 5.14 | wps 610.6 | ups 0.9 | wpb 678.5 | bsz 2 | num_updates 407 | lr 2.442e-05 | gnorm 6.484 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 462\n",
      "2023-04-17 10:59:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 10:59:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 10:59:41 | INFO | fairseq.trainer | begin training epoch 14\n",
      "2023-04-17 10:59:41 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 10:59:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 10:59:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 10:59:55 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 4.379 | nll_loss 2.442 | ppl 5.43 | wps 6126.5 | wpb 290.8 | bsz 1 | num_updates 439 | best_loss 4.379\n",
      "2023-04-17 10:59:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 439 updates\n",
      "2023-04-17 10:59:55 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_best.pt\n",
      "2023-04-17 11:00:09 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_best.pt\n",
      "2023-04-17 11:00:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 14 @ 439 updates, score 4.379) (writing took 22.386318473960273 seconds)\n",
      "2023-04-17 11:00:17 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)\n",
      "2023-04-17 11:00:17 | INFO | train | epoch 014 | loss 4.274 | nll_loss 2.286 | ppl 4.88 | wps 607.9 | ups 0.9 | wpb 678.5 | bsz 2 | num_updates 439 | lr 2.634e-05 | gnorm 8.658 | clip 100 | loss_scale 0.25 | train_wall 13 | gb_free 13.9 | wall 497\n",
      "2023-04-17 11:00:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:00:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:00:17 | INFO | fairseq.trainer | begin training epoch 15\n",
      "2023-04-17 11:00:17 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:00:30 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:00:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:00:30 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 4.35 | nll_loss 2.407 | ppl 5.3 | wps 6353.8 | wpb 290.8 | bsz 1 | num_updates 471 | best_loss 4.35\n",
      "2023-04-17 11:00:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 471 updates\n",
      "2023-04-17 11:00:30 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_best.pt\n",
      "2023-04-17 11:00:45 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_best.pt\n",
      "2023-04-17 11:00:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 15 @ 471 updates, score 4.35) (writing took 23.738724141963758 seconds)\n",
      "2023-04-17 11:00:54 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)\n",
      "2023-04-17 11:00:54 | INFO | train | epoch 015 | loss 4.23 | nll_loss 2.241 | ppl 4.73 | wps 594.4 | ups 0.88 | wpb 678.5 | bsz 2 | num_updates 471 | lr 2.826e-05 | gnorm 6.186 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 534\n",
      "2023-04-17 11:00:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:00:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:00:54 | INFO | fairseq.trainer | begin training epoch 16\n",
      "2023-04-17 11:00:54 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:01:05 | INFO | train_inner | epoch 016:     29 / 32 loss=4.237, nll_loss=2.25, ppl=4.76, wps=624.8, ups=0.92, wpb=682.2, bsz=2, num_updates=500, lr=3e-05, gnorm=6.611, clip=100, loss_scale=0.25, train_wall=39, gb_free=13.9, wall=545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 11:01:06 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:01:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:01:06 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 4.375 | nll_loss 2.392 | ppl 5.25 | wps 6425.9 | wpb 290.8 | bsz 1 | num_updates 503 | best_loss 4.35\n",
      "2023-04-17 11:01:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 503 updates\n",
      "2023-04-17 11:01:06 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:01:19 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:01:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 16 @ 503 updates, score 4.375) (writing took 13.033279648981988 seconds)\n",
      "2023-04-17 11:01:19 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)\n",
      "2023-04-17 11:01:19 | INFO | train | epoch 016 | loss 4.189 | nll_loss 2.201 | ppl 4.6 | wps 846.1 | ups 1.25 | wpb 678.5 | bsz 2 | num_updates 503 | lr 2.99989e-05 | gnorm 6.1 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 560\n",
      "2023-04-17 11:01:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:01:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:01:19 | INFO | fairseq.trainer | begin training epoch 17\n",
      "2023-04-17 11:01:19 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:01:31 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:01:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:01:31 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 4.248 | nll_loss 2.286 | ppl 4.88 | wps 6457.8 | wpb 290.8 | bsz 1 | num_updates 535 | best_loss 4.248\n",
      "2023-04-17 11:01:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 535 updates\n",
      "2023-04-17 11:01:31 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_best.pt\n",
      "2023-04-17 11:01:44 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_best.pt\n",
      "2023-04-17 11:01:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 17 @ 535 updates, score 4.248) (writing took 19.91550060501322 seconds)\n",
      "2023-04-17 11:01:51 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)\n",
      "2023-04-17 11:01:51 | INFO | train | epoch 017 | loss 4.147 | nll_loss 2.151 | ppl 4.44 | wps 678.6 | ups 1 | wpb 678.5 | bsz 2 | num_updates 535 | lr 2.99868e-05 | gnorm 4.756 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 592\n",
      "2023-04-17 11:01:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:01:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:01:51 | INFO | fairseq.trainer | begin training epoch 18\n",
      "2023-04-17 11:01:51 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:02:03 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:02:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:02:04 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 4.202 | nll_loss 2.264 | ppl 4.8 | wps 6746.2 | wpb 290.8 | bsz 1 | num_updates 567 | best_loss 4.202\n",
      "2023-04-17 11:02:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 567 updates\n",
      "2023-04-17 11:02:04 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_best.pt\n",
      "2023-04-17 11:02:17 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_best.pt\n",
      "2023-04-17 11:02:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 18 @ 567 updates, score 4.202) (writing took 20.623584270942956 seconds)\n",
      "2023-04-17 11:02:24 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)\n",
      "2023-04-17 11:02:24 | INFO | train | epoch 018 | loss 4.104 | nll_loss 2.11 | ppl 4.32 | wps 660.8 | ups 0.97 | wpb 678.5 | bsz 2 | num_updates 567 | lr 2.99747e-05 | gnorm 3.57 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 625\n",
      "2023-04-17 11:02:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:02:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:02:24 | INFO | fairseq.trainer | begin training epoch 19\n",
      "2023-04-17 11:02:24 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:02:37 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:02:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:02:37 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 4.172 | nll_loss 2.156 | ppl 4.46 | wps 6295.6 | wpb 290.8 | bsz 1 | num_updates 599 | best_loss 4.172\n",
      "2023-04-17 11:02:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 599 updates\n",
      "2023-04-17 11:02:37 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_best.pt\n",
      "2023-04-17 11:02:51 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_best.pt\n",
      "2023-04-17 11:02:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 19 @ 599 updates, score 4.172) (writing took 21.702022401965223 seconds)\n",
      "2023-04-17 11:02:59 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)\n",
      "2023-04-17 11:02:59 | INFO | train | epoch 019 | loss 4.069 | nll_loss 2.075 | ppl 4.21 | wps 625.1 | ups 0.92 | wpb 678.5 | bsz 2 | num_updates 599 | lr 2.99626e-05 | gnorm 6.517 | clip 100 | loss_scale 0.25 | train_wall 13 | gb_free 13.9 | wall 659\n",
      "2023-04-17 11:02:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:02:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:02:59 | INFO | fairseq.trainer | begin training epoch 20\n",
      "2023-04-17 11:02:59 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:03:00 | INFO | train_inner | epoch 020:      1 / 32 loss=4.11, nll_loss=2.115, ppl=4.33, wps=592.8, ups=0.87, wpb=678.2, bsz=2, num_updates=600, lr=2.99623e-05, gnorm=5.116, clip=100, loss_scale=0.25, train_wall=37, gb_free=13.9, wall=660\n",
      "2023-04-17 11:03:12 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:03:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:03:12 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 4.199 | nll_loss 2.201 | ppl 4.6 | wps 6051.7 | wpb 290.8 | bsz 1 | num_updates 631 | best_loss 4.172\n",
      "2023-04-17 11:03:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 631 updates\n",
      "2023-04-17 11:03:12 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:03:26 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:03:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 20 @ 631 updates, score 4.199) (writing took 13.967446647002362 seconds)\n",
      "2023-04-17 11:03:26 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)\n",
      "2023-04-17 11:03:26 | INFO | train | epoch 020 | loss 4.056 | nll_loss 2.06 | ppl 4.17 | wps 803.8 | ups 1.18 | wpb 678.5 | bsz 2 | num_updates 631 | lr 2.99506e-05 | gnorm 10.075 | clip 100 | loss_scale 0.25 | train_wall 13 | gb_free 13.9 | wall 686\n",
      "2023-04-17 11:03:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:03:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:03:26 | INFO | fairseq.trainer | begin training epoch 21\n",
      "2023-04-17 11:03:26 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:03:39 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:03:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 11:03:39 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 4.184 | nll_loss 2.216 | ppl 4.65 | wps 6486.1 | wpb 290.8 | bsz 1 | num_updates 663 | best_loss 4.172\n",
      "2023-04-17 11:03:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 663 updates\n",
      "2023-04-17 11:03:39 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:03:52 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:03:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 21 @ 663 updates, score 4.184) (writing took 13.450740476022474 seconds)\n",
      "2023-04-17 11:03:53 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)\n",
      "2023-04-17 11:03:53 | INFO | train | epoch 021 | loss 4.03 | nll_loss 2.033 | ppl 4.09 | wps 818.9 | ups 1.21 | wpb 678.5 | bsz 2 | num_updates 663 | lr 2.99385e-05 | gnorm 5.028 | clip 100 | loss_scale 0.25 | train_wall 13 | gb_free 13.9 | wall 713\n",
      "2023-04-17 11:03:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:03:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:03:53 | INFO | fairseq.trainer | begin training epoch 22\n",
      "2023-04-17 11:03:53 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:04:05 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:04:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:04:05 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 4.177 | nll_loss 2.2 | ppl 4.59 | wps 2949.7 | wpb 290.8 | bsz 1 | num_updates 695 | best_loss 4.172\n",
      "2023-04-17 11:04:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 695 updates\n",
      "2023-04-17 11:04:05 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:04:18 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:04:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 22 @ 695 updates, score 4.177) (writing took 13.213405831018463 seconds)\n",
      "2023-04-17 11:04:19 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)\n",
      "2023-04-17 11:04:19 | INFO | train | epoch 022 | loss 4.016 | nll_loss 2.022 | ppl 4.06 | wps 831.3 | ups 1.23 | wpb 678.5 | bsz 2 | num_updates 695 | lr 2.99264e-05 | gnorm 4.057 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 739\n",
      "2023-04-17 11:04:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:04:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:04:19 | INFO | fairseq.trainer | begin training epoch 23\n",
      "2023-04-17 11:04:19 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:04:21 | INFO | train_inner | epoch 023:      5 / 32 loss=4.031, nll_loss=2.036, ppl=4.1, wps=832.7, ups=1.23, wpb=675.4, bsz=2, num_updates=700, lr=2.99245e-05, gnorm=6.309, clip=100, loss_scale=0.25, train_wall=39, gb_free=13.9, wall=741\n",
      "2023-04-17 11:04:31 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:04:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:04:31 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 4.125 | nll_loss 2.132 | ppl 4.38 | wps 6256.3 | wpb 290.8 | bsz 1 | num_updates 727 | best_loss 4.125\n",
      "2023-04-17 11:04:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 727 updates\n",
      "2023-04-17 11:04:31 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_best.pt\n",
      "2023-04-17 11:04:44 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_best.pt\n",
      "2023-04-17 11:04:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 23 @ 727 updates, score 4.125) (writing took 20.854942047968507 seconds)\n",
      "2023-04-17 11:04:52 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)\n",
      "2023-04-17 11:04:52 | INFO | train | epoch 023 | loss 4.002 | nll_loss 2.01 | ppl 4.03 | wps 645.9 | ups 0.95 | wpb 678.5 | bsz 2 | num_updates 727 | lr 2.99143e-05 | gnorm 3.428 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 773\n",
      "2023-04-17 11:04:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:04:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:04:52 | INFO | fairseq.trainer | begin training epoch 24\n",
      "2023-04-17 11:04:52 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:05:05 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:05:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:05:05 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 4.122 | nll_loss 2.142 | ppl 4.41 | wps 6420.1 | wpb 290.8 | bsz 1 | num_updates 759 | best_loss 4.122\n",
      "2023-04-17 11:05:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 759 updates\n",
      "2023-04-17 11:05:05 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_best.pt\n",
      "2023-04-17 11:05:18 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_best.pt\n",
      "2023-04-17 11:05:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 24 @ 759 updates, score 4.122) (writing took 21.94357746304013 seconds)\n",
      "2023-04-17 11:05:27 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)\n",
      "2023-04-17 11:05:27 | INFO | train | epoch 024 | loss 3.993 | nll_loss 1.998 | ppl 4 | wps 627.8 | ups 0.93 | wpb 678.5 | bsz 2 | num_updates 759 | lr 2.99023e-05 | gnorm 3.523 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 807\n",
      "2023-04-17 11:05:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:05:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:05:27 | INFO | fairseq.trainer | begin training epoch 25\n",
      "2023-04-17 11:05:27 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:05:39 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:05:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:05:39 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 4.111 | nll_loss 2.132 | ppl 4.38 | wps 6433.9 | wpb 290.8 | bsz 1 | num_updates 791 | best_loss 4.111\n",
      "2023-04-17 11:05:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 791 updates\n",
      "2023-04-17 11:05:39 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_best.pt\n",
      "2023-04-17 11:05:52 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_best.pt\n",
      "2023-04-17 11:06:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 25 @ 791 updates, score 4.111) (writing took 21.025490127969533 seconds)\n",
      "2023-04-17 11:06:00 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)\n",
      "2023-04-17 11:06:00 | INFO | train | epoch 025 | loss 3.979 | nll_loss 1.983 | ppl 3.95 | wps 647.7 | ups 0.95 | wpb 678.5 | bsz 2 | num_updates 791 | lr 2.98902e-05 | gnorm 3.082 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 841\n",
      "2023-04-17 11:06:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:06:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:06:00 | INFO | fairseq.trainer | begin training epoch 26\n",
      "2023-04-17 11:06:00 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:06:04 | INFO | train_inner | epoch 026:      9 / 32 loss=3.992, nll_loss=1.999, ppl=4, wps=665.5, ups=0.97, wpb=688.3, bsz=2, num_updates=800, lr=2.98868e-05, gnorm=3.199, clip=100, loss_scale=0.25, train_wall=38, gb_free=13.9, wall=844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 11:06:14 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:06:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:06:14 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 4.102 | nll_loss 2.12 | ppl 4.35 | wps 6420.2 | wpb 290.8 | bsz 1 | num_updates 823 | best_loss 4.102\n",
      "2023-04-17 11:06:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 823 updates\n",
      "2023-04-17 11:06:14 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_best.pt\n",
      "2023-04-17 11:06:27 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_best.pt\n",
      "2023-04-17 11:06:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 26 @ 823 updates, score 4.102) (writing took 21.38138116407208 seconds)\n",
      "2023-04-17 11:06:35 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)\n",
      "2023-04-17 11:06:35 | INFO | train | epoch 026 | loss 3.963 | nll_loss 1.969 | ppl 3.92 | wps 619.5 | ups 0.91 | wpb 678.5 | bsz 2 | num_updates 823 | lr 2.98781e-05 | gnorm 2.243 | clip 100 | loss_scale 0.25 | train_wall 13 | gb_free 13.9 | wall 876\n",
      "2023-04-17 11:06:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:06:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:06:35 | INFO | fairseq.trainer | begin training epoch 27\n",
      "2023-04-17 11:06:35 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:06:48 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:06:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:06:48 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 4.103 | nll_loss 2.125 | ppl 4.36 | wps 6296.7 | wpb 290.8 | bsz 1 | num_updates 855 | best_loss 4.102\n",
      "2023-04-17 11:06:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 855 updates\n",
      "2023-04-17 11:06:48 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:07:01 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:07:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 27 @ 855 updates, score 4.103) (writing took 13.564731803024188 seconds)\n",
      "2023-04-17 11:07:02 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)\n",
      "2023-04-17 11:07:02 | INFO | train | epoch 027 | loss 3.957 | nll_loss 1.966 | ppl 3.91 | wps 827.4 | ups 1.22 | wpb 678.5 | bsz 2 | num_updates 855 | lr 2.9866e-05 | gnorm 2.3 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 902\n",
      "2023-04-17 11:07:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:07:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:07:02 | INFO | fairseq.trainer | begin training epoch 28\n",
      "2023-04-17 11:07:02 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:07:15 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:07:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:07:15 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 5.525 | nll_loss 3.404 | ppl 10.59 | wps 6480.2 | wpb 290.8 | bsz 1 | num_updates 887 | best_loss 4.102\n",
      "2023-04-17 11:07:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 887 updates\n",
      "2023-04-17 11:07:15 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:07:28 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:07:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 28 @ 887 updates, score 5.525) (writing took 13.07166147895623 seconds)\n",
      "2023-04-17 11:07:28 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)\n",
      "2023-04-17 11:07:28 | INFO | train | epoch 028 | loss 3.977 | nll_loss 1.984 | ppl 3.95 | wps 827 | ups 1.22 | wpb 678.5 | bsz 2 | num_updates 887 | lr 2.9854e-05 | gnorm 2.878 | clip 100 | loss_scale 0.25 | train_wall 13 | gb_free 13.9 | wall 928\n",
      "2023-04-17 11:07:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:07:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:07:28 | INFO | fairseq.trainer | begin training epoch 29\n",
      "2023-04-17 11:07:28 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:07:33 | INFO | train_inner | epoch 029:     13 / 32 loss=4.339, nll_loss=2.304, ppl=4.94, wps=761.7, ups=1.12, wpb=679.5, bsz=2, num_updates=900, lr=2.98491e-05, gnorm=18.423, clip=100, loss_scale=0.25, train_wall=40, gb_free=13.9, wall=934\n",
      "2023-04-17 11:07:41 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:07:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:07:41 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 13.226 | nll_loss 9.764 | ppl 869.48 | wps 6239 | wpb 290.8 | bsz 1 | num_updates 919 | best_loss 4.102\n",
      "2023-04-17 11:07:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 919 updates\n",
      "2023-04-17 11:07:41 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:07:54 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:07:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 29 @ 919 updates, score 13.226) (writing took 13.4039646940073 seconds)\n",
      "2023-04-17 11:07:54 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)\n",
      "2023-04-17 11:07:54 | INFO | train | epoch 029 | loss 10.345 | nll_loss 7.508 | ppl 182.08 | wps 823.2 | ups 1.21 | wpb 678.5 | bsz 2 | num_updates 919 | lr 2.98419e-05 | gnorm 91.81 | clip 100 | loss_scale 0.25 | train_wall 13 | gb_free 13.9 | wall 955\n",
      "2023-04-17 11:07:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:07:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:07:54 | INFO | fairseq.trainer | begin training epoch 30\n",
      "2023-04-17 11:07:54 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:08:06 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:08:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:08:07 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 4.361 | nll_loss 2.362 | ppl 5.14 | wps 6438.7 | wpb 290.8 | bsz 1 | num_updates 951 | best_loss 4.102\n",
      "2023-04-17 11:08:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 951 updates\n",
      "2023-04-17 11:08:07 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:08:19 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:08:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 30 @ 951 updates, score 4.361) (writing took 12.794079693034291 seconds)\n",
      "2023-04-17 11:08:19 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)\n",
      "2023-04-17 11:08:19 | INFO | train | epoch 030 | loss 6.014 | nll_loss 3.666 | ppl 12.69 | wps 862.8 | ups 1.27 | wpb 678.5 | bsz 2 | num_updates 951 | lr 2.98298e-05 | gnorm 67.961 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 980\n",
      "2023-04-17 11:08:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:08:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:08:19 | INFO | fairseq.trainer | begin training epoch 31\n",
      "2023-04-17 11:08:19 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:08:32 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:08:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 11:08:32 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 4.127 | nll_loss 2.162 | ppl 4.47 | wps 6431.2 | wpb 290.8 | bsz 1 | num_updates 983 | best_loss 4.102\n",
      "2023-04-17 11:08:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 983 updates\n",
      "2023-04-17 11:08:32 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:08:44 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:08:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 31 @ 983 updates, score 4.127) (writing took 12.58669671905227 seconds)\n",
      "2023-04-17 11:08:45 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)\n",
      "2023-04-17 11:08:45 | INFO | train | epoch 031 | loss 4.007 | nll_loss 2.011 | ppl 4.03 | wps 865.5 | ups 1.28 | wpb 678.5 | bsz 2 | num_updates 983 | lr 2.98177e-05 | gnorm 9.549 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 1005\n",
      "2023-04-17 11:08:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:08:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:08:45 | INFO | fairseq.trainer | begin training epoch 32\n",
      "2023-04-17 11:08:45 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:08:51 | INFO | train_inner | epoch 032:     17 / 32 loss=6.339, nll_loss=3.997, ppl=15.97, wps=855.2, ups=1.28, wpb=666.4, bsz=2, num_updates=1000, lr=2.98113e-05, gnorm=38.344, clip=100, loss_scale=0.25, train_wall=38, gb_free=13.9, wall=1011\n",
      "2023-04-17 11:08:57 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:08:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:08:57 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 4.128 | nll_loss 2.146 | ppl 4.43 | wps 6443.1 | wpb 290.8 | bsz 1 | num_updates 1015 | best_loss 4.102\n",
      "2023-04-17 11:08:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 1015 updates\n",
      "2023-04-17 11:08:57 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:09:10 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:09:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 32 @ 1015 updates, score 4.128) (writing took 13.360893888049759 seconds)\n",
      "2023-04-17 11:09:11 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)\n",
      "2023-04-17 11:09:11 | INFO | train | epoch 032 | loss 3.955 | nll_loss 1.965 | ppl 3.9 | wps 836.7 | ups 1.23 | wpb 678.5 | bsz 2 | num_updates 1015 | lr 2.98057e-05 | gnorm 2.362 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 1031\n",
      "2023-04-17 11:09:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:09:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:09:11 | INFO | fairseq.trainer | begin training epoch 33\n",
      "2023-04-17 11:09:11 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:09:22 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:09:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:09:23 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 4.1 | nll_loss 2.182 | ppl 4.54 | wps 6543.9 | wpb 290.8 | bsz 1 | num_updates 1047 | best_loss 4.1\n",
      "2023-04-17 11:09:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 1047 updates\n",
      "2023-04-17 11:09:23 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_best.pt\n",
      "2023-04-17 11:09:35 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_best.pt\n",
      "2023-04-17 11:09:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 33 @ 1047 updates, score 4.1) (writing took 20.91385175602045 seconds)\n",
      "2023-04-17 11:09:43 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)\n",
      "2023-04-17 11:09:44 | INFO | train | epoch 033 | loss 3.966 | nll_loss 1.974 | ppl 3.93 | wps 658 | ups 0.97 | wpb 678.5 | bsz 2 | num_updates 1047 | lr 2.97936e-05 | gnorm 4.195 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 1064\n",
      "2023-04-17 11:09:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:09:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:09:44 | INFO | fairseq.trainer | begin training epoch 34\n",
      "2023-04-17 11:09:44 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:09:56 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:09:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:09:56 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 4.108 | nll_loss 2.137 | ppl 4.4 | wps 6407.3 | wpb 290.8 | bsz 1 | num_updates 1079 | best_loss 4.1\n",
      "2023-04-17 11:09:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 1079 updates\n",
      "2023-04-17 11:09:56 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:10:09 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:10:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 34 @ 1079 updates, score 4.108) (writing took 13.619766287971288 seconds)\n",
      "2023-04-17 11:10:10 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)\n",
      "2023-04-17 11:10:10 | INFO | train | epoch 034 | loss 3.944 | nll_loss 1.956 | ppl 3.88 | wps 834.2 | ups 1.23 | wpb 678.5 | bsz 2 | num_updates 1079 | lr 2.97815e-05 | gnorm 3.107 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 1090\n",
      "2023-04-17 11:10:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:10:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:10:10 | INFO | fairseq.trainer | begin training epoch 35\n",
      "2023-04-17 11:10:10 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:10:18 | INFO | train_inner | epoch 035:     21 / 32 loss=3.953, nll_loss=1.964, ppl=3.9, wps=783.4, ups=1.15, wpb=681, bsz=2, num_updates=1100, lr=2.97736e-05, gnorm=3.409, clip=100, loss_scale=0.25, train_wall=38, gb_free=13.9, wall=1098\n",
      "2023-04-17 11:10:22 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:10:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:10:23 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 4.106 | nll_loss 2.135 | ppl 4.39 | wps 6226.5 | wpb 290.8 | bsz 1 | num_updates 1111 | best_loss 4.1\n",
      "2023-04-17 11:10:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 1111 updates\n",
      "2023-04-17 11:10:23 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:10:36 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:10:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 35 @ 1111 updates, score 4.106) (writing took 13.242958711925894 seconds)\n",
      "2023-04-17 11:10:36 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)\n",
      "2023-04-17 11:10:36 | INFO | train | epoch 035 | loss 3.948 | nll_loss 1.957 | ppl 3.88 | wps 822.3 | ups 1.21 | wpb 678.5 | bsz 2 | num_updates 1111 | lr 2.97694e-05 | gnorm 3.558 | clip 100 | loss_scale 0.25 | train_wall 13 | gb_free 13.9 | wall 1116\n",
      "2023-04-17 11:10:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:10:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:10:36 | INFO | fairseq.trainer | begin training epoch 36\n",
      "2023-04-17 11:10:36 | INFO | fairseq_cli.train | Start iterating over samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 11:10:49 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:10:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:10:49 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 4.126 | nll_loss 2.165 | ppl 4.49 | wps 5062.4 | wpb 290.8 | bsz 1 | num_updates 1143 | best_loss 4.1\n",
      "2023-04-17 11:10:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 1143 updates\n",
      "2023-04-17 11:10:49 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:11:03 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:11:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 36 @ 1143 updates, score 4.126) (writing took 14.022493703989312 seconds)\n",
      "2023-04-17 11:11:03 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)\n",
      "2023-04-17 11:11:03 | INFO | train | epoch 036 | loss 3.93 | nll_loss 1.938 | ppl 3.83 | wps 791.5 | ups 1.17 | wpb 678.5 | bsz 2 | num_updates 1143 | lr 2.97574e-05 | gnorm 2.8 | clip 100 | loss_scale 0.25 | train_wall 13 | gb_free 13.9 | wall 1144\n",
      "2023-04-17 11:11:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:11:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:11:03 | INFO | fairseq.trainer | begin training epoch 37\n",
      "2023-04-17 11:11:03 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:11:17 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:11:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:11:17 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 4.151 | nll_loss 2.177 | ppl 4.52 | wps 6424.9 | wpb 290.8 | bsz 1 | num_updates 1175 | best_loss 4.1\n",
      "2023-04-17 11:11:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 1175 updates\n",
      "2023-04-17 11:11:17 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:11:30 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:11:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 37 @ 1175 updates, score 4.151) (writing took 13.450739630963653 seconds)\n",
      "2023-04-17 11:11:30 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)\n",
      "2023-04-17 11:11:30 | INFO | train | epoch 037 | loss 3.927 | nll_loss 1.936 | ppl 3.83 | wps 800.8 | ups 1.18 | wpb 678.5 | bsz 2 | num_updates 1175 | lr 2.97453e-05 | gnorm 2.02 | clip 100 | loss_scale 0.25 | train_wall 13 | gb_free 13.9 | wall 1171\n",
      "2023-04-17 11:11:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:11:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:11:30 | INFO | fairseq.trainer | begin training epoch 38\n",
      "2023-04-17 11:11:30 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:11:42 | INFO | train_inner | epoch 038:     25 / 32 loss=3.934, nll_loss=1.944, ppl=3.85, wps=825.2, ups=1.2, wpb=689.2, bsz=2, num_updates=1200, lr=2.97358e-05, gnorm=2.525, clip=100, loss_scale=0.25, train_wall=41, gb_free=13.9, wall=1182\n",
      "2023-04-17 11:11:44 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:11:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:11:45 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 4.08 | nll_loss 2.133 | ppl 4.39 | wps 6008 | wpb 290.8 | bsz 1 | num_updates 1207 | best_loss 4.08\n",
      "2023-04-17 11:11:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 1207 updates\n",
      "2023-04-17 11:11:45 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_best.pt\n",
      "2023-04-17 11:11:58 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_best.pt\n",
      "2023-04-17 11:12:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 38 @ 1207 updates, score 4.08) (writing took 24.350123669952154 seconds)\n",
      "2023-04-17 11:12:09 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)\n",
      "2023-04-17 11:12:09 | INFO | train | epoch 038 | loss 3.928 | nll_loss 1.94 | ppl 3.84 | wps 563.3 | ups 0.83 | wpb 678.5 | bsz 2 | num_updates 1207 | lr 2.97332e-05 | gnorm 2.274 | clip 100 | loss_scale 0.25 | train_wall 14 | gb_free 13.9 | wall 1209\n",
      "2023-04-17 11:12:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:12:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:12:09 | INFO | fairseq.trainer | begin training epoch 39\n",
      "2023-04-17 11:12:09 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:12:22 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:12:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:12:22 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 4.076 | nll_loss 2.11 | ppl 4.32 | wps 5669 | wpb 290.8 | bsz 1 | num_updates 1239 | best_loss 4.076\n",
      "2023-04-17 11:12:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 1239 updates\n",
      "2023-04-17 11:12:22 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_best.pt\n",
      "2023-04-17 11:12:36 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_best.pt\n",
      "2023-04-17 11:12:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 39 @ 1239 updates, score 4.076) (writing took 22.734688627067953 seconds)\n",
      "2023-04-17 11:12:45 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)\n",
      "2023-04-17 11:12:45 | INFO | train | epoch 039 | loss 3.916 | nll_loss 1.927 | ppl 3.8 | wps 603 | ups 0.89 | wpb 678.5 | bsz 2 | num_updates 1239 | lr 2.97211e-05 | gnorm 1.93 | clip 100 | loss_scale 0.25 | train_wall 13 | gb_free 13.9 | wall 1245\n",
      "2023-04-17 11:12:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:12:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:12:45 | INFO | fairseq.trainer | begin training epoch 40\n",
      "2023-04-17 11:12:45 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:12:59 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:12:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:12:59 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 4.07 | nll_loss 2.071 | ppl 4.2 | wps 5440.4 | wpb 290.8 | bsz 1 | num_updates 1271 | best_loss 4.07\n",
      "2023-04-17 11:12:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 1271 updates\n",
      "2023-04-17 11:12:59 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_best.pt\n",
      "2023-04-17 11:13:12 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_best.pt\n",
      "2023-04-17 11:13:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 40 @ 1271 updates, score 4.07) (writing took 26.497331263963133 seconds)\n",
      "2023-04-17 11:13:25 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)\n",
      "2023-04-17 11:13:25 | INFO | train | epoch 040 | loss 3.918 | nll_loss 1.928 | ppl 3.8 | wps 538.2 | ups 0.79 | wpb 678.5 | bsz 2 | num_updates 1271 | lr 2.97091e-05 | gnorm 2.2 | clip 100 | loss_scale 0.25 | train_wall 13 | gb_free 13.9 | wall 1286\n",
      "2023-04-17 11:13:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:13:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:13:25 | INFO | fairseq.trainer | begin training epoch 41\n",
      "2023-04-17 11:13:25 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:13:37 | INFO | train_inner | epoch 041:     29 / 32 loss=3.915, nll_loss=1.925, ppl=3.8, wps=581.7, ups=0.87, wpb=671.9, bsz=2, num_updates=1300, lr=2.96981e-05, gnorm=2.297, clip=100, loss_scale=0.25, train_wall=40, gb_free=13.9, wall=1297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 11:13:38 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:13:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:13:38 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 4.088 | nll_loss 2.124 | ppl 4.36 | wps 6967.4 | wpb 290.8 | bsz 1 | num_updates 1303 | best_loss 4.07\n",
      "2023-04-17 11:13:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 1303 updates\n",
      "2023-04-17 11:13:38 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:13:52 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:13:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 41 @ 1303 updates, score 4.088) (writing took 13.689567137975246 seconds)\n",
      "2023-04-17 11:13:52 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)\n",
      "2023-04-17 11:13:52 | INFO | train | epoch 041 | loss 3.919 | nll_loss 1.926 | ppl 3.8 | wps 811.2 | ups 1.2 | wpb 678.5 | bsz 2 | num_updates 1303 | lr 2.9697e-05 | gnorm 2.911 | clip 100 | loss_scale 0.25 | train_wall 13 | gb_free 13.9 | wall 1312\n",
      "2023-04-17 11:13:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:13:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:13:52 | INFO | fairseq.trainer | begin training epoch 42\n",
      "2023-04-17 11:13:52 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:14:05 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:14:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:14:05 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 4.101 | nll_loss 2.138 | ppl 4.4 | wps 6224.8 | wpb 290.8 | bsz 1 | num_updates 1335 | best_loss 4.07\n",
      "2023-04-17 11:14:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 1335 updates\n",
      "2023-04-17 11:14:05 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:14:18 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:14:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 42 @ 1335 updates, score 4.101) (writing took 12.734587184968404 seconds)\n",
      "2023-04-17 11:14:18 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)\n",
      "2023-04-17 11:14:18 | INFO | train | epoch 042 | loss 3.911 | nll_loss 1.921 | ppl 3.79 | wps 847.2 | ups 1.25 | wpb 678.5 | bsz 2 | num_updates 1335 | lr 2.96849e-05 | gnorm 2.224 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 1338\n",
      "2023-04-17 11:14:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:14:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:14:18 | INFO | fairseq.trainer | begin training epoch 43\n",
      "2023-04-17 11:14:18 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:14:31 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:14:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:14:31 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 4.093 | nll_loss 2.114 | ppl 4.33 | wps 5723.9 | wpb 290.8 | bsz 1 | num_updates 1367 | best_loss 4.07\n",
      "2023-04-17 11:14:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 1367 updates\n",
      "2023-04-17 11:14:31 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:14:44 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:14:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 43 @ 1367 updates, score 4.093) (writing took 13.307626230991445 seconds)\n",
      "2023-04-17 11:14:44 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)\n",
      "2023-04-17 11:14:44 | INFO | train | epoch 043 | loss 3.93 | nll_loss 1.928 | ppl 3.81 | wps 818.3 | ups 1.21 | wpb 678.5 | bsz 2 | num_updates 1367 | lr 2.96728e-05 | gnorm 3.397 | clip 100 | loss_scale 0.25 | train_wall 13 | gb_free 13.9 | wall 1365\n",
      "2023-04-17 11:14:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:14:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:14:44 | INFO | fairseq.trainer | begin training epoch 44\n",
      "2023-04-17 11:14:44 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:14:57 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:14:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:14:58 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 4.061 | nll_loss 2.104 | ppl 4.3 | wps 6010 | wpb 290.8 | bsz 1 | num_updates 1399 | best_loss 4.061\n",
      "2023-04-17 11:14:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 1399 updates\n",
      "2023-04-17 11:14:58 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_best.pt\n",
      "2023-04-17 11:15:11 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_best.pt\n",
      "2023-04-17 11:15:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 44 @ 1399 updates, score 4.061) (writing took 22.054456550977193 seconds)\n",
      "2023-04-17 11:15:20 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)\n",
      "2023-04-17 11:15:20 | INFO | train | epoch 044 | loss 3.919 | nll_loss 1.92 | ppl 3.79 | wps 613.5 | ups 0.9 | wpb 678.5 | bsz 2 | num_updates 1399 | lr 2.96608e-05 | gnorm 2.714 | clip 100 | loss_scale 0.25 | train_wall 13 | gb_free 13.9 | wall 1400\n",
      "2023-04-17 11:15:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:15:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:15:20 | INFO | fairseq.trainer | begin training epoch 45\n",
      "2023-04-17 11:15:20 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:15:20 | INFO | train_inner | epoch 045:      1 / 32 loss=3.923, nll_loss=1.925, ppl=3.8, wps=657.4, ups=0.97, wpb=677.3, bsz=2, num_updates=1400, lr=2.96604e-05, gnorm=2.803, clip=100, loss_scale=0.25, train_wall=39, gb_free=13.9, wall=1400\n",
      "2023-04-17 11:15:33 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:15:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:15:33 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 4.073 | nll_loss 2.093 | ppl 4.27 | wps 5391 | wpb 290.8 | bsz 1 | num_updates 1431 | best_loss 4.061\n",
      "2023-04-17 11:15:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 1431 updates\n",
      "2023-04-17 11:15:33 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:15:47 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:15:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 45 @ 1431 updates, score 4.073) (writing took 13.920100476942025 seconds)\n",
      "2023-04-17 11:15:47 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)\n",
      "2023-04-17 11:15:47 | INFO | train | epoch 045 | loss 3.916 | nll_loss 1.921 | ppl 3.79 | wps 787.3 | ups 1.16 | wpb 678.5 | bsz 2 | num_updates 1431 | lr 2.96487e-05 | gnorm 2.886 | clip 100 | loss_scale 0.25 | train_wall 13 | gb_free 13.9 | wall 1428\n",
      "2023-04-17 11:15:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:15:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:15:47 | INFO | fairseq.trainer | begin training epoch 46\n",
      "2023-04-17 11:15:47 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:16:00 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:16:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 11:16:01 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 4.098 | nll_loss 2.107 | ppl 4.31 | wps 5708.2 | wpb 290.8 | bsz 1 | num_updates 1463 | best_loss 4.061\n",
      "2023-04-17 11:16:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 1463 updates\n",
      "2023-04-17 11:16:01 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:16:14 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:16:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 46 @ 1463 updates, score 4.098) (writing took 13.543129761936143 seconds)\n",
      "2023-04-17 11:16:14 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)\n",
      "2023-04-17 11:16:14 | INFO | train | epoch 046 | loss 3.915 | nll_loss 1.912 | ppl 3.76 | wps 804.8 | ups 1.19 | wpb 678.5 | bsz 2 | num_updates 1463 | lr 2.96366e-05 | gnorm 4.557 | clip 100 | loss_scale 0.25 | train_wall 13 | gb_free 13.9 | wall 1455\n",
      "2023-04-17 11:16:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:16:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:16:14 | INFO | fairseq.trainer | begin training epoch 47\n",
      "2023-04-17 11:16:14 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:16:28 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:16:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:16:28 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 4.071 | nll_loss 2.106 | ppl 4.31 | wps 5265.3 | wpb 290.8 | bsz 1 | num_updates 1495 | best_loss 4.061\n",
      "2023-04-17 11:16:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 1495 updates\n",
      "2023-04-17 11:16:28 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:16:42 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:16:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 47 @ 1495 updates, score 4.071) (writing took 13.958039526012726 seconds)\n",
      "2023-04-17 11:16:42 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)\n",
      "2023-04-17 11:16:42 | INFO | train | epoch 047 | loss 3.898 | nll_loss 1.902 | ppl 3.74 | wps 774.4 | ups 1.14 | wpb 678.5 | bsz 2 | num_updates 1495 | lr 2.96245e-05 | gnorm 2.294 | clip 100 | loss_scale 0.25 | train_wall 14 | gb_free 13.9 | wall 1483\n",
      "2023-04-17 11:16:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:16:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:16:42 | INFO | fairseq.trainer | begin training epoch 48\n",
      "2023-04-17 11:16:42 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:16:44 | INFO | train_inner | epoch 048:      5 / 32 loss=3.908, nll_loss=1.909, ppl=3.76, wps=793.4, ups=1.19, wpb=668.7, bsz=2, num_updates=1500, lr=2.96226e-05, gnorm=3.206, clip=100, loss_scale=0.25, train_wall=41, gb_free=13.9, wall=1485\n",
      "2023-04-17 11:16:56 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:16:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:16:56 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 4.071 | nll_loss 2.115 | ppl 4.33 | wps 6493.7 | wpb 290.8 | bsz 1 | num_updates 1527 | best_loss 4.061\n",
      "2023-04-17 11:16:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 1527 updates\n",
      "2023-04-17 11:16:56 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:17:11 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:17:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 48 @ 1527 updates, score 4.071) (writing took 14.768747127032839 seconds)\n",
      "2023-04-17 11:17:11 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)\n",
      "2023-04-17 11:17:11 | INFO | train | epoch 048 | loss 3.888 | nll_loss 1.896 | ppl 3.72 | wps 755 | ups 1.11 | wpb 678.5 | bsz 2 | num_updates 1527 | lr 2.96125e-05 | gnorm 1.983 | clip 100 | loss_scale 0.25 | train_wall 13 | gb_free 13.9 | wall 1511\n",
      "2023-04-17 11:17:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:17:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:17:11 | INFO | fairseq.trainer | begin training epoch 49\n",
      "2023-04-17 11:17:11 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:17:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:17:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:17:25 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 4.046 | nll_loss 2.079 | ppl 4.22 | wps 5608.7 | wpb 290.8 | bsz 1 | num_updates 1559 | best_loss 4.046\n",
      "2023-04-17 11:17:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 1559 updates\n",
      "2023-04-17 11:17:25 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_best.pt\n",
      "2023-04-17 11:17:40 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_best.pt\n",
      "2023-04-17 11:17:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 49 @ 1559 updates, score 4.046) (writing took 24.31107382499613 seconds)\n",
      "2023-04-17 11:17:49 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)\n",
      "2023-04-17 11:17:49 | INFO | train | epoch 049 | loss 3.884 | nll_loss 1.893 | ppl 3.71 | wps 566.4 | ups 0.83 | wpb 678.5 | bsz 2 | num_updates 1559 | lr 2.96004e-05 | gnorm 2.278 | clip 100 | loss_scale 0.25 | train_wall 14 | gb_free 13.9 | wall 1550\n",
      "2023-04-17 11:17:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:17:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:17:49 | INFO | fairseq.trainer | begin training epoch 50\n",
      "2023-04-17 11:17:49 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:18:02 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:18:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:18:03 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 4.095 | nll_loss 2.118 | ppl 4.34 | wps 5258.3 | wpb 290.8 | bsz 1 | num_updates 1591 | best_loss 4.046\n",
      "2023-04-17 11:18:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 1591 updates\n",
      "2023-04-17 11:18:03 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:18:17 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:18:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 50 @ 1591 updates, score 4.095) (writing took 14.272648119018413 seconds)\n",
      "2023-04-17 11:18:17 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)\n",
      "2023-04-17 11:18:17 | INFO | train | epoch 050 | loss 3.886 | nll_loss 1.894 | ppl 3.72 | wps 787.7 | ups 1.16 | wpb 678.5 | bsz 2 | num_updates 1591 | lr 2.95883e-05 | gnorm 2.654 | clip 100 | loss_scale 0.25 | train_wall 13 | gb_free 13.9 | wall 1577\n",
      "2023-04-17 11:18:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:18:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:18:17 | INFO | fairseq.trainer | begin training epoch 51\n",
      "2023-04-17 11:18:17 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:18:21 | INFO | train_inner | epoch 051:      9 / 32 loss=3.885, nll_loss=1.894, ppl=3.72, wps=714.6, ups=1.04, wpb=687.9, bsz=2, num_updates=1600, lr=2.95849e-05, gnorm=2.472, clip=100, loss_scale=0.25, train_wall=41, gb_free=13.9, wall=1581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 11:18:30 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:18:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:18:31 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 4.045 | nll_loss 2.086 | ppl 4.25 | wps 5068.5 | wpb 290.8 | bsz 1 | num_updates 1623 | best_loss 4.045\n",
      "2023-04-17 11:18:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 1623 updates\n",
      "2023-04-17 11:18:31 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_best.pt\n",
      "2023-04-17 11:18:45 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_best.pt\n",
      "2023-04-17 11:18:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 51 @ 1623 updates, score 4.045) (writing took 23.93973223795183 seconds)\n",
      "2023-04-17 11:18:55 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)\n",
      "2023-04-17 11:18:55 | INFO | train | epoch 051 | loss 3.88 | nll_loss 1.887 | ppl 3.7 | wps 575.9 | ups 0.85 | wpb 678.5 | bsz 2 | num_updates 1623 | lr 2.95762e-05 | gnorm 2.937 | clip 100 | loss_scale 0.25 | train_wall 13 | gb_free 13.9 | wall 1615\n",
      "2023-04-17 11:18:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:18:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:18:55 | INFO | fairseq.trainer | begin training epoch 52\n",
      "2023-04-17 11:18:55 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:19:08 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:19:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:19:08 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 4.074 | nll_loss 2.124 | ppl 4.36 | wps 5334 | wpb 290.8 | bsz 1 | num_updates 1655 | best_loss 4.045\n",
      "2023-04-17 11:19:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 1655 updates\n",
      "2023-04-17 11:19:08 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:19:23 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:19:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 52 @ 1655 updates, score 4.074) (writing took 14.98329103202559 seconds)\n",
      "2023-04-17 11:19:23 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)\n",
      "2023-04-17 11:19:23 | INFO | train | epoch 052 | loss 3.876 | nll_loss 1.884 | ppl 3.69 | wps 763.8 | ups 1.13 | wpb 678.5 | bsz 2 | num_updates 1655 | lr 2.95642e-05 | gnorm 2.385 | clip 100 | loss_scale 0.25 | train_wall 13 | gb_free 13.9 | wall 1643\n",
      "2023-04-17 11:19:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:19:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:19:23 | INFO | fairseq.trainer | begin training epoch 53\n",
      "2023-04-17 11:19:23 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:19:36 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:19:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:19:37 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 4.048 | nll_loss 2.076 | ppl 4.22 | wps 6100.2 | wpb 290.8 | bsz 1 | num_updates 1687 | best_loss 4.045\n",
      "2023-04-17 11:19:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 1687 updates\n",
      "2023-04-17 11:19:37 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:19:51 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:19:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 53 @ 1687 updates, score 4.048) (writing took 14.611476439982653 seconds)\n",
      "2023-04-17 11:19:51 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)\n",
      "2023-04-17 11:19:51 | INFO | train | epoch 053 | loss 3.867 | nll_loss 1.876 | ppl 3.67 | wps 773 | ups 1.14 | wpb 678.5 | bsz 2 | num_updates 1687 | lr 2.95521e-05 | gnorm 2.307 | clip 100 | loss_scale 0.25 | train_wall 13 | gb_free 13.9 | wall 1671\n",
      "2023-04-17 11:19:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:19:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:19:51 | INFO | fairseq.trainer | begin training epoch 54\n",
      "2023-04-17 11:19:51 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:19:57 | INFO | train_inner | epoch 054:     13 / 32 loss=3.87, nll_loss=1.876, ppl=3.67, wps=700.4, ups=1.04, wpb=673.3, bsz=2, num_updates=1700, lr=2.95472e-05, gnorm=2.457, clip=100, loss_scale=0.25, train_wall=41, gb_free=13.9, wall=1677\n",
      "2023-04-17 11:20:05 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:20:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:20:05 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 4.031 | nll_loss 2.055 | ppl 4.16 | wps 5652.6 | wpb 290.8 | bsz 1 | num_updates 1719 | best_loss 4.031\n",
      "2023-04-17 11:20:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 1719 updates\n",
      "2023-04-17 11:20:05 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_best.pt\n",
      "2023-04-17 11:20:18 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_best.pt\n",
      "2023-04-17 11:20:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 54 @ 1719 updates, score 4.031) (writing took 22.522739583044313 seconds)\n",
      "2023-04-17 11:20:27 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)\n",
      "2023-04-17 11:20:27 | INFO | train | epoch 054 | loss 3.868 | nll_loss 1.874 | ppl 3.67 | wps 600.6 | ups 0.89 | wpb 678.5 | bsz 2 | num_updates 1719 | lr 2.954e-05 | gnorm 3.009 | clip 100 | loss_scale 0.25 | train_wall 13 | gb_free 13.9 | wall 1708\n",
      "2023-04-17 11:20:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:20:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:20:27 | INFO | fairseq.trainer | begin training epoch 55\n",
      "2023-04-17 11:20:27 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:20:40 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:20:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:20:40 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 4.041 | nll_loss 2.072 | ppl 4.2 | wps 5699.2 | wpb 290.8 | bsz 1 | num_updates 1751 | best_loss 4.031\n",
      "2023-04-17 11:20:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 1751 updates\n",
      "2023-04-17 11:20:40 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:20:55 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:20:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 55 @ 1751 updates, score 4.041) (writing took 15.44681741495151 seconds)\n",
      "2023-04-17 11:20:56 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)\n",
      "2023-04-17 11:20:56 | INFO | train | epoch 055 | loss 3.852 | nll_loss 1.857 | ppl 3.62 | wps 771.1 | ups 1.14 | wpb 678.5 | bsz 2 | num_updates 1751 | lr 2.95279e-05 | gnorm 1.851 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 1736\n",
      "2023-04-17 11:20:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:20:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:20:56 | INFO | fairseq.trainer | begin training epoch 56\n",
      "2023-04-17 11:20:56 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:21:08 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:21:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 11:21:08 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 4.032 | nll_loss 2.03 | ppl 4.09 | wps 6472.4 | wpb 290.8 | bsz 1 | num_updates 1783 | best_loss 4.031\n",
      "2023-04-17 11:21:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 1783 updates\n",
      "2023-04-17 11:21:08 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:21:22 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:21:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 56 @ 1783 updates, score 4.032) (writing took 13.84692736796569 seconds)\n",
      "2023-04-17 11:21:22 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)\n",
      "2023-04-17 11:21:22 | INFO | train | epoch 056 | loss 3.847 | nll_loss 1.853 | ppl 3.61 | wps 822.7 | ups 1.21 | wpb 678.5 | bsz 2 | num_updates 1783 | lr 2.95158e-05 | gnorm 2.047 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 1762\n",
      "2023-04-17 11:21:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:21:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:21:22 | INFO | fairseq.trainer | begin training epoch 57\n",
      "2023-04-17 11:21:22 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:21:29 | INFO | train_inner | epoch 057:     17 / 32 loss=3.853, nll_loss=1.859, ppl=3.63, wps=751, ups=1.09, wpb=690.6, bsz=2, num_updates=1800, lr=2.95094e-05, gnorm=2.211, clip=100, loss_scale=0.25, train_wall=39, gb_free=13.9, wall=1769\n",
      "2023-04-17 11:21:35 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:21:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:21:35 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 4.041 | nll_loss 2.094 | ppl 4.27 | wps 6055.6 | wpb 290.8 | bsz 1 | num_updates 1815 | best_loss 4.031\n",
      "2023-04-17 11:21:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 1815 updates\n",
      "2023-04-17 11:21:35 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:21:49 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:21:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 57 @ 1815 updates, score 4.041) (writing took 14.411599515937269 seconds)\n",
      "2023-04-17 11:21:49 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)\n",
      "2023-04-17 11:21:49 | INFO | train | epoch 057 | loss 3.845 | nll_loss 1.847 | ppl 3.6 | wps 786.6 | ups 1.16 | wpb 678.5 | bsz 2 | num_updates 1815 | lr 2.95038e-05 | gnorm 3.147 | clip 100 | loss_scale 0.25 | train_wall 13 | gb_free 13.9 | wall 1790\n",
      "2023-04-17 11:21:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:21:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:21:50 | INFO | fairseq.trainer | begin training epoch 58\n",
      "2023-04-17 11:21:50 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:22:02 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:22:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:22:03 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 4.042 | nll_loss 2.059 | ppl 4.17 | wps 6411.7 | wpb 290.8 | bsz 1 | num_updates 1847 | best_loss 4.031\n",
      "2023-04-17 11:22:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 1847 updates\n",
      "2023-04-17 11:22:03 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:22:17 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:22:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 58 @ 1847 updates, score 4.042) (writing took 14.763551132055 seconds)\n",
      "2023-04-17 11:22:17 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)\n",
      "2023-04-17 11:22:17 | INFO | train | epoch 058 | loss 3.841 | nll_loss 1.847 | ppl 3.6 | wps 776.8 | ups 1.14 | wpb 678.5 | bsz 2 | num_updates 1847 | lr 2.94917e-05 | gnorm 2.501 | clip 100 | loss_scale 0.25 | train_wall 13 | gb_free 13.9 | wall 1818\n",
      "2023-04-17 11:22:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:22:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:22:17 | INFO | fairseq.trainer | begin training epoch 59\n",
      "2023-04-17 11:22:17 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:22:30 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:22:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:22:31 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 4.039 | nll_loss 2.076 | ppl 4.22 | wps 5260.6 | wpb 290.8 | bsz 1 | num_updates 1879 | best_loss 4.031\n",
      "2023-04-17 11:22:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 1879 updates\n",
      "2023-04-17 11:22:31 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:22:45 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:22:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 59 @ 1879 updates, score 4.039) (writing took 14.774475477985106 seconds)\n",
      "2023-04-17 11:22:45 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)\n",
      "2023-04-17 11:22:45 | INFO | train | epoch 059 | loss 3.831 | nll_loss 1.835 | ppl 3.57 | wps 774.4 | ups 1.14 | wpb 678.5 | bsz 2 | num_updates 1879 | lr 2.94796e-05 | gnorm 2.046 | clip 100 | loss_scale 0.25 | train_wall 13 | gb_free 13.9 | wall 1846\n",
      "2023-04-17 11:22:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:22:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:22:45 | INFO | fairseq.trainer | begin training epoch 60\n",
      "2023-04-17 11:22:45 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:22:54 | INFO | train_inner | epoch 060:     21 / 32 loss=3.835, nll_loss=1.838, ppl=3.58, wps=788.5, ups=1.17, wpb=671.2, bsz=2, num_updates=1900, lr=2.94717e-05, gnorm=2.558, clip=100, loss_scale=0.25, train_wall=40, gb_free=13.9, wall=1854\n",
      "2023-04-17 11:22:58 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:22:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:22:59 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 4.011 | nll_loss 2.04 | ppl 4.11 | wps 6118.1 | wpb 290.8 | bsz 1 | num_updates 1911 | best_loss 4.011\n",
      "2023-04-17 11:22:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 1911 updates\n",
      "2023-04-17 11:22:59 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_best.pt\n",
      "2023-04-17 11:23:12 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_best.pt\n",
      "2023-04-17 11:23:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 60 @ 1911 updates, score 4.011) (writing took 22.918019799049944 seconds)\n",
      "2023-04-17 11:23:21 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)\n",
      "2023-04-17 11:23:21 | INFO | train | epoch 060 | loss 3.829 | nll_loss 1.832 | ppl 3.56 | wps 603.5 | ups 0.89 | wpb 678.5 | bsz 2 | num_updates 1911 | lr 2.94675e-05 | gnorm 2.195 | clip 100 | loss_scale 0.25 | train_wall 13 | gb_free 13.9 | wall 1882\n",
      "2023-04-17 11:23:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:23:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:23:21 | INFO | fairseq.trainer | begin training epoch 61\n",
      "2023-04-17 11:23:21 | INFO | fairseq_cli.train | Start iterating over samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 11:23:34 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:23:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:23:34 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 4.016 | nll_loss 2.041 | ppl 4.12 | wps 5835.7 | wpb 290.8 | bsz 1 | num_updates 1943 | best_loss 4.011\n",
      "2023-04-17 11:23:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 1943 updates\n",
      "2023-04-17 11:23:34 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:23:51 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:23:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 61 @ 1943 updates, score 4.016) (writing took 16.46700946893543 seconds)\n",
      "2023-04-17 11:23:51 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)\n",
      "2023-04-17 11:23:51 | INFO | train | epoch 061 | loss 3.822 | nll_loss 1.823 | ppl 3.54 | wps 738.2 | ups 1.09 | wpb 678.5 | bsz 2 | num_updates 1943 | lr 2.94555e-05 | gnorm 2.399 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 1911\n",
      "2023-04-17 11:23:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:23:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:23:51 | INFO | fairseq.trainer | begin training epoch 62\n",
      "2023-04-17 11:23:51 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:24:04 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:24:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:24:04 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 4.023 | nll_loss 2.048 | ppl 4.13 | wps 5695.9 | wpb 290.8 | bsz 1 | num_updates 1975 | best_loss 4.011\n",
      "2023-04-17 11:24:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 1975 updates\n",
      "2023-04-17 11:24:04 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:24:18 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:24:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 62 @ 1975 updates, score 4.023) (writing took 14.459426822024398 seconds)\n",
      "2023-04-17 11:24:18 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)\n",
      "2023-04-17 11:24:18 | INFO | train | epoch 062 | loss 3.816 | nll_loss 1.819 | ppl 3.53 | wps 789.3 | ups 1.16 | wpb 678.5 | bsz 2 | num_updates 1975 | lr 2.94434e-05 | gnorm 2.261 | clip 100 | loss_scale 0.25 | train_wall 13 | gb_free 13.9 | wall 1939\n",
      "2023-04-17 11:24:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:24:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:24:18 | INFO | fairseq.trainer | begin training epoch 63\n",
      "2023-04-17 11:24:18 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:24:28 | INFO | train_inner | epoch 063:     25 / 32 loss=3.822, nll_loss=1.825, ppl=3.54, wps=722.8, ups=1.06, wpb=683.1, bsz=2, num_updates=2000, lr=2.9434e-05, gnorm=2.246, clip=100, loss_scale=0.25, train_wall=39, gb_free=13.9, wall=1949\n",
      "2023-04-17 11:24:31 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:24:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:24:31 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 4.014 | nll_loss 2.043 | ppl 4.12 | wps 6655.2 | wpb 290.8 | bsz 1 | num_updates 2007 | best_loss 4.011\n",
      "2023-04-17 11:24:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 2007 updates\n",
      "2023-04-17 11:24:31 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:24:45 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:24:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 63 @ 2007 updates, score 4.014) (writing took 13.941977228969336 seconds)\n",
      "2023-04-17 11:24:45 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)\n",
      "2023-04-17 11:24:45 | INFO | train | epoch 063 | loss 3.813 | nll_loss 1.812 | ppl 3.51 | wps 810.1 | ups 1.19 | wpb 678.5 | bsz 2 | num_updates 2007 | lr 2.94313e-05 | gnorm 2.224 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 1965\n",
      "2023-04-17 11:24:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:24:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:24:45 | INFO | fairseq.trainer | begin training epoch 64\n",
      "2023-04-17 11:24:45 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:24:57 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:24:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:24:58 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 4.021 | nll_loss 2.038 | ppl 4.11 | wps 6267.1 | wpb 290.8 | bsz 1 | num_updates 2039 | best_loss 4.011\n",
      "2023-04-17 11:24:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 2039 updates\n",
      "2023-04-17 11:24:58 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:25:11 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:25:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 64 @ 2039 updates, score 4.021) (writing took 13.936886933050118 seconds)\n",
      "2023-04-17 11:25:12 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)\n",
      "2023-04-17 11:25:12 | INFO | train | epoch 064 | loss 3.803 | nll_loss 1.805 | ppl 3.49 | wps 820.5 | ups 1.21 | wpb 678.5 | bsz 2 | num_updates 2039 | lr 2.94192e-05 | gnorm 2.13 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 1992\n",
      "2023-04-17 11:25:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:25:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:25:12 | INFO | fairseq.trainer | begin training epoch 65\n",
      "2023-04-17 11:25:12 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:25:24 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:25:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:25:25 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 4.018 | nll_loss 2.03 | ppl 4.08 | wps 6391.5 | wpb 290.8 | bsz 1 | num_updates 2071 | best_loss 4.011\n",
      "2023-04-17 11:25:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 2071 updates\n",
      "2023-04-17 11:25:25 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:25:38 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:25:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 65 @ 2071 updates, score 4.018) (writing took 14.117851805058308 seconds)\n",
      "2023-04-17 11:25:39 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)\n",
      "2023-04-17 11:25:39 | INFO | train | epoch 065 | loss 3.808 | nll_loss 1.808 | ppl 3.5 | wps 802.2 | ups 1.18 | wpb 678.5 | bsz 2 | num_updates 2071 | lr 2.94072e-05 | gnorm 2.995 | clip 100 | loss_scale 0.25 | train_wall 13 | gb_free 13.9 | wall 2019\n",
      "2023-04-17 11:25:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:25:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:25:39 | INFO | fairseq.trainer | begin training epoch 66\n",
      "2023-04-17 11:25:39 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:25:50 | INFO | train_inner | epoch 066:     29 / 32 loss=3.804, nll_loss=1.803, ppl=3.49, wps=832.9, ups=1.22, wpb=680.2, bsz=2, num_updates=2100, lr=2.93962e-05, gnorm=2.52, clip=100, loss_scale=0.25, train_wall=38, gb_free=13.9, wall=2030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 11:25:51 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:25:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:25:51 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 4.012 | nll_loss 2.064 | ppl 4.18 | wps 6304.3 | wpb 290.8 | bsz 1 | num_updates 2103 | best_loss 4.011\n",
      "2023-04-17 11:25:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 66 @ 2103 updates\n",
      "2023-04-17 11:25:51 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:26:05 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:26:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 66 @ 2103 updates, score 4.012) (writing took 13.909879440092482 seconds)\n",
      "2023-04-17 11:26:05 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)\n",
      "2023-04-17 11:26:05 | INFO | train | epoch 066 | loss 3.797 | nll_loss 1.796 | ppl 3.47 | wps 815.9 | ups 1.2 | wpb 678.5 | bsz 2 | num_updates 2103 | lr 2.93951e-05 | gnorm 2.468 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 2046\n",
      "2023-04-17 11:26:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:26:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:26:05 | INFO | fairseq.trainer | begin training epoch 67\n",
      "2023-04-17 11:26:05 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:26:18 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:26:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:26:18 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 4.033 | nll_loss 2.048 | ppl 4.13 | wps 5761.4 | wpb 290.8 | bsz 1 | num_updates 2135 | best_loss 4.011\n",
      "2023-04-17 11:26:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 67 @ 2135 updates\n",
      "2023-04-17 11:26:18 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:26:32 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:26:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 67 @ 2135 updates, score 4.033) (writing took 14.217801142949611 seconds)\n",
      "2023-04-17 11:26:32 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)\n",
      "2023-04-17 11:26:32 | INFO | train | epoch 067 | loss 3.794 | nll_loss 1.794 | ppl 3.47 | wps 805 | ups 1.19 | wpb 678.5 | bsz 2 | num_updates 2135 | lr 2.9383e-05 | gnorm 2.185 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 2073\n",
      "2023-04-17 11:26:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:26:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:26:32 | INFO | fairseq.trainer | begin training epoch 68\n",
      "2023-04-17 11:26:32 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:26:45 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:26:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:26:45 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 4.019 | nll_loss 2.057 | ppl 4.16 | wps 6462.4 | wpb 290.8 | bsz 1 | num_updates 2167 | best_loss 4.011\n",
      "2023-04-17 11:26:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 68 @ 2167 updates\n",
      "2023-04-17 11:26:45 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:27:00 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:27:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 68 @ 2167 updates, score 4.019) (writing took 14.797331226989627 seconds)\n",
      "2023-04-17 11:27:00 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)\n",
      "2023-04-17 11:27:00 | INFO | train | epoch 068 | loss 3.795 | nll_loss 1.793 | ppl 3.47 | wps 776.3 | ups 1.14 | wpb 678.5 | bsz 2 | num_updates 2167 | lr 2.93709e-05 | gnorm 2.75 | clip 100 | loss_scale 0.25 | train_wall 13 | gb_free 13.9 | wall 2101\n",
      "2023-04-17 11:27:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:27:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:27:00 | INFO | fairseq.trainer | begin training epoch 69\n",
      "2023-04-17 11:27:00 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:27:12 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:27:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:27:12 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 4.014 | nll_loss 2.05 | ppl 4.14 | wps 7024.2 | wpb 290.8 | bsz 1 | num_updates 2199 | best_loss 4.011\n",
      "2023-04-17 11:27:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 69 @ 2199 updates\n",
      "2023-04-17 11:27:12 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:27:26 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:27:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 69 @ 2199 updates, score 4.014) (writing took 13.807817936060019 seconds)\n",
      "2023-04-17 11:27:26 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)\n",
      "2023-04-17 11:27:26 | INFO | train | epoch 069 | loss 3.789 | nll_loss 1.785 | ppl 3.45 | wps 850.7 | ups 1.25 | wpb 678.5 | bsz 2 | num_updates 2199 | lr 2.93589e-05 | gnorm 2.699 | clip 100 | loss_scale 0.25 | train_wall 11 | gb_free 13.9 | wall 2126\n",
      "2023-04-17 11:27:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:27:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:27:26 | INFO | fairseq.trainer | begin training epoch 70\n",
      "2023-04-17 11:27:26 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:27:26 | INFO | train_inner | epoch 070:      1 / 32 loss=3.79, nll_loss=1.789, ppl=3.45, wps=697, ups=1.04, wpb=670, bsz=2, num_updates=2200, lr=2.93585e-05, gnorm=2.568, clip=100, loss_scale=0.25, train_wall=38, gb_free=13.9, wall=2127\n",
      "2023-04-17 11:27:37 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:27:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:27:38 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 4.007 | nll_loss 2.037 | ppl 4.1 | wps 7121.5 | wpb 290.8 | bsz 1 | num_updates 2231 | best_loss 4.007\n",
      "2023-04-17 11:27:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 70 @ 2231 updates\n",
      "2023-04-17 11:27:38 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_best.pt\n",
      "2023-04-17 11:27:51 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_best.pt\n",
      "2023-04-17 11:27:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 70 @ 2231 updates, score 4.007) (writing took 21.68415541097056 seconds)\n",
      "2023-04-17 11:27:59 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)\n",
      "2023-04-17 11:27:59 | INFO | train | epoch 070 | loss 3.787 | nll_loss 1.781 | ppl 3.44 | wps 648.8 | ups 0.96 | wpb 678.5 | bsz 2 | num_updates 2231 | lr 2.93468e-05 | gnorm 3.165 | clip 100 | loss_scale 0.25 | train_wall 11 | gb_free 13.9 | wall 2160\n",
      "2023-04-17 11:27:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:27:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:27:59 | INFO | fairseq.trainer | begin training epoch 71\n",
      "2023-04-17 11:27:59 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:28:11 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:28:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 11:28:11 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 4.011 | nll_loss 2.051 | ppl 4.14 | wps 6591.9 | wpb 290.8 | bsz 1 | num_updates 2263 | best_loss 4.007\n",
      "2023-04-17 11:28:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 71 @ 2263 updates\n",
      "2023-04-17 11:28:11 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:28:26 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:28:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 71 @ 2263 updates, score 4.011) (writing took 14.691934430971742 seconds)\n",
      "2023-04-17 11:28:26 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)\n",
      "2023-04-17 11:28:26 | INFO | train | epoch 071 | loss 3.779 | nll_loss 1.776 | ppl 3.43 | wps 808.6 | ups 1.19 | wpb 678.5 | bsz 2 | num_updates 2263 | lr 2.93347e-05 | gnorm 2.905 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 2186\n",
      "2023-04-17 11:28:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:28:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:28:26 | INFO | fairseq.trainer | begin training epoch 72\n",
      "2023-04-17 11:28:26 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:28:38 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:28:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:28:38 | INFO | valid | epoch 072 | valid on 'valid' subset | loss 4.042 | nll_loss 2.052 | ppl 4.15 | wps 6545.2 | wpb 290.8 | bsz 1 | num_updates 2295 | best_loss 4.007\n",
      "2023-04-17 11:28:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 72 @ 2295 updates\n",
      "2023-04-17 11:28:38 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:28:51 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:28:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 72 @ 2295 updates, score 4.042) (writing took 13.369292696006596 seconds)\n",
      "2023-04-17 11:28:52 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)\n",
      "2023-04-17 11:28:52 | INFO | train | epoch 072 | loss 3.781 | nll_loss 1.776 | ppl 3.42 | wps 852.3 | ups 1.26 | wpb 678.5 | bsz 2 | num_updates 2295 | lr 2.93226e-05 | gnorm 3.213 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 2212\n",
      "2023-04-17 11:28:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:28:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:28:52 | INFO | fairseq.trainer | begin training epoch 73\n",
      "2023-04-17 11:28:52 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:28:54 | INFO | train_inner | epoch 073:      5 / 32 loss=3.785, nll_loss=1.782, ppl=3.44, wps=784.7, ups=1.14, wpb=685.6, bsz=2, num_updates=2300, lr=2.93208e-05, gnorm=3.07, clip=100, loss_scale=0.25, train_wall=36, gb_free=13.9, wall=2214\n",
      "2023-04-17 11:29:04 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:29:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:29:04 | INFO | valid | epoch 073 | valid on 'valid' subset | loss 4.008 | nll_loss 2.024 | ppl 4.07 | wps 6624.4 | wpb 290.8 | bsz 1 | num_updates 2327 | best_loss 4.007\n",
      "2023-04-17 11:29:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 73 @ 2327 updates\n",
      "2023-04-17 11:29:04 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:29:17 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:29:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 73 @ 2327 updates, score 4.008) (writing took 13.51630447700154 seconds)\n",
      "2023-04-17 11:29:18 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)\n",
      "2023-04-17 11:29:18 | INFO | train | epoch 073 | loss 3.776 | nll_loss 1.774 | ppl 3.42 | wps 834.5 | ups 1.23 | wpb 678.5 | bsz 2 | num_updates 2327 | lr 2.93106e-05 | gnorm 2.912 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 2238\n",
      "2023-04-17 11:29:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:29:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:29:18 | INFO | fairseq.trainer | begin training epoch 74\n",
      "2023-04-17 11:29:18 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:29:30 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:29:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:29:30 | INFO | valid | epoch 074 | valid on 'valid' subset | loss 3.993 | nll_loss 2.007 | ppl 4.02 | wps 6562.9 | wpb 290.8 | bsz 1 | num_updates 2359 | best_loss 3.993\n",
      "2023-04-17 11:29:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 74 @ 2359 updates\n",
      "2023-04-17 11:29:30 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_best.pt\n",
      "2023-04-17 11:29:44 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_best.pt\n",
      "2023-04-17 11:29:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 74 @ 2359 updates, score 3.993) (writing took 22.380966600961983 seconds)\n",
      "2023-04-17 11:29:52 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)\n",
      "2023-04-17 11:29:52 | INFO | train | epoch 074 | loss 3.767 | nll_loss 1.765 | ppl 3.4 | wps 623.1 | ups 0.92 | wpb 678.5 | bsz 2 | num_updates 2359 | lr 2.92985e-05 | gnorm 2.944 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 2273\n",
      "2023-04-17 11:29:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:29:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:29:52 | INFO | fairseq.trainer | begin training epoch 75\n",
      "2023-04-17 11:29:52 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:30:05 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:30:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:30:05 | INFO | valid | epoch 075 | valid on 'valid' subset | loss 4.01 | nll_loss 2.043 | ppl 4.12 | wps 6317.4 | wpb 290.8 | bsz 1 | num_updates 2391 | best_loss 3.993\n",
      "2023-04-17 11:30:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 75 @ 2391 updates\n",
      "2023-04-17 11:30:05 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:30:20 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:30:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 75 @ 2391 updates, score 4.01) (writing took 15.699446758953854 seconds)\n",
      "2023-04-17 11:30:21 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)\n",
      "2023-04-17 11:30:21 | INFO | train | epoch 075 | loss 3.762 | nll_loss 1.758 | ppl 3.38 | wps 767 | ups 1.13 | wpb 678.5 | bsz 2 | num_updates 2391 | lr 2.92864e-05 | gnorm 2.578 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 2301\n",
      "2023-04-17 11:30:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:30:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:30:21 | INFO | fairseq.trainer | begin training epoch 76\n",
      "2023-04-17 11:30:21 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:30:25 | INFO | train_inner | epoch 076:      9 / 32 loss=3.769, nll_loss=1.764, ppl=3.4, wps=744.1, ups=1.1, wpb=676.9, bsz=2, num_updates=2400, lr=2.9283e-05, gnorm=2.808, clip=100, loss_scale=0.25, train_wall=38, gb_free=13.9, wall=2305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 11:30:34 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:30:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:30:34 | INFO | valid | epoch 076 | valid on 'valid' subset | loss 3.993 | nll_loss 2.015 | ppl 4.04 | wps 6206.3 | wpb 290.8 | bsz 1 | num_updates 2423 | best_loss 3.993\n",
      "2023-04-17 11:30:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 76 @ 2423 updates\n",
      "2023-04-17 11:30:34 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_best.pt\n",
      "2023-04-17 11:30:48 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_best.pt\n",
      "2023-04-17 11:30:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 76 @ 2423 updates, score 3.993) (writing took 22.11890505196061 seconds)\n",
      "2023-04-17 11:30:56 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)\n",
      "2023-04-17 11:30:56 | INFO | train | epoch 076 | loss 3.757 | nll_loss 1.753 | ppl 3.37 | wps 609.5 | ups 0.9 | wpb 678.5 | bsz 2 | num_updates 2423 | lr 2.92743e-05 | gnorm 2.607 | clip 100 | loss_scale 0.25 | train_wall 13 | gb_free 13.9 | wall 2337\n",
      "2023-04-17 11:30:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:30:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:30:56 | INFO | fairseq.trainer | begin training epoch 77\n",
      "2023-04-17 11:30:56 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:31:09 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:31:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:31:09 | INFO | valid | epoch 077 | valid on 'valid' subset | loss 4.024 | nll_loss 2.038 | ppl 4.11 | wps 6585.4 | wpb 290.8 | bsz 1 | num_updates 2455 | best_loss 3.993\n",
      "2023-04-17 11:31:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 77 @ 2455 updates\n",
      "2023-04-17 11:31:09 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:31:22 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:31:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 77 @ 2455 updates, score 4.024) (writing took 13.41424985404592 seconds)\n",
      "2023-04-17 11:31:23 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)\n",
      "2023-04-17 11:31:23 | INFO | train | epoch 077 | loss 3.753 | nll_loss 1.747 | ppl 3.36 | wps 829.5 | ups 1.22 | wpb 678.5 | bsz 2 | num_updates 2455 | lr 2.92623e-05 | gnorm 2.599 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 2363\n",
      "2023-04-17 11:31:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:31:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:31:23 | INFO | fairseq.trainer | begin training epoch 78\n",
      "2023-04-17 11:31:23 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:31:35 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:31:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:31:35 | INFO | valid | epoch 078 | valid on 'valid' subset | loss 3.996 | nll_loss 2.01 | ppl 4.03 | wps 6458.2 | wpb 290.8 | bsz 1 | num_updates 2487 | best_loss 3.993\n",
      "2023-04-17 11:31:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 78 @ 2487 updates\n",
      "2023-04-17 11:31:35 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:31:49 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:31:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 78 @ 2487 updates, score 3.996) (writing took 14.506322515080683 seconds)\n",
      "2023-04-17 11:31:49 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)\n",
      "2023-04-17 11:31:49 | INFO | train | epoch 078 | loss 3.751 | nll_loss 1.745 | ppl 3.35 | wps 812.9 | ups 1.2 | wpb 678.5 | bsz 2 | num_updates 2487 | lr 2.92502e-05 | gnorm 2.458 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 2390\n",
      "2023-04-17 11:31:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:31:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:31:49 | INFO | fairseq.trainer | begin training epoch 79\n",
      "2023-04-17 11:31:49 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:31:54 | INFO | train_inner | epoch 079:     13 / 32 loss=3.748, nll_loss=1.742, ppl=3.35, wps=759.1, ups=1.12, wpb=680.5, bsz=2, num_updates=2500, lr=2.92453e-05, gnorm=2.485, clip=100, loss_scale=0.25, train_wall=38, gb_free=13.9, wall=2395\n",
      "2023-04-17 11:32:01 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:32:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:32:02 | INFO | valid | epoch 079 | valid on 'valid' subset | loss 4.017 | nll_loss 2.029 | ppl 4.08 | wps 6426.1 | wpb 290.8 | bsz 1 | num_updates 2519 | best_loss 3.993\n",
      "2023-04-17 11:32:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 79 @ 2519 updates\n",
      "2023-04-17 11:32:02 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:32:15 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:32:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 79 @ 2519 updates, score 4.017) (writing took 13.720025726943277 seconds)\n",
      "2023-04-17 11:32:15 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)\n",
      "2023-04-17 11:32:15 | INFO | train | epoch 079 | loss 3.744 | nll_loss 1.74 | ppl 3.34 | wps 831.3 | ups 1.23 | wpb 678.5 | bsz 2 | num_updates 2519 | lr 2.92381e-05 | gnorm 2.729 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 2416\n",
      "2023-04-17 11:32:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:32:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:32:15 | INFO | fairseq.trainer | begin training epoch 80\n",
      "2023-04-17 11:32:15 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:32:28 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:32:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:32:28 | INFO | valid | epoch 080 | valid on 'valid' subset | loss 4.016 | nll_loss 2.06 | ppl 4.17 | wps 6982.6 | wpb 290.8 | bsz 1 | num_updates 2551 | best_loss 3.993\n",
      "2023-04-17 11:32:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 80 @ 2551 updates\n",
      "2023-04-17 11:32:28 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:32:42 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:32:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 80 @ 2551 updates, score 4.016) (writing took 14.29514941596426 seconds)\n",
      "2023-04-17 11:32:42 | INFO | fairseq_cli.train | end of epoch 80 (average epoch stats below)\n",
      "2023-04-17 11:32:42 | INFO | train | epoch 080 | loss 3.737 | nll_loss 1.731 | ppl 3.32 | wps 814.3 | ups 1.2 | wpb 678.5 | bsz 2 | num_updates 2551 | lr 2.9226e-05 | gnorm 2.572 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 2442\n",
      "2023-04-17 11:32:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:32:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:32:42 | INFO | fairseq.trainer | begin training epoch 81\n",
      "2023-04-17 11:32:42 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:32:54 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:32:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 11:32:54 | INFO | valid | epoch 081 | valid on 'valid' subset | loss 4.008 | nll_loss 2.032 | ppl 4.09 | wps 7184.9 | wpb 290.8 | bsz 1 | num_updates 2583 | best_loss 3.993\n",
      "2023-04-17 11:32:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 81 @ 2583 updates\n",
      "2023-04-17 11:32:54 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:33:07 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:33:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 81 @ 2583 updates, score 4.008) (writing took 12.770387794007547 seconds)\n",
      "2023-04-17 11:33:07 | INFO | fairseq_cli.train | end of epoch 81 (average epoch stats below)\n",
      "2023-04-17 11:33:07 | INFO | train | epoch 081 | loss 3.74 | nll_loss 1.734 | ppl 3.33 | wps 869.8 | ups 1.28 | wpb 678.5 | bsz 2 | num_updates 2583 | lr 2.9214e-05 | gnorm 2.486 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 2467\n",
      "2023-04-17 11:33:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:33:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:33:07 | INFO | fairseq.trainer | begin training epoch 82\n",
      "2023-04-17 11:33:07 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:33:13 | INFO | train_inner | epoch 082:     17 / 32 loss=3.733, nll_loss=1.726, ppl=3.31, wps=855.3, ups=1.27, wpb=675.7, bsz=2, num_updates=2600, lr=2.92075e-05, gnorm=2.601, clip=100, loss_scale=0.25, train_wall=37, gb_free=13.9, wall=2473\n",
      "2023-04-17 11:33:19 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:33:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:33:19 | INFO | valid | epoch 082 | valid on 'valid' subset | loss 4 | nll_loss 2.024 | ppl 4.07 | wps 7112.7 | wpb 290.8 | bsz 1 | num_updates 2615 | best_loss 3.993\n",
      "2023-04-17 11:33:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 82 @ 2615 updates\n",
      "2023-04-17 11:33:19 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:33:32 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:33:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 82 @ 2615 updates, score 4.0) (writing took 13.319509697030298 seconds)\n",
      "2023-04-17 11:33:32 | INFO | fairseq_cli.train | end of epoch 82 (average epoch stats below)\n",
      "2023-04-17 11:33:32 | INFO | train | epoch 082 | loss 3.729 | nll_loss 1.722 | ppl 3.3 | wps 868.1 | ups 1.28 | wpb 678.5 | bsz 2 | num_updates 2615 | lr 2.92019e-05 | gnorm 2.649 | clip 100 | loss_scale 0.25 | train_wall 11 | gb_free 13.9 | wall 2492\n",
      "2023-04-17 11:33:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:33:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:33:32 | INFO | fairseq.trainer | begin training epoch 83\n",
      "2023-04-17 11:33:32 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:33:44 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:33:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:33:44 | INFO | valid | epoch 083 | valid on 'valid' subset | loss 3.992 | nll_loss 2.035 | ppl 4.1 | wps 6937.9 | wpb 290.8 | bsz 1 | num_updates 2647 | best_loss 3.992\n",
      "2023-04-17 11:33:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 83 @ 2647 updates\n",
      "2023-04-17 11:33:44 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_best.pt\n",
      "2023-04-17 11:33:56 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_best.pt\n",
      "2023-04-17 11:34:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 83 @ 2647 updates, score 3.992) (writing took 20.325566587038338 seconds)\n",
      "2023-04-17 11:34:04 | INFO | fairseq_cli.train | end of epoch 83 (average epoch stats below)\n",
      "2023-04-17 11:34:04 | INFO | train | epoch 083 | loss 3.73 | nll_loss 1.722 | ppl 3.3 | wps 677.6 | ups 1 | wpb 678.5 | bsz 2 | num_updates 2647 | lr 2.91898e-05 | gnorm 2.398 | clip 100 | loss_scale 0.25 | train_wall 11 | gb_free 13.9 | wall 2524\n",
      "2023-04-17 11:34:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:34:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:34:04 | INFO | fairseq.trainer | begin training epoch 84\n",
      "2023-04-17 11:34:04 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:34:16 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:34:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:34:17 | INFO | valid | epoch 084 | valid on 'valid' subset | loss 4.008 | nll_loss 2.01 | ppl 4.03 | wps 6577.7 | wpb 290.8 | bsz 1 | num_updates 2679 | best_loss 3.992\n",
      "2023-04-17 11:34:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 84 @ 2679 updates\n",
      "2023-04-17 11:34:17 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:34:32 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:34:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 84 @ 2679 updates, score 4.008) (writing took 15.219558986020274 seconds)\n",
      "2023-04-17 11:34:32 | INFO | fairseq_cli.train | end of epoch 84 (average epoch stats below)\n",
      "2023-04-17 11:34:32 | INFO | train | epoch 084 | loss 3.721 | nll_loss 1.713 | ppl 3.28 | wps 783.1 | ups 1.15 | wpb 678.5 | bsz 2 | num_updates 2679 | lr 2.91777e-05 | gnorm 2.46 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 2552\n",
      "2023-04-17 11:34:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:34:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:34:32 | INFO | fairseq.trainer | begin training epoch 85\n",
      "2023-04-17 11:34:32 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:34:40 | INFO | train_inner | epoch 085:     21 / 32 loss=3.728, nll_loss=1.721, ppl=3.3, wps=775.3, ups=1.15, wpb=672.6, bsz=2, num_updates=2700, lr=2.91698e-05, gnorm=2.523, clip=100, loss_scale=0.25, train_wall=37, gb_free=13.9, wall=2560\n",
      "2023-04-17 11:34:44 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:34:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:34:44 | INFO | valid | epoch 085 | valid on 'valid' subset | loss 3.979 | nll_loss 1.996 | ppl 3.99 | wps 6638.9 | wpb 290.8 | bsz 1 | num_updates 2711 | best_loss 3.979\n",
      "2023-04-17 11:34:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 85 @ 2711 updates\n",
      "2023-04-17 11:34:44 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_best.pt\n",
      "2023-04-17 11:34:58 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_best.pt\n",
      "2023-04-17 11:35:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 85 @ 2711 updates, score 3.979) (writing took 22.55614175205119 seconds)\n",
      "2023-04-17 11:35:07 | INFO | fairseq_cli.train | end of epoch 85 (average epoch stats below)\n",
      "2023-04-17 11:35:07 | INFO | train | epoch 085 | loss 3.713 | nll_loss 1.702 | ppl 3.25 | wps 620 | ups 0.91 | wpb 678.5 | bsz 2 | num_updates 2711 | lr 2.91657e-05 | gnorm 2.374 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 2587\n",
      "2023-04-17 11:35:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:35:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:35:07 | INFO | fairseq.trainer | begin training epoch 86\n",
      "2023-04-17 11:35:07 | INFO | fairseq_cli.train | Start iterating over samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 11:35:19 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:35:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:35:19 | INFO | valid | epoch 086 | valid on 'valid' subset | loss 4.003 | nll_loss 2.029 | ppl 4.08 | wps 6433.2 | wpb 290.8 | bsz 1 | num_updates 2743 | best_loss 3.979\n",
      "2023-04-17 11:35:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 86 @ 2743 updates\n",
      "2023-04-17 11:35:19 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:35:34 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:35:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 86 @ 2743 updates, score 4.003) (writing took 15.187277738004923 seconds)\n",
      "2023-04-17 11:35:34 | INFO | fairseq_cli.train | end of epoch 86 (average epoch stats below)\n",
      "2023-04-17 11:35:34 | INFO | train | epoch 086 | loss 3.713 | nll_loss 1.702 | ppl 3.25 | wps 789.5 | ups 1.16 | wpb 678.5 | bsz 2 | num_updates 2743 | lr 2.91536e-05 | gnorm 2.437 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 2615\n",
      "2023-04-17 11:35:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:35:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:35:34 | INFO | fairseq.trainer | begin training epoch 87\n",
      "2023-04-17 11:35:34 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:35:46 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:35:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:35:47 | INFO | valid | epoch 087 | valid on 'valid' subset | loss 4.017 | nll_loss 2.065 | ppl 4.18 | wps 5049.2 | wpb 290.8 | bsz 1 | num_updates 2775 | best_loss 3.979\n",
      "2023-04-17 11:35:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 87 @ 2775 updates\n",
      "2023-04-17 11:35:47 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:36:00 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:36:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 87 @ 2775 updates, score 4.017) (writing took 13.175631821039133 seconds)\n",
      "2023-04-17 11:36:00 | INFO | fairseq_cli.train | end of epoch 87 (average epoch stats below)\n",
      "2023-04-17 11:36:00 | INFO | train | epoch 087 | loss 3.705 | nll_loss 1.69 | ppl 3.23 | wps 853.3 | ups 1.26 | wpb 678.5 | bsz 2 | num_updates 2775 | lr 2.91415e-05 | gnorm 2.623 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 2640\n",
      "2023-04-17 11:36:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:36:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:36:00 | INFO | fairseq.trainer | begin training epoch 88\n",
      "2023-04-17 11:36:00 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:36:09 | INFO | train_inner | epoch 088:     25 / 32 loss=3.715, nll_loss=1.704, ppl=3.26, wps=768.5, ups=1.12, wpb=686.5, bsz=2, num_updates=2800, lr=2.91321e-05, gnorm=2.539, clip=100, loss_scale=0.25, train_wall=37, gb_free=13.9, wall=2650\n",
      "2023-04-17 11:36:12 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:36:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:36:12 | INFO | valid | epoch 088 | valid on 'valid' subset | loss 4.014 | nll_loss 2.04 | ppl 4.11 | wps 6657.8 | wpb 290.8 | bsz 1 | num_updates 2807 | best_loss 3.979\n",
      "2023-04-17 11:36:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 88 @ 2807 updates\n",
      "2023-04-17 11:36:12 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:36:25 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:36:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 88 @ 2807 updates, score 4.014) (writing took 13.365695760003291 seconds)\n",
      "2023-04-17 11:36:25 | INFO | fairseq_cli.train | end of epoch 88 (average epoch stats below)\n",
      "2023-04-17 11:36:25 | INFO | train | epoch 088 | loss 3.733 | nll_loss 1.721 | ppl 3.3 | wps 848 | ups 1.25 | wpb 678.5 | bsz 2 | num_updates 2807 | lr 2.91294e-05 | gnorm 3.937 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 2666\n",
      "2023-04-17 11:36:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:36:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:36:25 | INFO | fairseq.trainer | begin training epoch 89\n",
      "2023-04-17 11:36:25 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:36:37 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:36:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:36:38 | INFO | valid | epoch 089 | valid on 'valid' subset | loss 4.006 | nll_loss 2.031 | ppl 4.09 | wps 6518.8 | wpb 290.8 | bsz 1 | num_updates 2839 | best_loss 3.979\n",
      "2023-04-17 11:36:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 89 @ 2839 updates\n",
      "2023-04-17 11:36:38 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:36:51 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:36:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 89 @ 2839 updates, score 4.006) (writing took 13.866622933070175 seconds)\n",
      "2023-04-17 11:36:52 | INFO | fairseq_cli.train | end of epoch 89 (average epoch stats below)\n",
      "2023-04-17 11:36:52 | INFO | train | epoch 089 | loss 3.699 | nll_loss 1.686 | ppl 3.22 | wps 828.3 | ups 1.22 | wpb 678.5 | bsz 2 | num_updates 2839 | lr 2.91174e-05 | gnorm 2.369 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 2692\n",
      "2023-04-17 11:36:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:36:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:36:52 | INFO | fairseq.trainer | begin training epoch 90\n",
      "2023-04-17 11:36:52 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:37:04 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:37:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:37:04 | INFO | valid | epoch 090 | valid on 'valid' subset | loss 4.027 | nll_loss 2.033 | ppl 4.09 | wps 6490.8 | wpb 290.8 | bsz 1 | num_updates 2871 | best_loss 3.979\n",
      "2023-04-17 11:37:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 90 @ 2871 updates\n",
      "2023-04-17 11:37:04 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:37:17 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:37:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 90 @ 2871 updates, score 4.027) (writing took 13.34937086806167 seconds)\n",
      "2023-04-17 11:37:17 | INFO | fairseq_cli.train | end of epoch 90 (average epoch stats below)\n",
      "2023-04-17 11:37:17 | INFO | train | epoch 090 | loss 3.701 | nll_loss 1.69 | ppl 3.23 | wps 847 | ups 1.25 | wpb 678.5 | bsz 2 | num_updates 2871 | lr 2.91053e-05 | gnorm 2.573 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 2717\n",
      "2023-04-17 11:37:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:37:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:37:17 | INFO | fairseq.trainer | begin training epoch 91\n",
      "2023-04-17 11:37:17 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:37:28 | INFO | train_inner | epoch 091:     29 / 32 loss=3.698, nll_loss=1.684, ppl=3.21, wps=854.2, ups=1.27, wpb=674.2, bsz=2, num_updates=2900, lr=2.90943e-05, gnorm=2.85, clip=100, loss_scale=0.25, train_wall=37, gb_free=13.9, wall=2729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 11:37:29 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:37:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:37:29 | INFO | valid | epoch 091 | valid on 'valid' subset | loss 4.021 | nll_loss 2.051 | ppl 4.14 | wps 6601.7 | wpb 290.8 | bsz 1 | num_updates 2903 | best_loss 3.979\n",
      "2023-04-17 11:37:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 91 @ 2903 updates\n",
      "2023-04-17 11:37:29 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:37:43 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:37:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 91 @ 2903 updates, score 4.021) (writing took 14.050252425018698 seconds)\n",
      "2023-04-17 11:37:44 | INFO | fairseq_cli.train | end of epoch 91 (average epoch stats below)\n",
      "2023-04-17 11:37:44 | INFO | train | epoch 091 | loss 3.684 | nll_loss 1.67 | ppl 3.18 | wps 825.6 | ups 1.22 | wpb 678.5 | bsz 2 | num_updates 2903 | lr 2.90932e-05 | gnorm 2.323 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 2744\n",
      "2023-04-17 11:37:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:37:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:37:44 | INFO | fairseq.trainer | begin training epoch 92\n",
      "2023-04-17 11:37:44 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:37:56 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:37:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:37:56 | INFO | valid | epoch 092 | valid on 'valid' subset | loss 3.984 | nll_loss 2.005 | ppl 4.02 | wps 6534.3 | wpb 290.8 | bsz 1 | num_updates 2935 | best_loss 3.979\n",
      "2023-04-17 11:37:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 92 @ 2935 updates\n",
      "2023-04-17 11:37:56 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:38:10 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:38:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 92 @ 2935 updates, score 3.984) (writing took 13.92862685106229 seconds)\n",
      "2023-04-17 11:38:10 | INFO | fairseq_cli.train | end of epoch 92 (average epoch stats below)\n",
      "2023-04-17 11:38:10 | INFO | train | epoch 092 | loss 3.682 | nll_loss 1.669 | ppl 3.18 | wps 825.1 | ups 1.22 | wpb 678.5 | bsz 2 | num_updates 2935 | lr 2.90811e-05 | gnorm 2.301 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 2770\n",
      "2023-04-17 11:38:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:38:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:38:10 | INFO | fairseq.trainer | begin training epoch 93\n",
      "2023-04-17 11:38:10 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:38:22 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:38:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:38:22 | INFO | valid | epoch 093 | valid on 'valid' subset | loss 4.003 | nll_loss 2.028 | ppl 4.08 | wps 6522.9 | wpb 290.8 | bsz 1 | num_updates 2967 | best_loss 3.979\n",
      "2023-04-17 11:38:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 93 @ 2967 updates\n",
      "2023-04-17 11:38:22 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:38:35 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:38:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 93 @ 2967 updates, score 4.003) (writing took 13.366541653987952 seconds)\n",
      "2023-04-17 11:38:36 | INFO | fairseq_cli.train | end of epoch 93 (average epoch stats below)\n",
      "2023-04-17 11:38:36 | INFO | train | epoch 093 | loss 3.676 | nll_loss 1.659 | ppl 3.16 | wps 844.8 | ups 1.24 | wpb 678.5 | bsz 2 | num_updates 2967 | lr 2.90691e-05 | gnorm 2.172 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 2796\n",
      "2023-04-17 11:38:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:38:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:38:36 | INFO | fairseq.trainer | begin training epoch 94\n",
      "2023-04-17 11:38:36 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:38:48 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:38:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:38:48 | INFO | valid | epoch 094 | valid on 'valid' subset | loss 4.062 | nll_loss 2.074 | ppl 4.21 | wps 6615.3 | wpb 290.8 | bsz 1 | num_updates 2999 | best_loss 3.979\n",
      "2023-04-17 11:38:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 94 @ 2999 updates\n",
      "2023-04-17 11:38:48 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:39:01 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:39:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 94 @ 2999 updates, score 4.062) (writing took 13.40239138097968 seconds)\n",
      "2023-04-17 11:39:01 | INFO | fairseq_cli.train | end of epoch 94 (average epoch stats below)\n",
      "2023-04-17 11:39:01 | INFO | train | epoch 094 | loss 3.672 | nll_loss 1.657 | ppl 3.15 | wps 843.9 | ups 1.24 | wpb 678.5 | bsz 2 | num_updates 2999 | lr 2.9057e-05 | gnorm 2.496 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 2822\n",
      "2023-04-17 11:39:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:39:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:39:01 | INFO | fairseq.trainer | begin training epoch 95\n",
      "2023-04-17 11:39:01 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:39:02 | INFO | train_inner | epoch 095:      1 / 32 loss=3.679, nll_loss=1.664, ppl=3.17, wps=726.9, ups=1.07, wpb=679.2, bsz=2, num_updates=3000, lr=2.90566e-05, gnorm=2.326, clip=100, loss_scale=0.25, train_wall=37, gb_free=13.9, wall=2822\n",
      "2023-04-17 11:39:13 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:39:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:39:13 | INFO | valid | epoch 095 | valid on 'valid' subset | loss 4 | nll_loss 2.016 | ppl 4.04 | wps 7028 | wpb 290.8 | bsz 1 | num_updates 3031 | best_loss 3.979\n",
      "2023-04-17 11:39:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 95 @ 3031 updates\n",
      "2023-04-17 11:39:13 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:39:26 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:39:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 95 @ 3031 updates, score 4.0) (writing took 13.3701462529134 seconds)\n",
      "2023-04-17 11:39:27 | INFO | fairseq_cli.train | end of epoch 95 (average epoch stats below)\n",
      "2023-04-17 11:39:27 | INFO | train | epoch 095 | loss 3.667 | nll_loss 1.652 | ppl 3.14 | wps 858.7 | ups 1.27 | wpb 678.5 | bsz 2 | num_updates 3031 | lr 2.90449e-05 | gnorm 2.516 | clip 100 | loss_scale 0.25 | train_wall 11 | gb_free 13.9 | wall 2847\n",
      "2023-04-17 11:39:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:39:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:39:27 | INFO | fairseq.trainer | begin training epoch 96\n",
      "2023-04-17 11:39:27 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:39:38 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:39:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 11:39:38 | INFO | valid | epoch 096 | valid on 'valid' subset | loss 4.025 | nll_loss 2.042 | ppl 4.12 | wps 6277.2 | wpb 290.8 | bsz 1 | num_updates 3063 | best_loss 3.979\n",
      "2023-04-17 11:39:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 96 @ 3063 updates\n",
      "2023-04-17 11:39:38 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:39:52 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:39:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 96 @ 3063 updates, score 4.025) (writing took 14.81393081706483 seconds)\n",
      "2023-04-17 11:39:53 | INFO | fairseq_cli.train | end of epoch 96 (average epoch stats below)\n",
      "2023-04-17 11:39:53 | INFO | train | epoch 096 | loss 3.659 | nll_loss 1.64 | ppl 3.12 | wps 827.5 | ups 1.22 | wpb 678.5 | bsz 2 | num_updates 3063 | lr 2.90328e-05 | gnorm 2.573 | clip 100 | loss_scale 0.25 | train_wall 11 | gb_free 13.9 | wall 2873\n",
      "2023-04-17 11:39:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:39:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:39:53 | INFO | fairseq.trainer | begin training epoch 97\n",
      "2023-04-17 11:39:53 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:40:06 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:40:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:40:06 | INFO | valid | epoch 097 | valid on 'valid' subset | loss 4.025 | nll_loss 2.042 | ppl 4.12 | wps 5576.2 | wpb 290.8 | bsz 1 | num_updates 3095 | best_loss 3.979\n",
      "2023-04-17 11:40:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 97 @ 3095 updates\n",
      "2023-04-17 11:40:06 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:40:20 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:40:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 97 @ 3095 updates, score 4.025) (writing took 14.020688123069704 seconds)\n",
      "2023-04-17 11:40:20 | INFO | fairseq_cli.train | end of epoch 97 (average epoch stats below)\n",
      "2023-04-17 11:40:20 | INFO | train | epoch 097 | loss 3.657 | nll_loss 1.638 | ppl 3.11 | wps 797.8 | ups 1.18 | wpb 678.5 | bsz 2 | num_updates 3095 | lr 2.90208e-05 | gnorm 2.585 | clip 100 | loss_scale 0.25 | train_wall 13 | gb_free 13.9 | wall 2900\n",
      "2023-04-17 11:40:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:40:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:40:20 | INFO | fairseq.trainer | begin training epoch 98\n",
      "2023-04-17 11:40:20 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:40:22 | INFO | train_inner | epoch 098:      5 / 32 loss=3.663, nll_loss=1.646, ppl=3.13, wps=844.3, ups=1.24, wpb=679.6, bsz=2, num_updates=3100, lr=2.90189e-05, gnorm=2.556, clip=100, loss_scale=0.25, train_wall=37, gb_free=13.8, wall=2902\n",
      "2023-04-17 11:40:33 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:40:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:40:33 | INFO | valid | epoch 098 | valid on 'valid' subset | loss 4.05 | nll_loss 2.08 | ppl 4.23 | wps 6443.6 | wpb 290.8 | bsz 1 | num_updates 3127 | best_loss 3.979\n",
      "2023-04-17 11:40:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 98 @ 3127 updates\n",
      "2023-04-17 11:40:33 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:40:47 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:40:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 98 @ 3127 updates, score 4.05) (writing took 14.423029155936092 seconds)\n",
      "2023-04-17 11:40:48 | INFO | fairseq_cli.train | end of epoch 98 (average epoch stats below)\n",
      "2023-04-17 11:40:48 | INFO | train | epoch 098 | loss 3.644 | nll_loss 1.623 | ppl 3.08 | wps 789.8 | ups 1.16 | wpb 678.5 | bsz 2 | num_updates 3127 | lr 2.90087e-05 | gnorm 3.072 | clip 100 | loss_scale 0.25 | train_wall 13 | gb_free 13.9 | wall 2928\n",
      "2023-04-17 11:40:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:40:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:40:48 | INFO | fairseq.trainer | begin training epoch 99\n",
      "2023-04-17 11:40:48 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:41:00 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:41:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:41:00 | INFO | valid | epoch 099 | valid on 'valid' subset | loss 4.022 | nll_loss 2.032 | ppl 4.09 | wps 6622.6 | wpb 290.8 | bsz 1 | num_updates 3159 | best_loss 3.979\n",
      "2023-04-17 11:41:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 99 @ 3159 updates\n",
      "2023-04-17 11:41:00 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:41:15 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:41:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 99 @ 3159 updates, score 4.022) (writing took 14.647964279050939 seconds)\n",
      "2023-04-17 11:41:15 | INFO | fairseq_cli.train | end of epoch 99 (average epoch stats below)\n",
      "2023-04-17 11:41:15 | INFO | train | epoch 099 | loss 3.644 | nll_loss 1.625 | ppl 3.08 | wps 788.2 | ups 1.16 | wpb 678.5 | bsz 2 | num_updates 3159 | lr 2.89966e-05 | gnorm 2.573 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 2955\n",
      "2023-04-17 11:41:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:41:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:41:15 | INFO | fairseq.trainer | begin training epoch 100\n",
      "2023-04-17 11:41:15 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:41:28 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:41:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:41:28 | INFO | valid | epoch 100 | valid on 'valid' subset | loss 4.054 | nll_loss 2.05 | ppl 4.14 | wps 5531 | wpb 290.8 | bsz 1 | num_updates 3191 | best_loss 3.979\n",
      "2023-04-17 11:41:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 100 @ 3191 updates\n",
      "2023-04-17 11:41:28 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:41:43 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:41:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 100 @ 3191 updates, score 4.054) (writing took 14.794011141988449 seconds)\n",
      "2023-04-17 11:41:43 | INFO | fairseq_cli.train | end of epoch 100 (average epoch stats below)\n",
      "2023-04-17 11:41:43 | INFO | train | epoch 100 | loss 3.633 | nll_loss 1.609 | ppl 3.05 | wps 776.3 | ups 1.14 | wpb 678.5 | bsz 2 | num_updates 3191 | lr 2.89845e-05 | gnorm 2.866 | clip 100 | loss_scale 0.25 | train_wall 13 | gb_free 13.9 | wall 2983\n",
      "2023-04-17 11:41:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:41:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:41:43 | INFO | fairseq.trainer | begin training epoch 101\n",
      "2023-04-17 11:41:43 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:41:47 | INFO | train_inner | epoch 101:      9 / 32 loss=3.639, nll_loss=1.617, ppl=3.07, wps=806.5, ups=1.18, wpb=683.2, bsz=2, num_updates=3200, lr=2.89811e-05, gnorm=2.856, clip=100, loss_scale=0.25, train_wall=39, gb_free=13.7, wall=2987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 11:41:56 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:41:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:41:56 | INFO | valid | epoch 101 | valid on 'valid' subset | loss 4.034 | nll_loss 2.043 | ppl 4.12 | wps 6544.8 | wpb 290.8 | bsz 1 | num_updates 3223 | best_loss 3.979\n",
      "2023-04-17 11:41:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 101 @ 3223 updates\n",
      "2023-04-17 11:41:56 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:42:11 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:42:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 101 @ 3223 updates, score 4.034) (writing took 15.064352386980318 seconds)\n",
      "2023-04-17 11:42:12 | INFO | fairseq_cli.train | end of epoch 101 (average epoch stats below)\n",
      "2023-04-17 11:42:12 | INFO | train | epoch 101 | loss 3.626 | nll_loss 1.603 | ppl 3.04 | wps 763.1 | ups 1.12 | wpb 678.5 | bsz 2 | num_updates 3223 | lr 2.89725e-05 | gnorm 2.745 | clip 100 | loss_scale 0.25 | train_wall 13 | gb_free 13.9 | wall 3012\n",
      "2023-04-17 11:42:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:42:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:42:12 | INFO | fairseq.trainer | begin training epoch 102\n",
      "2023-04-17 11:42:12 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:42:24 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:42:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:42:25 | INFO | valid | epoch 102 | valid on 'valid' subset | loss 4.019 | nll_loss 2.031 | ppl 4.09 | wps 6562.3 | wpb 290.8 | bsz 1 | num_updates 3255 | best_loss 3.979\n",
      "2023-04-17 11:42:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 102 @ 3255 updates\n",
      "2023-04-17 11:42:25 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:42:39 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:42:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 102 @ 3255 updates, score 4.019) (writing took 15.083403708063997 seconds)\n",
      "2023-04-17 11:42:40 | INFO | fairseq_cli.train | end of epoch 102 (average epoch stats below)\n",
      "2023-04-17 11:42:40 | INFO | train | epoch 102 | loss 3.627 | nll_loss 1.602 | ppl 3.04 | wps 768.7 | ups 1.13 | wpb 678.5 | bsz 2 | num_updates 3255 | lr 2.89604e-05 | gnorm 2.904 | clip 100 | loss_scale 0.25 | train_wall 13 | gb_free 13.9 | wall 3040\n",
      "2023-04-17 11:42:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:42:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:42:40 | INFO | fairseq.trainer | begin training epoch 103\n",
      "2023-04-17 11:42:40 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:42:52 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:42:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:42:53 | INFO | valid | epoch 103 | valid on 'valid' subset | loss 4.023 | nll_loss 2.052 | ppl 4.15 | wps 6297.8 | wpb 290.8 | bsz 1 | num_updates 3287 | best_loss 3.979\n",
      "2023-04-17 11:42:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 103 @ 3287 updates\n",
      "2023-04-17 11:42:53 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:43:07 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:43:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 103 @ 3287 updates, score 4.023) (writing took 15.113442072994076 seconds)\n",
      "2023-04-17 11:43:08 | INFO | fairseq_cli.train | end of epoch 103 (average epoch stats below)\n",
      "2023-04-17 11:43:08 | INFO | train | epoch 103 | loss 3.614 | nll_loss 1.587 | ppl 3 | wps 774.8 | ups 1.14 | wpb 678.5 | bsz 2 | num_updates 3287 | lr 2.89483e-05 | gnorm 2.727 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 3068\n",
      "2023-04-17 11:43:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:43:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:43:08 | INFO | fairseq.trainer | begin training epoch 104\n",
      "2023-04-17 11:43:08 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:43:13 | INFO | train_inner | epoch 104:     13 / 32 loss=3.614, nll_loss=1.588, ppl=3.01, wps=785.1, ups=1.16, wpb=675.1, bsz=2, num_updates=3300, lr=2.89434e-05, gnorm=2.801, clip=100, loss_scale=0.25, train_wall=39, gb_free=13.9, wall=3073\n",
      "2023-04-17 11:43:20 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:43:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:43:20 | INFO | valid | epoch 104 | valid on 'valid' subset | loss 4.024 | nll_loss 2.028 | ppl 4.08 | wps 7190.7 | wpb 290.8 | bsz 1 | num_updates 3319 | best_loss 3.979\n",
      "2023-04-17 11:43:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 104 @ 3319 updates\n",
      "2023-04-17 11:43:20 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:43:34 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:43:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 104 @ 3319 updates, score 4.024) (writing took 14.19949520600494 seconds)\n",
      "2023-04-17 11:43:35 | INFO | fairseq_cli.train | end of epoch 104 (average epoch stats below)\n",
      "2023-04-17 11:43:35 | INFO | train | epoch 104 | loss 3.617 | nll_loss 1.593 | ppl 3.02 | wps 806.8 | ups 1.19 | wpb 678.5 | bsz 2 | num_updates 3319 | lr 2.89362e-05 | gnorm 3.394 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 3095\n",
      "2023-04-17 11:43:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:43:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:43:35 | INFO | fairseq.trainer | begin training epoch 105\n",
      "2023-04-17 11:43:35 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:43:47 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:43:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:43:47 | INFO | valid | epoch 105 | valid on 'valid' subset | loss 4.037 | nll_loss 2.057 | ppl 4.16 | wps 6991.4 | wpb 290.8 | bsz 1 | num_updates 3351 | best_loss 3.979\n",
      "2023-04-17 11:43:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 105 @ 3351 updates\n",
      "2023-04-17 11:43:47 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:44:01 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:44:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 105 @ 3351 updates, score 4.037) (writing took 13.831794881029055 seconds)\n",
      "2023-04-17 11:44:01 | INFO | fairseq_cli.train | end of epoch 105 (average epoch stats below)\n",
      "2023-04-17 11:44:01 | INFO | train | epoch 105 | loss 3.601 | nll_loss 1.571 | ppl 2.97 | wps 819.4 | ups 1.21 | wpb 678.5 | bsz 2 | num_updates 3351 | lr 2.89242e-05 | gnorm 2.649 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 3121\n",
      "2023-04-17 11:44:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:44:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:44:01 | INFO | fairseq.trainer | begin training epoch 106\n",
      "2023-04-17 11:44:01 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:44:13 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:44:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 11:44:14 | INFO | valid | epoch 106 | valid on 'valid' subset | loss 4.026 | nll_loss 2.05 | ppl 4.14 | wps 6502.2 | wpb 290.8 | bsz 1 | num_updates 3383 | best_loss 3.979\n",
      "2023-04-17 11:44:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 106 @ 3383 updates\n",
      "2023-04-17 11:44:14 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:44:28 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:44:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 106 @ 3383 updates, score 4.026) (writing took 14.348709783982486 seconds)\n",
      "2023-04-17 11:44:28 | INFO | fairseq_cli.train | end of epoch 106 (average epoch stats below)\n",
      "2023-04-17 11:44:28 | INFO | train | epoch 106 | loss 3.594 | nll_loss 1.566 | ppl 2.96 | wps 808.1 | ups 1.19 | wpb 678.5 | bsz 2 | num_updates 3383 | lr 2.89121e-05 | gnorm 2.529 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 3148\n",
      "2023-04-17 11:44:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:44:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:44:28 | INFO | fairseq.trainer | begin training epoch 107\n",
      "2023-04-17 11:44:28 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:44:35 | INFO | train_inner | epoch 107:     17 / 32 loss=3.601, nll_loss=1.573, ppl=2.98, wps=831.1, ups=1.22, wpb=678.6, bsz=2, num_updates=3400, lr=2.89057e-05, gnorm=2.803, clip=100, loss_scale=0.25, train_wall=38, gb_free=13.9, wall=3155\n",
      "2023-04-17 11:44:40 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:44:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:44:40 | INFO | valid | epoch 107 | valid on 'valid' subset | loss 4.033 | nll_loss 2.048 | ppl 4.13 | wps 6803.8 | wpb 290.8 | bsz 1 | num_updates 3415 | best_loss 3.979\n",
      "2023-04-17 11:44:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 107 @ 3415 updates\n",
      "2023-04-17 11:44:40 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:44:53 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:44:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 107 @ 3415 updates, score 4.033) (writing took 13.169490456930362 seconds)\n",
      "2023-04-17 11:44:53 | INFO | fairseq_cli.train | end of epoch 107 (average epoch stats below)\n",
      "2023-04-17 11:44:53 | INFO | train | epoch 107 | loss 3.598 | nll_loss 1.569 | ppl 2.97 | wps 858.1 | ups 1.26 | wpb 678.5 | bsz 2 | num_updates 3415 | lr 2.89e-05 | gnorm 2.61 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 3174\n",
      "2023-04-17 11:44:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:44:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:44:53 | INFO | fairseq.trainer | begin training epoch 108\n",
      "2023-04-17 11:44:53 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:45:05 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:45:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:45:06 | INFO | valid | epoch 108 | valid on 'valid' subset | loss 4.072 | nll_loss 2.088 | ppl 4.25 | wps 6890.2 | wpb 290.8 | bsz 1 | num_updates 3447 | best_loss 3.979\n",
      "2023-04-17 11:45:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 108 @ 3447 updates\n",
      "2023-04-17 11:45:06 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:45:19 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:45:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 108 @ 3447 updates, score 4.072) (writing took 14.13590855896473 seconds)\n",
      "2023-04-17 11:45:20 | INFO | fairseq_cli.train | end of epoch 108 (average epoch stats below)\n",
      "2023-04-17 11:45:20 | INFO | train | epoch 108 | loss 3.586 | nll_loss 1.556 | ppl 2.94 | wps 824.1 | ups 1.21 | wpb 678.5 | bsz 2 | num_updates 3447 | lr 2.88879e-05 | gnorm 2.649 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 3200\n",
      "2023-04-17 11:45:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:45:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:45:20 | INFO | fairseq.trainer | begin training epoch 109\n",
      "2023-04-17 11:45:20 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:45:32 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:45:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:45:32 | INFO | valid | epoch 109 | valid on 'valid' subset | loss 4.087 | nll_loss 2.116 | ppl 4.33 | wps 5232.6 | wpb 290.8 | bsz 1 | num_updates 3479 | best_loss 3.979\n",
      "2023-04-17 11:45:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 109 @ 3479 updates\n",
      "2023-04-17 11:45:32 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:45:46 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:45:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 109 @ 3479 updates, score 4.087) (writing took 13.861789216985926 seconds)\n",
      "2023-04-17 11:45:46 | INFO | fairseq_cli.train | end of epoch 109 (average epoch stats below)\n",
      "2023-04-17 11:45:46 | INFO | train | epoch 109 | loss 3.576 | nll_loss 1.544 | ppl 2.92 | wps 822.3 | ups 1.21 | wpb 678.5 | bsz 2 | num_updates 3479 | lr 2.88758e-05 | gnorm 2.803 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 3226\n",
      "2023-04-17 11:45:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:45:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:45:46 | INFO | fairseq.trainer | begin training epoch 110\n",
      "2023-04-17 11:45:46 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:45:54 | INFO | train_inner | epoch 110:     21 / 32 loss=3.588, nll_loss=1.559, ppl=2.95, wps=855.6, ups=1.26, wpb=681.2, bsz=2, num_updates=3500, lr=2.88679e-05, gnorm=2.759, clip=100, loss_scale=0.25, train_wall=37, gb_free=13.9, wall=3234\n",
      "2023-04-17 11:45:58 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:45:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:45:58 | INFO | valid | epoch 110 | valid on 'valid' subset | loss 4.091 | nll_loss 2.106 | ppl 4.31 | wps 6739.4 | wpb 290.8 | bsz 1 | num_updates 3511 | best_loss 3.979\n",
      "2023-04-17 11:45:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 110 @ 3511 updates\n",
      "2023-04-17 11:45:58 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:46:12 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:46:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 110 @ 3511 updates, score 4.091) (writing took 14.30589235399384 seconds)\n",
      "2023-04-17 11:46:13 | INFO | fairseq_cli.train | end of epoch 110 (average epoch stats below)\n",
      "2023-04-17 11:46:13 | INFO | train | epoch 110 | loss 3.571 | nll_loss 1.539 | ppl 2.91 | wps 818.2 | ups 1.21 | wpb 678.5 | bsz 2 | num_updates 3511 | lr 2.88638e-05 | gnorm 2.917 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 3253\n",
      "2023-04-17 11:46:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:46:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:46:13 | INFO | fairseq.trainer | begin training epoch 111\n",
      "2023-04-17 11:46:13 | INFO | fairseq_cli.train | Start iterating over samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 11:46:24 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:46:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:46:25 | INFO | valid | epoch 111 | valid on 'valid' subset | loss 4.079 | nll_loss 2.102 | ppl 4.29 | wps 6508.4 | wpb 290.8 | bsz 1 | num_updates 3543 | best_loss 3.979\n",
      "2023-04-17 11:46:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 111 @ 3543 updates\n",
      "2023-04-17 11:46:25 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:46:38 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:46:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 111 @ 3543 updates, score 4.079) (writing took 13.338736890931614 seconds)\n",
      "2023-04-17 11:46:38 | INFO | fairseq_cli.train | end of epoch 111 (average epoch stats below)\n",
      "2023-04-17 11:46:38 | INFO | train | epoch 111 | loss 3.569 | nll_loss 1.534 | ppl 2.9 | wps 858.6 | ups 1.27 | wpb 678.5 | bsz 2 | num_updates 3543 | lr 2.88517e-05 | gnorm 2.816 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 3278\n",
      "2023-04-17 11:46:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:46:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:46:38 | INFO | fairseq.trainer | begin training epoch 112\n",
      "2023-04-17 11:46:38 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:46:50 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:46:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:46:50 | INFO | valid | epoch 112 | valid on 'valid' subset | loss 4.087 | nll_loss 2.116 | ppl 4.34 | wps 5443.6 | wpb 290.8 | bsz 1 | num_updates 3575 | best_loss 3.979\n",
      "2023-04-17 11:46:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 112 @ 3575 updates\n",
      "2023-04-17 11:46:50 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:47:04 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:47:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 112 @ 3575 updates, score 4.087) (writing took 14.058975719031878 seconds)\n",
      "2023-04-17 11:47:05 | INFO | fairseq_cli.train | end of epoch 112 (average epoch stats below)\n",
      "2023-04-17 11:47:05 | INFO | train | epoch 112 | loss 3.559 | nll_loss 1.525 | ppl 2.88 | wps 815.7 | ups 1.2 | wpb 678.5 | bsz 2 | num_updates 3575 | lr 2.88396e-05 | gnorm 2.789 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 3305\n",
      "2023-04-17 11:47:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:47:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:47:05 | INFO | fairseq.trainer | begin training epoch 113\n",
      "2023-04-17 11:47:05 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:47:14 | INFO | train_inner | epoch 113:     25 / 32 loss=3.56, nll_loss=1.524, ppl=2.88, wps=851, ups=1.26, wpb=677.6, bsz=2, num_updates=3600, lr=2.88302e-05, gnorm=2.828, clip=100, loss_scale=0.25, train_wall=37, gb_free=13.9, wall=3314\n",
      "2023-04-17 11:47:16 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:47:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:47:16 | INFO | valid | epoch 113 | valid on 'valid' subset | loss 4.08 | nll_loss 2.096 | ppl 4.27 | wps 6800.8 | wpb 290.8 | bsz 1 | num_updates 3607 | best_loss 3.979\n",
      "2023-04-17 11:47:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 113 @ 3607 updates\n",
      "2023-04-17 11:47:16 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:47:29 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:47:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 113 @ 3607 updates, score 4.08) (writing took 13.048564972938038 seconds)\n",
      "2023-04-17 11:47:29 | INFO | fairseq_cli.train | end of epoch 113 (average epoch stats below)\n",
      "2023-04-17 11:47:29 | INFO | train | epoch 113 | loss 3.55 | nll_loss 1.515 | ppl 2.86 | wps 872.5 | ups 1.29 | wpb 678.5 | bsz 2 | num_updates 3607 | lr 2.88275e-05 | gnorm 2.899 | clip 100 | loss_scale 0.25 | train_wall 11 | gb_free 13.9 | wall 3330\n",
      "2023-04-17 11:47:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:47:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:47:29 | INFO | fairseq.trainer | begin training epoch 114\n",
      "2023-04-17 11:47:29 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:47:42 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:47:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:47:42 | INFO | valid | epoch 114 | valid on 'valid' subset | loss 4.085 | nll_loss 2.111 | ppl 4.32 | wps 5399.1 | wpb 290.8 | bsz 1 | num_updates 3639 | best_loss 3.979\n",
      "2023-04-17 11:47:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 114 @ 3639 updates\n",
      "2023-04-17 11:47:42 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:47:58 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:47:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 114 @ 3639 updates, score 4.085) (writing took 15.82478905795142 seconds)\n",
      "2023-04-17 11:47:58 | INFO | fairseq_cli.train | end of epoch 114 (average epoch stats below)\n",
      "2023-04-17 11:47:58 | INFO | train | epoch 114 | loss 3.549 | nll_loss 1.513 | ppl 2.85 | wps 762.3 | ups 1.12 | wpb 678.5 | bsz 2 | num_updates 3639 | lr 2.88155e-05 | gnorm 2.789 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 3358\n",
      "2023-04-17 11:47:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:47:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:47:58 | INFO | fairseq.trainer | begin training epoch 115\n",
      "2023-04-17 11:47:58 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:48:11 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:48:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:48:11 | INFO | valid | epoch 115 | valid on 'valid' subset | loss 4.112 | nll_loss 2.127 | ppl 4.37 | wps 6182.2 | wpb 290.8 | bsz 1 | num_updates 3671 | best_loss 3.979\n",
      "2023-04-17 11:48:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 115 @ 3671 updates\n",
      "2023-04-17 11:48:11 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:48:24 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:48:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 115 @ 3671 updates, score 4.112) (writing took 14.03328082698863 seconds)\n",
      "2023-04-17 11:48:25 | INFO | fairseq_cli.train | end of epoch 115 (average epoch stats below)\n",
      "2023-04-17 11:48:25 | INFO | train | epoch 115 | loss 3.544 | nll_loss 1.505 | ppl 2.84 | wps 807.5 | ups 1.19 | wpb 678.5 | bsz 2 | num_updates 3671 | lr 2.88034e-05 | gnorm 3.046 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 3385\n",
      "2023-04-17 11:48:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:48:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:48:25 | INFO | fairseq.trainer | begin training epoch 116\n",
      "2023-04-17 11:48:25 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:48:36 | INFO | train_inner | epoch 116:     29 / 32 loss=3.54, nll_loss=1.501, ppl=2.83, wps=828.4, ups=1.22, wpb=681, bsz=2, num_updates=3700, lr=2.87925e-05, gnorm=2.849, clip=100, loss_scale=0.25, train_wall=38, gb_free=13.9, wall=3396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 11:48:37 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:48:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:48:37 | INFO | valid | epoch 116 | valid on 'valid' subset | loss 4.091 | nll_loss 2.111 | ppl 4.32 | wps 6391.9 | wpb 290.8 | bsz 1 | num_updates 3703 | best_loss 3.979\n",
      "2023-04-17 11:48:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 116 @ 3703 updates\n",
      "2023-04-17 11:48:37 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:48:51 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:48:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 116 @ 3703 updates, score 4.091) (writing took 14.276588250999339 seconds)\n",
      "2023-04-17 11:48:52 | INFO | fairseq_cli.train | end of epoch 116 (average epoch stats below)\n",
      "2023-04-17 11:48:52 | INFO | train | epoch 116 | loss 3.528 | nll_loss 1.489 | ppl 2.81 | wps 812.5 | ups 1.2 | wpb 678.5 | bsz 2 | num_updates 3703 | lr 2.87913e-05 | gnorm 2.808 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 3412\n",
      "2023-04-17 11:48:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:48:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:48:52 | INFO | fairseq.trainer | begin training epoch 117\n",
      "2023-04-17 11:48:52 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:49:04 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:49:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:49:04 | INFO | valid | epoch 117 | valid on 'valid' subset | loss 4.138 | nll_loss 2.139 | ppl 4.41 | wps 6266.3 | wpb 290.8 | bsz 1 | num_updates 3735 | best_loss 3.979\n",
      "2023-04-17 11:49:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 117 @ 3735 updates\n",
      "2023-04-17 11:49:04 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:49:17 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:49:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 117 @ 3735 updates, score 4.138) (writing took 13.444965449976735 seconds)\n",
      "2023-04-17 11:49:17 | INFO | fairseq_cli.train | end of epoch 117 (average epoch stats below)\n",
      "2023-04-17 11:49:17 | INFO | train | epoch 117 | loss 3.525 | nll_loss 1.485 | ppl 2.8 | wps 839.3 | ups 1.24 | wpb 678.5 | bsz 2 | num_updates 3735 | lr 2.87792e-05 | gnorm 2.877 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 3438\n",
      "2023-04-17 11:49:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:49:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:49:17 | INFO | fairseq.trainer | begin training epoch 118\n",
      "2023-04-17 11:49:17 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:49:30 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:49:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:49:30 | INFO | valid | epoch 118 | valid on 'valid' subset | loss 4.09 | nll_loss 2.119 | ppl 4.34 | wps 5501.2 | wpb 290.8 | bsz 1 | num_updates 3767 | best_loss 3.979\n",
      "2023-04-17 11:49:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 118 @ 3767 updates\n",
      "2023-04-17 11:49:30 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:49:44 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:49:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 118 @ 3767 updates, score 4.09) (writing took 13.559047730988823 seconds)\n",
      "2023-04-17 11:49:44 | INFO | fairseq_cli.train | end of epoch 118 (average epoch stats below)\n",
      "2023-04-17 11:49:44 | INFO | train | epoch 118 | loss 3.523 | nll_loss 1.48 | ppl 2.79 | wps 824.2 | ups 1.21 | wpb 678.5 | bsz 2 | num_updates 3767 | lr 2.87672e-05 | gnorm 3.278 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 3464\n",
      "2023-04-17 11:49:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:49:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:49:44 | INFO | fairseq.trainer | begin training epoch 119\n",
      "2023-04-17 11:49:44 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:49:56 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:49:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:49:56 | INFO | valid | epoch 119 | valid on 'valid' subset | loss 4.098 | nll_loss 2.111 | ppl 4.32 | wps 6644.1 | wpb 290.8 | bsz 1 | num_updates 3799 | best_loss 3.979\n",
      "2023-04-17 11:49:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 119 @ 3799 updates\n",
      "2023-04-17 11:49:56 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:50:09 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:50:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 119 @ 3799 updates, score 4.098) (writing took 13.34660549997352 seconds)\n",
      "2023-04-17 11:50:09 | INFO | fairseq_cli.train | end of epoch 119 (average epoch stats below)\n",
      "2023-04-17 11:50:09 | INFO | train | epoch 119 | loss 3.511 | nll_loss 1.466 | ppl 2.76 | wps 847.8 | ups 1.25 | wpb 678.5 | bsz 2 | num_updates 3799 | lr 2.87551e-05 | gnorm 3.052 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 3490\n",
      "2023-04-17 11:50:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:50:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:50:09 | INFO | fairseq.trainer | begin training epoch 120\n",
      "2023-04-17 11:50:09 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:50:10 | INFO | train_inner | epoch 120:      1 / 32 loss=3.521, nll_loss=1.478, ppl=2.79, wps=715.9, ups=1.07, wpb=671.6, bsz=2, num_updates=3800, lr=2.87547e-05, gnorm=3.093, clip=100, loss_scale=0.25, train_wall=37, gb_free=13.9, wall=3490\n",
      "2023-04-17 11:50:21 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:50:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:50:22 | INFO | valid | epoch 120 | valid on 'valid' subset | loss 4.082 | nll_loss 2.106 | ppl 4.3 | wps 6762 | wpb 290.8 | bsz 1 | num_updates 3831 | best_loss 3.979\n",
      "2023-04-17 11:50:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 120 @ 3831 updates\n",
      "2023-04-17 11:50:22 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:50:35 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:50:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 120 @ 3831 updates, score 4.082) (writing took 13.710642116959207 seconds)\n",
      "2023-04-17 11:50:35 | INFO | fairseq_cli.train | end of epoch 120 (average epoch stats below)\n",
      "2023-04-17 11:50:35 | INFO | train | epoch 120 | loss 3.503 | nll_loss 1.459 | ppl 2.75 | wps 835.6 | ups 1.23 | wpb 678.5 | bsz 2 | num_updates 3831 | lr 2.8743e-05 | gnorm 3.058 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 3516\n",
      "2023-04-17 11:50:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:50:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:50:35 | INFO | fairseq.trainer | begin training epoch 121\n",
      "2023-04-17 11:50:35 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:50:47 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:50:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 11:50:48 | INFO | valid | epoch 121 | valid on 'valid' subset | loss 4.131 | nll_loss 2.153 | ppl 4.45 | wps 6467.2 | wpb 290.8 | bsz 1 | num_updates 3863 | best_loss 3.979\n",
      "2023-04-17 11:50:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 121 @ 3863 updates\n",
      "2023-04-17 11:50:48 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:51:00 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:51:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 121 @ 3863 updates, score 4.131) (writing took 13.066532399971038 seconds)\n",
      "2023-04-17 11:51:01 | INFO | fairseq_cli.train | end of epoch 121 (average epoch stats below)\n",
      "2023-04-17 11:51:01 | INFO | train | epoch 121 | loss 3.495 | nll_loss 1.448 | ppl 2.73 | wps 858.2 | ups 1.26 | wpb 678.5 | bsz 2 | num_updates 3863 | lr 2.87309e-05 | gnorm 3.094 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 3541\n",
      "2023-04-17 11:51:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:51:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:51:01 | INFO | fairseq.trainer | begin training epoch 122\n",
      "2023-04-17 11:51:01 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:51:13 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:51:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:51:13 | INFO | valid | epoch 122 | valid on 'valid' subset | loss 4.122 | nll_loss 2.143 | ppl 4.42 | wps 6676.7 | wpb 290.8 | bsz 1 | num_updates 3895 | best_loss 3.979\n",
      "2023-04-17 11:51:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 122 @ 3895 updates\n",
      "2023-04-17 11:51:13 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:51:28 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:51:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 122 @ 3895 updates, score 4.122) (writing took 14.800783881917596 seconds)\n",
      "2023-04-17 11:51:28 | INFO | fairseq_cli.train | end of epoch 122 (average epoch stats below)\n",
      "2023-04-17 11:51:28 | INFO | train | epoch 122 | loss 3.483 | nll_loss 1.432 | ppl 2.7 | wps 792.3 | ups 1.17 | wpb 678.5 | bsz 2 | num_updates 3895 | lr 2.87189e-05 | gnorm 3.026 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 3568\n",
      "2023-04-17 11:51:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:51:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:51:28 | INFO | fairseq.trainer | begin training epoch 123\n",
      "2023-04-17 11:51:28 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:51:30 | INFO | train_inner | epoch 123:      5 / 32 loss=3.495, nll_loss=1.448, ppl=2.73, wps=854.2, ups=1.25, wpb=684.8, bsz=2, num_updates=3900, lr=2.8717e-05, gnorm=3.043, clip=100, loss_scale=0.25, train_wall=37, gb_free=13.9, wall=3570\n",
      "2023-04-17 11:51:40 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:51:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:51:40 | INFO | valid | epoch 123 | valid on 'valid' subset | loss 4.15 | nll_loss 2.161 | ppl 4.47 | wps 6292.4 | wpb 290.8 | bsz 1 | num_updates 3927 | best_loss 3.979\n",
      "2023-04-17 11:51:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 123 @ 3927 updates\n",
      "2023-04-17 11:51:40 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:51:54 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:51:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 123 @ 3927 updates, score 4.15) (writing took 13.369095001951791 seconds)\n",
      "2023-04-17 11:51:54 | INFO | fairseq_cli.train | end of epoch 123 (average epoch stats below)\n",
      "2023-04-17 11:51:54 | INFO | train | epoch 123 | loss 3.482 | nll_loss 1.433 | ppl 2.7 | wps 842.6 | ups 1.24 | wpb 678.5 | bsz 2 | num_updates 3927 | lr 2.87068e-05 | gnorm 3.228 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 3594\n",
      "2023-04-17 11:51:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:51:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:51:54 | INFO | fairseq.trainer | begin training epoch 124\n",
      "2023-04-17 11:51:54 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:52:06 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:52:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:52:07 | INFO | valid | epoch 124 | valid on 'valid' subset | loss 4.17 | nll_loss 2.197 | ppl 4.59 | wps 6266.6 | wpb 290.8 | bsz 1 | num_updates 3959 | best_loss 3.979\n",
      "2023-04-17 11:52:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 124 @ 3959 updates\n",
      "2023-04-17 11:52:07 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:52:21 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:52:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 124 @ 3959 updates, score 4.17) (writing took 14.520065332995728 seconds)\n",
      "2023-04-17 11:52:21 | INFO | fairseq_cli.train | end of epoch 124 (average epoch stats below)\n",
      "2023-04-17 11:52:21 | INFO | train | epoch 124 | loss 3.47 | nll_loss 1.42 | ppl 2.68 | wps 797.9 | ups 1.18 | wpb 678.5 | bsz 2 | num_updates 3959 | lr 2.86947e-05 | gnorm 3.387 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 3621\n",
      "2023-04-17 11:52:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:52:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:52:21 | INFO | fairseq.trainer | begin training epoch 125\n",
      "2023-04-17 11:52:21 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:52:33 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:52:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:52:34 | INFO | valid | epoch 125 | valid on 'valid' subset | loss 4.137 | nll_loss 2.162 | ppl 4.48 | wps 6356.8 | wpb 290.8 | bsz 1 | num_updates 3991 | best_loss 3.979\n",
      "2023-04-17 11:52:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 125 @ 3991 updates\n",
      "2023-04-17 11:52:34 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:52:47 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:52:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 125 @ 3991 updates, score 4.137) (writing took 13.890017142985016 seconds)\n",
      "2023-04-17 11:52:47 | INFO | fairseq_cli.train | end of epoch 125 (average epoch stats below)\n",
      "2023-04-17 11:52:47 | INFO | train | epoch 125 | loss 3.469 | nll_loss 1.416 | ppl 2.67 | wps 822.6 | ups 1.21 | wpb 678.5 | bsz 2 | num_updates 3991 | lr 2.86826e-05 | gnorm 3.484 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 3648\n",
      "2023-04-17 11:52:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:52:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:52:47 | INFO | fairseq.trainer | begin training epoch 126\n",
      "2023-04-17 11:52:47 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:52:51 | INFO | train_inner | epoch 126:      9 / 32 loss=3.471, nll_loss=1.419, ppl=2.67, wps=835.2, ups=1.24, wpb=676.1, bsz=2, num_updates=4000, lr=2.86792e-05, gnorm=3.361, clip=100, loss_scale=0.25, train_wall=38, gb_free=13.9, wall=3651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 11:52:59 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:52:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:53:00 | INFO | valid | epoch 126 | valid on 'valid' subset | loss 4.19 | nll_loss 2.217 | ppl 4.65 | wps 6268.8 | wpb 290.8 | bsz 1 | num_updates 4023 | best_loss 3.979\n",
      "2023-04-17 11:53:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 126 @ 4023 updates\n",
      "2023-04-17 11:53:00 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:53:14 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:53:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 126 @ 4023 updates, score 4.19) (writing took 14.097372469026595 seconds)\n",
      "2023-04-17 11:53:14 | INFO | fairseq_cli.train | end of epoch 126 (average epoch stats below)\n",
      "2023-04-17 11:53:14 | INFO | train | epoch 126 | loss 3.457 | nll_loss 1.405 | ppl 2.65 | wps 823.3 | ups 1.21 | wpb 678.5 | bsz 2 | num_updates 4023 | lr 2.86706e-05 | gnorm 3.209 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 3674\n",
      "2023-04-17 11:53:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:53:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:53:14 | INFO | fairseq.trainer | begin training epoch 127\n",
      "2023-04-17 11:53:14 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:53:26 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:53:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:53:27 | INFO | valid | epoch 127 | valid on 'valid' subset | loss 4.169 | nll_loss 2.205 | ppl 4.61 | wps 6590.1 | wpb 290.8 | bsz 1 | num_updates 4055 | best_loss 3.979\n",
      "2023-04-17 11:53:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 127 @ 4055 updates\n",
      "2023-04-17 11:53:27 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:53:40 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:53:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 127 @ 4055 updates, score 4.169) (writing took 13.282398659037426 seconds)\n",
      "2023-04-17 11:53:40 | INFO | fairseq_cli.train | end of epoch 127 (average epoch stats below)\n",
      "2023-04-17 11:53:40 | INFO | train | epoch 127 | loss 3.452 | nll_loss 1.397 | ppl 2.63 | wps 829.3 | ups 1.22 | wpb 678.5 | bsz 2 | num_updates 4055 | lr 2.86585e-05 | gnorm 3.359 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 3700\n",
      "2023-04-17 11:53:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:53:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:53:40 | INFO | fairseq.trainer | begin training epoch 128\n",
      "2023-04-17 11:53:40 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:53:52 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:53:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:53:53 | INFO | valid | epoch 128 | valid on 'valid' subset | loss 4.189 | nll_loss 2.2 | ppl 4.59 | wps 6484.6 | wpb 290.8 | bsz 1 | num_updates 4087 | best_loss 3.979\n",
      "2023-04-17 11:53:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 128 @ 4087 updates\n",
      "2023-04-17 11:53:53 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:54:07 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:54:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 128 @ 4087 updates, score 4.189) (writing took 14.42143019195646 seconds)\n",
      "2023-04-17 11:54:07 | INFO | fairseq_cli.train | end of epoch 128 (average epoch stats below)\n",
      "2023-04-17 11:54:07 | INFO | train | epoch 128 | loss 3.435 | nll_loss 1.379 | ppl 2.6 | wps 805.8 | ups 1.19 | wpb 678.5 | bsz 2 | num_updates 4087 | lr 2.86464e-05 | gnorm 3.372 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 3727\n",
      "2023-04-17 11:54:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:54:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:54:07 | INFO | fairseq.trainer | begin training epoch 129\n",
      "2023-04-17 11:54:07 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:54:12 | INFO | train_inner | epoch 129:     13 / 32 loss=3.443, nll_loss=1.387, ppl=2.61, wps=841.8, ups=1.24, wpb=681.5, bsz=2, num_updates=4100, lr=2.86415e-05, gnorm=3.339, clip=100, loss_scale=0.25, train_wall=38, gb_free=13.9, wall=3732\n",
      "2023-04-17 11:54:19 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:54:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:54:19 | INFO | valid | epoch 129 | valid on 'valid' subset | loss 4.194 | nll_loss 2.209 | ppl 4.62 | wps 6428.1 | wpb 290.8 | bsz 1 | num_updates 4119 | best_loss 3.979\n",
      "2023-04-17 11:54:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 129 @ 4119 updates\n",
      "2023-04-17 11:54:19 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:54:32 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:54:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 129 @ 4119 updates, score 4.194) (writing took 13.591639016987756 seconds)\n",
      "2023-04-17 11:54:33 | INFO | fairseq_cli.train | end of epoch 129 (average epoch stats below)\n",
      "2023-04-17 11:54:33 | INFO | train | epoch 129 | loss 3.429 | nll_loss 1.371 | ppl 2.59 | wps 844 | ups 1.24 | wpb 678.5 | bsz 2 | num_updates 4119 | lr 2.86343e-05 | gnorm 3.411 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 3753\n",
      "2023-04-17 11:54:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:54:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:54:33 | INFO | fairseq.trainer | begin training epoch 130\n",
      "2023-04-17 11:54:33 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:54:45 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:54:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:54:45 | INFO | valid | epoch 130 | valid on 'valid' subset | loss 4.205 | nll_loss 2.237 | ppl 4.71 | wps 6699.7 | wpb 290.8 | bsz 1 | num_updates 4151 | best_loss 3.979\n",
      "2023-04-17 11:54:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 130 @ 4151 updates\n",
      "2023-04-17 11:54:45 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:54:59 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:54:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 130 @ 4151 updates, score 4.205) (writing took 13.97587239393033 seconds)\n",
      "2023-04-17 11:54:59 | INFO | fairseq_cli.train | end of epoch 130 (average epoch stats below)\n",
      "2023-04-17 11:54:59 | INFO | train | epoch 130 | loss 3.422 | nll_loss 1.363 | ppl 2.57 | wps 830.9 | ups 1.22 | wpb 678.5 | bsz 2 | num_updates 4151 | lr 2.86223e-05 | gnorm 3.485 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 3779\n",
      "2023-04-17 11:54:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:54:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:54:59 | INFO | fairseq.trainer | begin training epoch 131\n",
      "2023-04-17 11:54:59 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:55:11 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:55:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 11:55:11 | INFO | valid | epoch 131 | valid on 'valid' subset | loss 4.181 | nll_loss 2.208 | ppl 4.62 | wps 6447.4 | wpb 290.8 | bsz 1 | num_updates 4183 | best_loss 3.979\n",
      "2023-04-17 11:55:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 131 @ 4183 updates\n",
      "2023-04-17 11:55:11 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:55:24 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:55:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 131 @ 4183 updates, score 4.181) (writing took 13.07044154102914 seconds)\n",
      "2023-04-17 11:55:24 | INFO | fairseq_cli.train | end of epoch 131 (average epoch stats below)\n",
      "2023-04-17 11:55:25 | INFO | train | epoch 131 | loss 3.41 | nll_loss 1.349 | ppl 2.55 | wps 844.7 | ups 1.24 | wpb 678.5 | bsz 2 | num_updates 4183 | lr 2.86102e-05 | gnorm 3.312 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 3805\n",
      "2023-04-17 11:55:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:55:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:55:25 | INFO | fairseq.trainer | begin training epoch 132\n",
      "2023-04-17 11:55:25 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:55:31 | INFO | train_inner | epoch 132:     17 / 32 loss=3.416, nll_loss=1.355, ppl=2.56, wps=842.5, ups=1.26, wpb=669.4, bsz=2, num_updates=4200, lr=2.86038e-05, gnorm=3.462, clip=100, loss_scale=0.25, train_wall=37, gb_free=13.7, wall=3812\n",
      "2023-04-17 11:55:37 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:55:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:55:37 | INFO | valid | epoch 132 | valid on 'valid' subset | loss 4.203 | nll_loss 2.22 | ppl 4.66 | wps 6202 | wpb 290.8 | bsz 1 | num_updates 4215 | best_loss 3.979\n",
      "2023-04-17 11:55:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 132 @ 4215 updates\n",
      "2023-04-17 11:55:37 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:55:51 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:55:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 132 @ 4215 updates, score 4.203) (writing took 13.938395061064512 seconds)\n",
      "2023-04-17 11:55:51 | INFO | fairseq_cli.train | end of epoch 132 (average epoch stats below)\n",
      "2023-04-17 11:55:51 | INFO | train | epoch 132 | loss 3.4 | nll_loss 1.337 | ppl 2.53 | wps 816.7 | ups 1.2 | wpb 678.5 | bsz 2 | num_updates 4215 | lr 2.85981e-05 | gnorm 3.55 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 3831\n",
      "2023-04-17 11:55:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:55:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:55:51 | INFO | fairseq.trainer | begin training epoch 133\n",
      "2023-04-17 11:55:51 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:56:03 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:56:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:56:04 | INFO | valid | epoch 133 | valid on 'valid' subset | loss 4.181 | nll_loss 2.207 | ppl 4.62 | wps 7111 | wpb 290.8 | bsz 1 | num_updates 4247 | best_loss 3.979\n",
      "2023-04-17 11:56:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 133 @ 4247 updates\n",
      "2023-04-17 11:56:04 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:56:16 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:56:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 133 @ 4247 updates, score 4.181) (writing took 13.002952617011033 seconds)\n",
      "2023-04-17 11:56:17 | INFO | fairseq_cli.train | end of epoch 133 (average epoch stats below)\n",
      "2023-04-17 11:56:17 | INFO | train | epoch 133 | loss 3.404 | nll_loss 1.34 | ppl 2.53 | wps 850.8 | ups 1.25 | wpb 678.5 | bsz 2 | num_updates 4247 | lr 2.8586e-05 | gnorm 3.555 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 3857\n",
      "2023-04-17 11:56:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:56:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:56:17 | INFO | fairseq.trainer | begin training epoch 134\n",
      "2023-04-17 11:56:17 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:56:28 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:56:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:56:28 | INFO | valid | epoch 134 | valid on 'valid' subset | loss 4.201 | nll_loss 2.21 | ppl 4.63 | wps 6823.7 | wpb 290.8 | bsz 1 | num_updates 4279 | best_loss 3.979\n",
      "2023-04-17 11:56:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 134 @ 4279 updates\n",
      "2023-04-17 11:56:28 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:56:42 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:56:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 134 @ 4279 updates, score 4.201) (writing took 13.807814026949927 seconds)\n",
      "2023-04-17 11:56:42 | INFO | fairseq_cli.train | end of epoch 134 (average epoch stats below)\n",
      "2023-04-17 11:56:42 | INFO | train | epoch 134 | loss 3.392 | nll_loss 1.328 | ppl 2.51 | wps 848.8 | ups 1.25 | wpb 678.5 | bsz 2 | num_updates 4279 | lr 2.8574e-05 | gnorm 3.498 | clip 100 | loss_scale 0.25 | train_wall 11 | gb_free 13.9 | wall 3882\n",
      "2023-04-17 11:56:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:56:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:56:42 | INFO | fairseq.trainer | begin training epoch 135\n",
      "2023-04-17 11:56:42 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:56:50 | INFO | train_inner | epoch 135:     21 / 32 loss=3.394, nll_loss=1.33, ppl=2.51, wps=869.6, ups=1.27, wpb=684.1, bsz=2, num_updates=4300, lr=2.8566e-05, gnorm=3.518, clip=100, loss_scale=0.25, train_wall=37, gb_free=13.9, wall=3890\n",
      "2023-04-17 11:56:54 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:56:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:56:54 | INFO | valid | epoch 135 | valid on 'valid' subset | loss 4.231 | nll_loss 2.264 | ppl 4.8 | wps 6994.7 | wpb 290.8 | bsz 1 | num_updates 4311 | best_loss 3.979\n",
      "2023-04-17 11:56:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 135 @ 4311 updates\n",
      "2023-04-17 11:56:54 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:57:07 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:57:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 135 @ 4311 updates, score 4.231) (writing took 13.133333643898368 seconds)\n",
      "2023-04-17 11:57:07 | INFO | fairseq_cli.train | end of epoch 135 (average epoch stats below)\n",
      "2023-04-17 11:57:07 | INFO | train | epoch 135 | loss 3.378 | nll_loss 1.308 | ppl 2.48 | wps 864.5 | ups 1.27 | wpb 678.5 | bsz 2 | num_updates 4311 | lr 2.85619e-05 | gnorm 3.587 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 3908\n",
      "2023-04-17 11:57:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:57:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:57:07 | INFO | fairseq.trainer | begin training epoch 136\n",
      "2023-04-17 11:57:07 | INFO | fairseq_cli.train | Start iterating over samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 11:57:18 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:57:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:57:18 | INFO | valid | epoch 136 | valid on 'valid' subset | loss 4.203 | nll_loss 2.237 | ppl 4.71 | wps 7313.8 | wpb 290.8 | bsz 1 | num_updates 4343 | best_loss 3.979\n",
      "2023-04-17 11:57:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 136 @ 4343 updates\n",
      "2023-04-17 11:57:18 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:57:31 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:57:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 136 @ 4343 updates, score 4.203) (writing took 12.582161496044137 seconds)\n",
      "2023-04-17 11:57:31 | INFO | fairseq_cli.train | end of epoch 136 (average epoch stats below)\n",
      "2023-04-17 11:57:31 | INFO | train | epoch 136 | loss 3.376 | nll_loss 1.308 | ppl 2.48 | wps 922.9 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 4343 | lr 2.85498e-05 | gnorm 3.632 | clip 100 | loss_scale 0.25 | train_wall 11 | gb_free 13.9 | wall 3931\n",
      "2023-04-17 11:57:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:57:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:57:31 | INFO | fairseq.trainer | begin training epoch 137\n",
      "2023-04-17 11:57:31 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:57:42 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:57:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:57:43 | INFO | valid | epoch 137 | valid on 'valid' subset | loss 4.251 | nll_loss 2.296 | ppl 4.91 | wps 7276.8 | wpb 290.8 | bsz 1 | num_updates 4375 | best_loss 3.979\n",
      "2023-04-17 11:57:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 137 @ 4375 updates\n",
      "2023-04-17 11:57:43 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:57:56 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:57:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 137 @ 4375 updates, score 4.251) (writing took 13.146921275998466 seconds)\n",
      "2023-04-17 11:57:56 | INFO | fairseq_cli.train | end of epoch 137 (average epoch stats below)\n",
      "2023-04-17 11:57:56 | INFO | train | epoch 137 | loss 3.358 | nll_loss 1.288 | ppl 2.44 | wps 867.3 | ups 1.28 | wpb 678.5 | bsz 2 | num_updates 4375 | lr 2.85377e-05 | gnorm 3.502 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 3956\n",
      "2023-04-17 11:57:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:57:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:57:56 | INFO | fairseq.trainer | begin training epoch 138\n",
      "2023-04-17 11:57:56 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:58:05 | INFO | train_inner | epoch 138:     25 / 32 loss=3.365, nll_loss=1.296, ppl=2.45, wps=900.4, ups=1.33, wpb=674.8, bsz=2, num_updates=4400, lr=2.85283e-05, gnorm=3.587, clip=100, loss_scale=0.25, train_wall=35, gb_free=13.9, wall=3965\n",
      "2023-04-17 11:58:07 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:58:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:58:08 | INFO | valid | epoch 138 | valid on 'valid' subset | loss 4.239 | nll_loss 2.29 | ppl 4.89 | wps 6229 | wpb 290.8 | bsz 1 | num_updates 4407 | best_loss 3.979\n",
      "2023-04-17 11:58:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 138 @ 4407 updates\n",
      "2023-04-17 11:58:08 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:58:21 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:58:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 138 @ 4407 updates, score 4.239) (writing took 13.26631938898936 seconds)\n",
      "2023-04-17 11:58:21 | INFO | fairseq_cli.train | end of epoch 138 (average epoch stats below)\n",
      "2023-04-17 11:58:21 | INFO | train | epoch 138 | loss 3.355 | nll_loss 1.283 | ppl 2.43 | wps 866.6 | ups 1.28 | wpb 678.5 | bsz 2 | num_updates 4407 | lr 2.85257e-05 | gnorm 3.651 | clip 100 | loss_scale 0.25 | train_wall 11 | gb_free 13.9 | wall 3981\n",
      "2023-04-17 11:58:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:58:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:58:21 | INFO | fairseq.trainer | begin training epoch 139\n",
      "2023-04-17 11:58:21 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:58:32 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:58:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:58:33 | INFO | valid | epoch 139 | valid on 'valid' subset | loss 4.281 | nll_loss 2.293 | ppl 4.9 | wps 7202.3 | wpb 290.8 | bsz 1 | num_updates 4439 | best_loss 3.979\n",
      "2023-04-17 11:58:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 139 @ 4439 updates\n",
      "2023-04-17 11:58:33 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:58:46 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:58:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 139 @ 4439 updates, score 4.281) (writing took 13.282930045970716 seconds)\n",
      "2023-04-17 11:58:46 | INFO | fairseq_cli.train | end of epoch 139 (average epoch stats below)\n",
      "2023-04-17 11:58:46 | INFO | train | epoch 139 | loss 3.342 | nll_loss 1.269 | ppl 2.41 | wps 866.9 | ups 1.28 | wpb 678.5 | bsz 2 | num_updates 4439 | lr 2.85136e-05 | gnorm 4.013 | clip 100 | loss_scale 0.25 | train_wall 11 | gb_free 13.9 | wall 4006\n",
      "2023-04-17 11:58:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:58:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:58:46 | INFO | fairseq.trainer | begin training epoch 140\n",
      "2023-04-17 11:58:46 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:58:58 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:58:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:58:58 | INFO | valid | epoch 140 | valid on 'valid' subset | loss 4.311 | nll_loss 2.349 | ppl 5.09 | wps 7337.2 | wpb 290.8 | bsz 1 | num_updates 4471 | best_loss 3.979\n",
      "2023-04-17 11:58:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 140 @ 4471 updates\n",
      "2023-04-17 11:58:58 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:59:10 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:59:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 140 @ 4471 updates, score 4.311) (writing took 12.602359106065705 seconds)\n",
      "2023-04-17 11:59:10 | INFO | fairseq_cli.train | end of epoch 140 (average epoch stats below)\n",
      "2023-04-17 11:59:10 | INFO | train | epoch 140 | loss 3.33 | nll_loss 1.254 | ppl 2.39 | wps 889.8 | ups 1.31 | wpb 678.5 | bsz 2 | num_updates 4471 | lr 2.85015e-05 | gnorm 3.732 | clip 100 | loss_scale 0.25 | train_wall 11 | gb_free 13.9 | wall 4031\n",
      "2023-04-17 11:59:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:59:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:59:10 | INFO | fairseq.trainer | begin training epoch 141\n",
      "2023-04-17 11:59:10 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:59:21 | INFO | train_inner | epoch 141:     29 / 32 loss=3.333, nll_loss=1.257, ppl=2.39, wps=896.9, ups=1.32, wpb=680.8, bsz=2, num_updates=4500, lr=2.84906e-05, gnorm=3.828, clip=100, loss_scale=0.25, train_wall=36, gb_free=13.9, wall=4041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 11:59:22 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:59:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:59:22 | INFO | valid | epoch 141 | valid on 'valid' subset | loss 4.268 | nll_loss 2.298 | ppl 4.92 | wps 7254.7 | wpb 290.8 | bsz 1 | num_updates 4503 | best_loss 3.979\n",
      "2023-04-17 11:59:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 141 @ 4503 updates\n",
      "2023-04-17 11:59:22 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:59:34 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:59:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 141 @ 4503 updates, score 4.268) (writing took 12.132965552969836 seconds)\n",
      "2023-04-17 11:59:34 | INFO | fairseq_cli.train | end of epoch 141 (average epoch stats below)\n",
      "2023-04-17 11:59:34 | INFO | train | epoch 141 | loss 3.328 | nll_loss 1.251 | ppl 2.38 | wps 913.6 | ups 1.35 | wpb 678.5 | bsz 2 | num_updates 4503 | lr 2.84894e-05 | gnorm 3.763 | clip 100 | loss_scale 0.25 | train_wall 11 | gb_free 13.9 | wall 4054\n",
      "2023-04-17 11:59:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:59:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:59:34 | INFO | fairseq.trainer | begin training epoch 142\n",
      "2023-04-17 11:59:34 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 11:59:45 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 11:59:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:59:46 | INFO | valid | epoch 142 | valid on 'valid' subset | loss 4.325 | nll_loss 2.364 | ppl 5.15 | wps 7259.1 | wpb 290.8 | bsz 1 | num_updates 4535 | best_loss 3.979\n",
      "2023-04-17 11:59:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 142 @ 4535 updates\n",
      "2023-04-17 11:59:46 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:59:58 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 11:59:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 142 @ 4535 updates, score 4.325) (writing took 13.012820236035623 seconds)\n",
      "2023-04-17 11:59:59 | INFO | fairseq_cli.train | end of epoch 142 (average epoch stats below)\n",
      "2023-04-17 11:59:59 | INFO | train | epoch 142 | loss 3.319 | nll_loss 1.242 | ppl 2.36 | wps 885.3 | ups 1.3 | wpb 678.5 | bsz 2 | num_updates 4535 | lr 2.84774e-05 | gnorm 3.603 | clip 100 | loss_scale 0.25 | train_wall 11 | gb_free 13.9 | wall 4079\n",
      "2023-04-17 11:59:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 11:59:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 11:59:59 | INFO | fairseq.trainer | begin training epoch 143\n",
      "2023-04-17 11:59:59 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:00:10 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:00:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:00:10 | INFO | valid | epoch 143 | valid on 'valid' subset | loss 4.283 | nll_loss 2.308 | ppl 4.95 | wps 7306.4 | wpb 290.8 | bsz 1 | num_updates 4567 | best_loss 3.979\n",
      "2023-04-17 12:00:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 143 @ 4567 updates\n",
      "2023-04-17 12:00:10 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:00:22 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:00:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 143 @ 4567 updates, score 4.283) (writing took 12.362959534977563 seconds)\n",
      "2023-04-17 12:00:22 | INFO | fairseq_cli.train | end of epoch 143 (average epoch stats below)\n",
      "2023-04-17 12:00:22 | INFO | train | epoch 143 | loss 3.307 | nll_loss 1.228 | ppl 2.34 | wps 914.7 | ups 1.35 | wpb 678.5 | bsz 2 | num_updates 4567 | lr 2.84653e-05 | gnorm 3.835 | clip 100 | loss_scale 0.25 | train_wall 11 | gb_free 13.9 | wall 4103\n",
      "2023-04-17 12:00:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:00:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:00:22 | INFO | fairseq.trainer | begin training epoch 144\n",
      "2023-04-17 12:00:22 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:00:34 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:00:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:00:34 | INFO | valid | epoch 144 | valid on 'valid' subset | loss 4.316 | nll_loss 2.354 | ppl 5.11 | wps 7259.5 | wpb 290.8 | bsz 1 | num_updates 4599 | best_loss 3.979\n",
      "2023-04-17 12:00:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 144 @ 4599 updates\n",
      "2023-04-17 12:00:34 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:00:46 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:00:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 144 @ 4599 updates, score 4.316) (writing took 12.75158503698185 seconds)\n",
      "2023-04-17 12:00:47 | INFO | fairseq_cli.train | end of epoch 144 (average epoch stats below)\n",
      "2023-04-17 12:00:47 | INFO | train | epoch 144 | loss 3.296 | nll_loss 1.215 | ppl 2.32 | wps 900.8 | ups 1.33 | wpb 678.5 | bsz 2 | num_updates 4599 | lr 2.84532e-05 | gnorm 3.933 | clip 100 | loss_scale 0.25 | train_wall 11 | gb_free 13.9 | wall 4127\n",
      "2023-04-17 12:00:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:00:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:00:47 | INFO | fairseq.trainer | begin training epoch 145\n",
      "2023-04-17 12:00:47 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:00:47 | INFO | train_inner | epoch 145:      1 / 32 loss=3.31, nll_loss=1.232, ppl=2.35, wps=790.6, ups=1.16, wpb=680.5, bsz=2, num_updates=4600, lr=2.84528e-05, gnorm=3.793, clip=100, loss_scale=0.25, train_wall=34, gb_free=13.9, wall=4127\n",
      "2023-04-17 12:00:58 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:00:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:00:58 | INFO | valid | epoch 145 | valid on 'valid' subset | loss 4.327 | nll_loss 2.359 | ppl 5.13 | wps 7280.2 | wpb 290.8 | bsz 1 | num_updates 4631 | best_loss 3.979\n",
      "2023-04-17 12:00:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 145 @ 4631 updates\n",
      "2023-04-17 12:00:58 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:01:10 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:01:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 145 @ 4631 updates, score 4.327) (writing took 12.287743969005533 seconds)\n",
      "2023-04-17 12:01:10 | INFO | fairseq_cli.train | end of epoch 145 (average epoch stats below)\n",
      "2023-04-17 12:01:10 | INFO | train | epoch 145 | loss 3.284 | nll_loss 1.2 | ppl 2.3 | wps 917.7 | ups 1.35 | wpb 678.5 | bsz 2 | num_updates 4631 | lr 2.84411e-05 | gnorm 3.768 | clip 100 | loss_scale 0.25 | train_wall 11 | gb_free 13.9 | wall 4150\n",
      "2023-04-17 12:01:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:01:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:01:10 | INFO | fairseq.trainer | begin training epoch 146\n",
      "2023-04-17 12:01:10 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:01:21 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:01:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 12:01:22 | INFO | valid | epoch 146 | valid on 'valid' subset | loss 4.318 | nll_loss 2.369 | ppl 5.16 | wps 7121.4 | wpb 290.8 | bsz 1 | num_updates 4663 | best_loss 3.979\n",
      "2023-04-17 12:01:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 146 @ 4663 updates\n",
      "2023-04-17 12:01:22 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:01:34 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:01:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 146 @ 4663 updates, score 4.318) (writing took 12.73158700298518 seconds)\n",
      "2023-04-17 12:01:34 | INFO | fairseq_cli.train | end of epoch 146 (average epoch stats below)\n",
      "2023-04-17 12:01:34 | INFO | train | epoch 146 | loss 3.28 | nll_loss 1.196 | ppl 2.29 | wps 901.1 | ups 1.33 | wpb 678.5 | bsz 2 | num_updates 4663 | lr 2.84291e-05 | gnorm 3.819 | clip 100 | loss_scale 0.25 | train_wall 11 | gb_free 13.9 | wall 4175\n",
      "2023-04-17 12:01:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:01:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:01:34 | INFO | fairseq.trainer | begin training epoch 147\n",
      "2023-04-17 12:01:34 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:01:45 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:01:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:01:46 | INFO | valid | epoch 147 | valid on 'valid' subset | loss 4.336 | nll_loss 2.357 | ppl 5.12 | wps 7204.3 | wpb 290.8 | bsz 1 | num_updates 4695 | best_loss 3.979\n",
      "2023-04-17 12:01:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 147 @ 4695 updates\n",
      "2023-04-17 12:01:46 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:01:58 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:01:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 147 @ 4695 updates, score 4.336) (writing took 12.315721021965146 seconds)\n",
      "2023-04-17 12:01:58 | INFO | fairseq_cli.train | end of epoch 147 (average epoch stats below)\n",
      "2023-04-17 12:01:58 | INFO | train | epoch 147 | loss 3.272 | nll_loss 1.188 | ppl 2.28 | wps 917.3 | ups 1.35 | wpb 678.5 | bsz 2 | num_updates 4695 | lr 2.8417e-05 | gnorm 4.124 | clip 100 | loss_scale 0.25 | train_wall 11 | gb_free 13.9 | wall 4198\n",
      "2023-04-17 12:01:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:01:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:01:58 | INFO | fairseq.trainer | begin training epoch 148\n",
      "2023-04-17 12:01:58 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:02:00 | INFO | train_inner | epoch 148:      5 / 32 loss=3.277, nll_loss=1.192, ppl=2.29, wps=930.3, ups=1.37, wpb=677.3, bsz=2, num_updates=4700, lr=2.84151e-05, gnorm=3.913, clip=100, loss_scale=0.25, train_wall=34, gb_free=13.9, wall=4200\n",
      "2023-04-17 12:02:09 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:02:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:02:09 | INFO | valid | epoch 148 | valid on 'valid' subset | loss 4.344 | nll_loss 2.361 | ppl 5.14 | wps 7213.6 | wpb 290.8 | bsz 1 | num_updates 4727 | best_loss 3.979\n",
      "2023-04-17 12:02:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 148 @ 4727 updates\n",
      "2023-04-17 12:02:09 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:02:22 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:02:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 148 @ 4727 updates, score 4.344) (writing took 12.722237654961646 seconds)\n",
      "2023-04-17 12:02:22 | INFO | fairseq_cli.train | end of epoch 148 (average epoch stats below)\n",
      "2023-04-17 12:02:22 | INFO | train | epoch 148 | loss 3.277 | nll_loss 1.192 | ppl 2.28 | wps 901.6 | ups 1.33 | wpb 678.5 | bsz 2 | num_updates 4727 | lr 2.84049e-05 | gnorm 5.175 | clip 100 | loss_scale 0.25 | train_wall 11 | gb_free 13.9 | wall 4222\n",
      "2023-04-17 12:02:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:02:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:02:22 | INFO | fairseq.trainer | begin training epoch 149\n",
      "2023-04-17 12:02:22 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:02:33 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:02:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:02:33 | INFO | valid | epoch 149 | valid on 'valid' subset | loss 4.351 | nll_loss 2.378 | ppl 5.2 | wps 7282.5 | wpb 290.8 | bsz 1 | num_updates 4759 | best_loss 3.979\n",
      "2023-04-17 12:02:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 149 @ 4759 updates\n",
      "2023-04-17 12:02:33 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:02:46 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:02:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 149 @ 4759 updates, score 4.351) (writing took 12.36023224296514 seconds)\n",
      "2023-04-17 12:02:46 | INFO | fairseq_cli.train | end of epoch 149 (average epoch stats below)\n",
      "2023-04-17 12:02:46 | INFO | train | epoch 149 | loss 3.257 | nll_loss 1.168 | ppl 2.25 | wps 914 | ups 1.35 | wpb 678.5 | bsz 2 | num_updates 4759 | lr 2.83928e-05 | gnorm 3.929 | clip 100 | loss_scale 0.25 | train_wall 11 | gb_free 13.9 | wall 4246\n",
      "2023-04-17 12:02:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:02:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:02:46 | INFO | fairseq.trainer | begin training epoch 150\n",
      "2023-04-17 12:02:46 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:02:57 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:02:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:02:57 | INFO | valid | epoch 150 | valid on 'valid' subset | loss 4.402 | nll_loss 2.434 | ppl 5.41 | wps 7316.4 | wpb 290.8 | bsz 1 | num_updates 4791 | best_loss 3.979\n",
      "2023-04-17 12:02:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 150 @ 4791 updates\n",
      "2023-04-17 12:02:57 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:03:06 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:03:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 150 @ 4791 updates, score 4.402) (writing took 8.730686784023419 seconds)\n",
      "2023-04-17 12:03:06 | INFO | fairseq_cli.train | end of epoch 150 (average epoch stats below)\n",
      "2023-04-17 12:03:06 | INFO | train | epoch 150 | loss 3.249 | nll_loss 1.157 | ppl 2.23 | wps 1079.1 | ups 1.59 | wpb 678.5 | bsz 2 | num_updates 4791 | lr 2.83808e-05 | gnorm 4.215 | clip 100 | loss_scale 0.25 | train_wall 11 | gb_free 13.9 | wall 4266\n",
      "2023-04-17 12:03:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:03:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:03:06 | INFO | fairseq.trainer | begin training epoch 151\n",
      "2023-04-17 12:03:06 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:03:09 | INFO | train_inner | epoch 151:      9 / 32 loss=3.263, nll_loss=1.175, ppl=2.26, wps=977.3, ups=1.44, wpb=678.9, bsz=2, num_updates=4800, lr=2.83774e-05, gnorm=4.446, clip=100, loss_scale=0.25, train_wall=35, gb_free=13.9, wall=4269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 12:03:17 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:03:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:03:17 | INFO | valid | epoch 151 | valid on 'valid' subset | loss 4.391 | nll_loss 2.405 | ppl 5.3 | wps 7340.5 | wpb 290.8 | bsz 1 | num_updates 4823 | best_loss 3.979\n",
      "2023-04-17 12:03:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 151 @ 4823 updates\n",
      "2023-04-17 12:03:17 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:03:29 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:03:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 151 @ 4823 updates, score 4.391) (writing took 12.472116600023583 seconds)\n",
      "2023-04-17 12:03:30 | INFO | fairseq_cli.train | end of epoch 151 (average epoch stats below)\n",
      "2023-04-17 12:03:30 | INFO | train | epoch 151 | loss 3.243 | nll_loss 1.152 | ppl 2.22 | wps 910.2 | ups 1.34 | wpb 678.5 | bsz 2 | num_updates 4823 | lr 2.83687e-05 | gnorm 4.144 | clip 100 | loss_scale 0.25 | train_wall 11 | gb_free 13.9 | wall 4290\n",
      "2023-04-17 12:03:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:03:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:03:30 | INFO | fairseq.trainer | begin training epoch 152\n",
      "2023-04-17 12:03:30 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:03:41 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:03:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:03:41 | INFO | valid | epoch 152 | valid on 'valid' subset | loss 4.427 | nll_loss 2.475 | ppl 5.56 | wps 7320.4 | wpb 290.8 | bsz 1 | num_updates 4855 | best_loss 3.979\n",
      "2023-04-17 12:03:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 152 @ 4855 updates\n",
      "2023-04-17 12:03:41 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:03:54 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:03:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 152 @ 4855 updates, score 4.427) (writing took 12.703785888967104 seconds)\n",
      "2023-04-17 12:03:54 | INFO | fairseq_cli.train | end of epoch 152 (average epoch stats below)\n",
      "2023-04-17 12:03:54 | INFO | train | epoch 152 | loss 3.217 | nll_loss 1.122 | ppl 2.18 | wps 902.2 | ups 1.33 | wpb 678.5 | bsz 2 | num_updates 4855 | lr 2.83566e-05 | gnorm 4.063 | clip 100 | loss_scale 0.25 | train_wall 11 | gb_free 13.9 | wall 4314\n",
      "2023-04-17 12:03:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:03:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:03:54 | INFO | fairseq.trainer | begin training epoch 153\n",
      "2023-04-17 12:03:54 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:04:05 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:04:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:04:05 | INFO | valid | epoch 153 | valid on 'valid' subset | loss 4.474 | nll_loss 2.48 | ppl 5.58 | wps 7267.9 | wpb 290.8 | bsz 1 | num_updates 4887 | best_loss 3.979\n",
      "2023-04-17 12:04:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 153 @ 4887 updates\n",
      "2023-04-17 12:04:05 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:04:17 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:04:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 153 @ 4887 updates, score 4.474) (writing took 12.321935693966225 seconds)\n",
      "2023-04-17 12:04:17 | INFO | fairseq_cli.train | end of epoch 153 (average epoch stats below)\n",
      "2023-04-17 12:04:17 | INFO | train | epoch 153 | loss 3.213 | nll_loss 1.115 | ppl 2.17 | wps 917.6 | ups 1.35 | wpb 678.5 | bsz 2 | num_updates 4887 | lr 2.83445e-05 | gnorm 4.735 | clip 100 | loss_scale 0.25 | train_wall 11 | gb_free 13.9 | wall 4338\n",
      "2023-04-17 12:04:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:04:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:04:17 | INFO | fairseq.trainer | begin training epoch 154\n",
      "2023-04-17 12:04:17 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:04:22 | INFO | train_inner | epoch 154:     13 / 32 loss=3.218, nll_loss=1.121, ppl=2.17, wps=931.7, ups=1.37, wpb=678.9, bsz=2, num_updates=4900, lr=2.83396e-05, gnorm=4.388, clip=100, loss_scale=0.25, train_wall=34, gb_free=13.9, wall=4342\n",
      "2023-04-17 12:04:29 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:04:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:04:29 | INFO | valid | epoch 154 | valid on 'valid' subset | loss 4.431 | nll_loss 2.476 | ppl 5.56 | wps 7317.3 | wpb 290.8 | bsz 1 | num_updates 4919 | best_loss 3.979\n",
      "2023-04-17 12:04:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 154 @ 4919 updates\n",
      "2023-04-17 12:04:29 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:04:41 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:04:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 154 @ 4919 updates, score 4.431) (writing took 12.67386226193048 seconds)\n",
      "2023-04-17 12:04:41 | INFO | fairseq_cli.train | end of epoch 154 (average epoch stats below)\n",
      "2023-04-17 12:04:41 | INFO | train | epoch 154 | loss 3.212 | nll_loss 1.116 | ppl 2.17 | wps 905.6 | ups 1.33 | wpb 678.5 | bsz 2 | num_updates 4919 | lr 2.83325e-05 | gnorm 4.566 | clip 100 | loss_scale 0.25 | train_wall 11 | gb_free 13.9 | wall 4362\n",
      "2023-04-17 12:04:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:04:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:04:41 | INFO | fairseq.trainer | begin training epoch 155\n",
      "2023-04-17 12:04:41 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:04:53 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:04:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:04:53 | INFO | valid | epoch 155 | valid on 'valid' subset | loss 4.418 | nll_loss 2.476 | ppl 5.56 | wps 7160.5 | wpb 290.8 | bsz 1 | num_updates 4951 | best_loss 3.979\n",
      "2023-04-17 12:04:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 155 @ 4951 updates\n",
      "2023-04-17 12:04:53 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:05:05 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:05:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 155 @ 4951 updates, score 4.418) (writing took 12.194067121949047 seconds)\n",
      "2023-04-17 12:05:05 | INFO | fairseq_cli.train | end of epoch 155 (average epoch stats below)\n",
      "2023-04-17 12:05:05 | INFO | train | epoch 155 | loss 3.2 | nll_loss 1.098 | ppl 2.14 | wps 923.8 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 4951 | lr 2.83204e-05 | gnorm 4.215 | clip 100 | loss_scale 0.25 | train_wall 11 | gb_free 13.9 | wall 4385\n",
      "2023-04-17 12:05:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:05:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:05:05 | INFO | fairseq.trainer | begin training epoch 156\n",
      "2023-04-17 12:05:05 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:05:16 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:05:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 12:05:16 | INFO | valid | epoch 156 | valid on 'valid' subset | loss 4.47 | nll_loss 2.497 | ppl 5.65 | wps 7382.5 | wpb 290.8 | bsz 1 | num_updates 4983 | best_loss 3.979\n",
      "2023-04-17 12:05:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 156 @ 4983 updates\n",
      "2023-04-17 12:05:16 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:05:29 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:05:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 156 @ 4983 updates, score 4.47) (writing took 12.609425515984185 seconds)\n",
      "2023-04-17 12:05:29 | INFO | fairseq_cli.train | end of epoch 156 (average epoch stats below)\n",
      "2023-04-17 12:05:29 | INFO | train | epoch 156 | loss 3.188 | nll_loss 1.088 | ppl 2.13 | wps 907.3 | ups 1.34 | wpb 678.5 | bsz 2 | num_updates 4983 | lr 2.83083e-05 | gnorm 4.471 | clip 100 | loss_scale 0.25 | train_wall 11 | gb_free 13.9 | wall 4409\n",
      "2023-04-17 12:05:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:05:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:05:29 | INFO | fairseq.trainer | begin training epoch 157\n",
      "2023-04-17 12:05:29 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:05:35 | INFO | train_inner | epoch 157:     17 / 32 loss=3.188, nll_loss=1.087, ppl=2.12, wps=931.3, ups=1.37, wpb=678.1, bsz=2, num_updates=5000, lr=2.83019e-05, gnorm=4.306, clip=100, loss_scale=0.25, train_wall=34, gb_free=13.9, wall=4415\n",
      "2023-04-17 12:05:35 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:05:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:05:35 | INFO | valid | epoch 157 | valid on 'valid' subset | loss 4.492 | nll_loss 2.53 | ppl 5.78 | wps 7004.6 | wpb 290.8 | bsz 1 | num_updates 5000 | best_loss 3.979\n",
      "2023-04-17 12:05:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 157 @ 5000 updates\n",
      "2023-04-17 12:05:35 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_157_5000.pt\n",
      "2023-04-17 12:05:42 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_157_5000.pt\n",
      "2023-04-17 12:05:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_157_5000.pt (epoch 157 @ 5000 updates, score 4.492) (writing took 14.259142360999249 seconds)\n",
      "2023-04-17 12:05:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:05:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:05:55 | INFO | valid | epoch 157 | valid on 'valid' subset | loss 4.47 | nll_loss 2.504 | ppl 5.67 | wps 7212.5 | wpb 290.8 | bsz 1 | num_updates 5015 | best_loss 3.979\n",
      "2023-04-17 12:05:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 157 @ 5015 updates\n",
      "2023-04-17 12:05:55 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:06:08 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:06:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 157 @ 5015 updates, score 4.47) (writing took 13.731531349942088 seconds)\n",
      "2023-04-17 12:06:09 | INFO | fairseq_cli.train | end of epoch 157 (average epoch stats below)\n",
      "2023-04-17 12:06:09 | INFO | train | epoch 157 | loss 3.168 | nll_loss 1.063 | ppl 2.09 | wps 547.9 | ups 0.81 | wpb 678.5 | bsz 2 | num_updates 5015 | lr 2.82962e-05 | gnorm 4.243 | clip 100 | loss_scale 0.25 | train_wall 11 | gb_free 13.9 | wall 4449\n",
      "2023-04-17 12:06:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:06:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:06:09 | INFO | fairseq.trainer | begin training epoch 158\n",
      "2023-04-17 12:06:09 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:06:20 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:06:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:06:20 | INFO | valid | epoch 158 | valid on 'valid' subset | loss 4.498 | nll_loss 2.551 | ppl 5.86 | wps 7190.2 | wpb 290.8 | bsz 1 | num_updates 5047 | best_loss 3.979\n",
      "2023-04-17 12:06:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 158 @ 5047 updates\n",
      "2023-04-17 12:06:20 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:06:32 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:06:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 158 @ 5047 updates, score 4.498) (writing took 12.665869531105272 seconds)\n",
      "2023-04-17 12:06:32 | INFO | fairseq_cli.train | end of epoch 158 (average epoch stats below)\n",
      "2023-04-17 12:06:32 | INFO | train | epoch 158 | loss 3.175 | nll_loss 1.072 | ppl 2.1 | wps 907.6 | ups 1.34 | wpb 678.5 | bsz 2 | num_updates 5047 | lr 2.82842e-05 | gnorm 4.115 | clip 100 | loss_scale 0.25 | train_wall 11 | gb_free 13.9 | wall 4473\n",
      "2023-04-17 12:06:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:06:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:06:32 | INFO | fairseq.trainer | begin training epoch 159\n",
      "2023-04-17 12:06:32 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:06:44 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:06:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:06:44 | INFO | valid | epoch 159 | valid on 'valid' subset | loss 4.461 | nll_loss 2.495 | ppl 5.64 | wps 7204.8 | wpb 290.8 | bsz 1 | num_updates 5079 | best_loss 3.979\n",
      "2023-04-17 12:06:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 159 @ 5079 updates\n",
      "2023-04-17 12:06:44 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:06:56 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:06:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 159 @ 5079 updates, score 4.461) (writing took 12.905377030954696 seconds)\n",
      "2023-04-17 12:06:57 | INFO | fairseq_cli.train | end of epoch 159 (average epoch stats below)\n",
      "2023-04-17 12:06:57 | INFO | train | epoch 159 | loss 3.159 | nll_loss 1.054 | ppl 2.08 | wps 895.1 | ups 1.32 | wpb 678.5 | bsz 2 | num_updates 5079 | lr 2.82721e-05 | gnorm 4.3 | clip 100 | loss_scale 0.25 | train_wall 11 | gb_free 13.9 | wall 4497\n",
      "2023-04-17 12:06:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:06:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:06:57 | INFO | fairseq.trainer | begin training epoch 160\n",
      "2023-04-17 12:06:57 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:07:05 | INFO | train_inner | epoch 160:     21 / 32 loss=3.164, nll_loss=1.059, ppl=2.08, wps=752.6, ups=1.11, wpb=677, bsz=2, num_updates=5100, lr=2.82642e-05, gnorm=4.243, clip=100, loss_scale=0.25, train_wall=35, gb_free=13.9, wall=4505\n",
      "2023-04-17 12:07:09 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:07:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:07:09 | INFO | valid | epoch 160 | valid on 'valid' subset | loss 4.478 | nll_loss 2.52 | ppl 5.73 | wps 7339.8 | wpb 290.8 | bsz 1 | num_updates 5111 | best_loss 3.979\n",
      "2023-04-17 12:07:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 160 @ 5111 updates\n",
      "2023-04-17 12:07:09 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 12:07:22 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:07:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 160 @ 5111 updates, score 4.478) (writing took 12.752983910031617 seconds)\n",
      "2023-04-17 12:07:22 | INFO | fairseq_cli.train | end of epoch 160 (average epoch stats below)\n",
      "2023-04-17 12:07:22 | INFO | train | epoch 160 | loss 3.15 | nll_loss 1.044 | ppl 2.06 | wps 863.3 | ups 1.27 | wpb 678.5 | bsz 2 | num_updates 5111 | lr 2.826e-05 | gnorm 4.225 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 4522\n",
      "2023-04-17 12:07:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:07:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:07:22 | INFO | fairseq.trainer | begin training epoch 161\n",
      "2023-04-17 12:07:22 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:07:34 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:07:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:07:34 | INFO | valid | epoch 161 | valid on 'valid' subset | loss 4.51 | nll_loss 2.537 | ppl 5.81 | wps 7287.2 | wpb 290.8 | bsz 1 | num_updates 5143 | best_loss 3.979\n",
      "2023-04-17 12:07:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 161 @ 5143 updates\n",
      "2023-04-17 12:07:34 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:07:47 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:07:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 161 @ 5143 updates, score 4.51) (writing took 12.969742464018054 seconds)\n",
      "2023-04-17 12:07:47 | INFO | fairseq_cli.train | end of epoch 161 (average epoch stats below)\n",
      "2023-04-17 12:07:47 | INFO | train | epoch 161 | loss 3.131 | nll_loss 1.02 | ppl 2.03 | wps 848.5 | ups 1.25 | wpb 678.5 | bsz 2 | num_updates 5143 | lr 2.82479e-05 | gnorm 4.211 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 4548\n",
      "2023-04-17 12:07:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:07:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:07:47 | INFO | fairseq.trainer | begin training epoch 162\n",
      "2023-04-17 12:07:47 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:08:00 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:08:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:08:00 | INFO | valid | epoch 162 | valid on 'valid' subset | loss 4.498 | nll_loss 2.531 | ppl 5.78 | wps 7337.4 | wpb 290.8 | bsz 1 | num_updates 5175 | best_loss 3.979\n",
      "2023-04-17 12:08:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 162 @ 5175 updates\n",
      "2023-04-17 12:08:00 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:08:12 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:08:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 162 @ 5175 updates, score 4.498) (writing took 12.72574283205904 seconds)\n",
      "2023-04-17 12:08:13 | INFO | fairseq_cli.train | end of epoch 162 (average epoch stats below)\n",
      "2023-04-17 12:08:13 | INFO | train | epoch 162 | loss 3.139 | nll_loss 1.029 | ppl 2.04 | wps 866.5 | ups 1.28 | wpb 678.5 | bsz 2 | num_updates 5175 | lr 2.82358e-05 | gnorm 4.942 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 4573\n",
      "2023-04-17 12:08:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:08:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:08:13 | INFO | fairseq.trainer | begin training epoch 163\n",
      "2023-04-17 12:08:13 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:08:23 | INFO | train_inner | epoch 163:     25 / 32 loss=3.136, nll_loss=1.026, ppl=2.04, wps=867.7, ups=1.28, wpb=676, bsz=2, num_updates=5200, lr=2.82264e-05, gnorm=4.484, clip=100, loss_scale=0.25, train_wall=38, gb_free=13.9, wall=4583\n",
      "2023-04-17 12:08:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:08:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:08:26 | INFO | valid | epoch 163 | valid on 'valid' subset | loss 4.536 | nll_loss 2.583 | ppl 5.99 | wps 6421.6 | wpb 290.8 | bsz 1 | num_updates 5207 | best_loss 3.979\n",
      "2023-04-17 12:08:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 163 @ 5207 updates\n",
      "2023-04-17 12:08:26 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:08:41 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:08:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 163 @ 5207 updates, score 4.536) (writing took 15.310443212045357 seconds)\n",
      "2023-04-17 12:08:41 | INFO | fairseq_cli.train | end of epoch 163 (average epoch stats below)\n",
      "2023-04-17 12:08:41 | INFO | train | epoch 163 | loss 3.123 | nll_loss 1.011 | ppl 2.02 | wps 762 | ups 1.12 | wpb 678.5 | bsz 2 | num_updates 5207 | lr 2.82238e-05 | gnorm 4.441 | clip 100 | loss_scale 0.25 | train_wall 13 | gb_free 13.9 | wall 4601\n",
      "2023-04-17 12:08:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:08:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:08:41 | INFO | fairseq.trainer | begin training epoch 164\n",
      "2023-04-17 12:08:41 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:08:54 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:08:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:08:54 | INFO | valid | epoch 164 | valid on 'valid' subset | loss 4.529 | nll_loss 2.582 | ppl 5.99 | wps 6962.8 | wpb 290.8 | bsz 1 | num_updates 5239 | best_loss 3.979\n",
      "2023-04-17 12:08:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 164 @ 5239 updates\n",
      "2023-04-17 12:08:54 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:09:07 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:09:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 164 @ 5239 updates, score 4.529) (writing took 13.560910544008948 seconds)\n",
      "2023-04-17 12:09:08 | INFO | fairseq_cli.train | end of epoch 164 (average epoch stats below)\n",
      "2023-04-17 12:09:08 | INFO | train | epoch 164 | loss 3.116 | nll_loss 1.002 | ppl 2 | wps 813.4 | ups 1.2 | wpb 678.5 | bsz 2 | num_updates 5239 | lr 2.82117e-05 | gnorm 4.351 | clip 100 | loss_scale 0.25 | train_wall 13 | gb_free 13.9 | wall 4628\n",
      "2023-04-17 12:09:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:09:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:09:08 | INFO | fairseq.trainer | begin training epoch 165\n",
      "2023-04-17 12:09:08 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:09:20 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:09:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:09:20 | INFO | valid | epoch 165 | valid on 'valid' subset | loss 4.569 | nll_loss 2.597 | ppl 6.05 | wps 6851.4 | wpb 290.8 | bsz 1 | num_updates 5271 | best_loss 3.979\n",
      "2023-04-17 12:09:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 165 @ 5271 updates\n",
      "2023-04-17 12:09:20 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:09:34 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 12:09:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 165 @ 5271 updates, score 4.569) (writing took 13.572076713084243 seconds)\n",
      "2023-04-17 12:09:34 | INFO | fairseq_cli.train | end of epoch 165 (average epoch stats below)\n",
      "2023-04-17 12:09:34 | INFO | train | epoch 165 | loss 3.1 | nll_loss 0.985 | ppl 1.98 | wps 833.7 | ups 1.23 | wpb 678.5 | bsz 2 | num_updates 5271 | lr 2.81996e-05 | gnorm 4.487 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 4654\n",
      "2023-04-17 12:09:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:09:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:09:34 | INFO | fairseq.trainer | begin training epoch 166\n",
      "2023-04-17 12:09:34 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:09:45 | INFO | train_inner | epoch 166:     29 / 32 loss=3.103, nll_loss=0.988, ppl=1.98, wps=829.8, ups=1.22, wpb=680.2, bsz=2, num_updates=5300, lr=2.81887e-05, gnorm=4.697, clip=100, loss_scale=0.25, train_wall=38, gb_free=13.9, wall=4665\n",
      "2023-04-17 12:09:46 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:09:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:09:46 | INFO | valid | epoch 166 | valid on 'valid' subset | loss 4.582 | nll_loss 2.625 | ppl 6.17 | wps 6780.9 | wpb 290.8 | bsz 1 | num_updates 5303 | best_loss 3.979\n",
      "2023-04-17 12:09:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 166 @ 5303 updates\n",
      "2023-04-17 12:09:46 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:09:58 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:09:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 166 @ 5303 updates, score 4.582) (writing took 12.748537303996272 seconds)\n",
      "2023-04-17 12:09:59 | INFO | fairseq_cli.train | end of epoch 166 (average epoch stats below)\n",
      "2023-04-17 12:09:59 | INFO | train | epoch 166 | loss 3.094 | nll_loss 0.976 | ppl 1.97 | wps 870.9 | ups 1.28 | wpb 678.5 | bsz 2 | num_updates 5303 | lr 2.81875e-05 | gnorm 5.256 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 4679\n",
      "2023-04-17 12:09:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:09:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:09:59 | INFO | fairseq.trainer | begin training epoch 167\n",
      "2023-04-17 12:09:59 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:10:11 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:10:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:10:12 | INFO | valid | epoch 167 | valid on 'valid' subset | loss 4.588 | nll_loss 2.645 | ppl 6.25 | wps 6948.6 | wpb 290.8 | bsz 1 | num_updates 5335 | best_loss 3.979\n",
      "2023-04-17 12:10:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 167 @ 5335 updates\n",
      "2023-04-17 12:10:12 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:10:26 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:10:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 167 @ 5335 updates, score 4.588) (writing took 14.1704440000467 seconds)\n",
      "2023-04-17 12:10:26 | INFO | fairseq_cli.train | end of epoch 167 (average epoch stats below)\n",
      "2023-04-17 12:10:26 | INFO | train | epoch 167 | loss 3.089 | nll_loss 0.969 | ppl 1.96 | wps 800.9 | ups 1.18 | wpb 678.5 | bsz 2 | num_updates 5335 | lr 2.81755e-05 | gnorm 4.621 | clip 100 | loss_scale 0.25 | train_wall 13 | gb_free 13.9 | wall 4706\n",
      "2023-04-17 12:10:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:10:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:10:26 | INFO | fairseq.trainer | begin training epoch 168\n",
      "2023-04-17 12:10:26 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:10:38 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:10:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:10:38 | INFO | valid | epoch 168 | valid on 'valid' subset | loss 4.61 | nll_loss 2.652 | ppl 6.29 | wps 6944.4 | wpb 290.8 | bsz 1 | num_updates 5367 | best_loss 3.979\n",
      "2023-04-17 12:10:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 168 @ 5367 updates\n",
      "2023-04-17 12:10:38 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:10:52 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:10:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 168 @ 5367 updates, score 4.61) (writing took 13.330564128002152 seconds)\n",
      "2023-04-17 12:10:52 | INFO | fairseq_cli.train | end of epoch 168 (average epoch stats below)\n",
      "2023-04-17 12:10:52 | INFO | train | epoch 168 | loss 3.09 | nll_loss 0.973 | ppl 1.96 | wps 834 | ups 1.23 | wpb 678.5 | bsz 2 | num_updates 5367 | lr 2.81634e-05 | gnorm 4.734 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 4732\n",
      "2023-04-17 12:10:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:10:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:10:52 | INFO | fairseq.trainer | begin training epoch 169\n",
      "2023-04-17 12:10:52 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:11:04 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:11:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:11:05 | INFO | valid | epoch 169 | valid on 'valid' subset | loss 4.617 | nll_loss 2.659 | ppl 6.32 | wps 6615.6 | wpb 290.8 | bsz 1 | num_updates 5399 | best_loss 3.979\n",
      "2023-04-17 12:11:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 169 @ 5399 updates\n",
      "2023-04-17 12:11:05 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:11:18 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:11:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 169 @ 5399 updates, score 4.617) (writing took 13.383139899000525 seconds)\n",
      "2023-04-17 12:11:18 | INFO | fairseq_cli.train | end of epoch 169 (average epoch stats below)\n",
      "2023-04-17 12:11:18 | INFO | train | epoch 169 | loss 3.076 | nll_loss 0.956 | ppl 1.94 | wps 826.7 | ups 1.22 | wpb 678.5 | bsz 2 | num_updates 5399 | lr 2.81513e-05 | gnorm 4.483 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 4758\n",
      "2023-04-17 12:11:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:11:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:11:18 | INFO | fairseq.trainer | begin training epoch 170\n",
      "2023-04-17 12:11:18 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:11:19 | INFO | train_inner | epoch 170:      1 / 32 loss=3.088, nll_loss=0.97, ppl=1.96, wps=722.9, ups=1.07, wpb=678, bsz=2, num_updates=5400, lr=2.81509e-05, gnorm=4.615, clip=100, loss_scale=0.25, train_wall=39, gb_free=13.9, wall=4759\n",
      "2023-04-17 12:11:31 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:11:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:11:31 | INFO | valid | epoch 170 | valid on 'valid' subset | loss 4.633 | nll_loss 2.68 | ppl 6.41 | wps 6342.9 | wpb 290.8 | bsz 1 | num_updates 5431 | best_loss 3.979\n",
      "2023-04-17 12:11:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 170 @ 5431 updates\n",
      "2023-04-17 12:11:31 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 12:11:44 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:11:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 170 @ 5431 updates, score 4.633) (writing took 13.523669900954701 seconds)\n",
      "2023-04-17 12:11:45 | INFO | fairseq_cli.train | end of epoch 170 (average epoch stats below)\n",
      "2023-04-17 12:11:45 | INFO | train | epoch 170 | loss 3.062 | nll_loss 0.941 | ppl 1.92 | wps 818.6 | ups 1.21 | wpb 678.5 | bsz 2 | num_updates 5431 | lr 2.81392e-05 | gnorm 4.582 | clip 100 | loss_scale 0.25 | train_wall 13 | gb_free 13.9 | wall 4785\n",
      "2023-04-17 12:11:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:11:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:11:45 | INFO | fairseq.trainer | begin training epoch 171\n",
      "2023-04-17 12:11:45 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:11:56 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:11:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:11:57 | INFO | valid | epoch 171 | valid on 'valid' subset | loss 4.627 | nll_loss 2.692 | ppl 6.46 | wps 6494 | wpb 290.8 | bsz 1 | num_updates 5463 | best_loss 3.979\n",
      "2023-04-17 12:11:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 171 @ 5463 updates\n",
      "2023-04-17 12:11:57 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:12:11 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:12:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 171 @ 5463 updates, score 4.627) (writing took 14.233704383019358 seconds)\n",
      "2023-04-17 12:12:11 | INFO | fairseq_cli.train | end of epoch 171 (average epoch stats below)\n",
      "2023-04-17 12:12:11 | INFO | train | epoch 171 | loss 3.063 | nll_loss 0.94 | ppl 1.92 | wps 826.9 | ups 1.22 | wpb 678.5 | bsz 2 | num_updates 5463 | lr 2.81272e-05 | gnorm 4.842 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 4811\n",
      "2023-04-17 12:12:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:12:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:12:11 | INFO | fairseq.trainer | begin training epoch 172\n",
      "2023-04-17 12:12:11 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:12:23 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:12:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:12:23 | INFO | valid | epoch 172 | valid on 'valid' subset | loss 4.614 | nll_loss 2.679 | ppl 6.4 | wps 6518.8 | wpb 290.8 | bsz 1 | num_updates 5495 | best_loss 3.979\n",
      "2023-04-17 12:12:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 172 @ 5495 updates\n",
      "2023-04-17 12:12:23 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:12:36 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:12:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 172 @ 5495 updates, score 4.614) (writing took 13.379660672973841 seconds)\n",
      "2023-04-17 12:12:36 | INFO | fairseq_cli.train | end of epoch 172 (average epoch stats below)\n",
      "2023-04-17 12:12:36 | INFO | train | epoch 172 | loss 3.056 | nll_loss 0.931 | ppl 1.91 | wps 853.5 | ups 1.26 | wpb 678.5 | bsz 2 | num_updates 5495 | lr 2.81151e-05 | gnorm 4.743 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 4837\n",
      "2023-04-17 12:12:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:12:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:12:36 | INFO | fairseq.trainer | begin training epoch 173\n",
      "2023-04-17 12:12:36 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:12:38 | INFO | train_inner | epoch 173:      5 / 32 loss=3.059, nll_loss=0.936, ppl=1.91, wps=860.6, ups=1.25, wpb=686.1, bsz=2, num_updates=5500, lr=2.81132e-05, gnorm=4.717, clip=100, loss_scale=0.25, train_wall=37, gb_free=13.9, wall=4838\n",
      "2023-04-17 12:12:48 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:12:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:12:48 | INFO | valid | epoch 173 | valid on 'valid' subset | loss 4.652 | nll_loss 2.697 | ppl 6.49 | wps 7153.6 | wpb 290.8 | bsz 1 | num_updates 5527 | best_loss 3.979\n",
      "2023-04-17 12:12:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 173 @ 5527 updates\n",
      "2023-04-17 12:12:48 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:13:03 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:13:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 173 @ 5527 updates, score 4.652) (writing took 14.457860431983136 seconds)\n",
      "2023-04-17 12:13:03 | INFO | fairseq_cli.train | end of epoch 173 (average epoch stats below)\n",
      "2023-04-17 12:13:03 | INFO | train | epoch 173 | loss 3.04 | nll_loss 0.915 | ppl 1.89 | wps 817.2 | ups 1.2 | wpb 678.5 | bsz 2 | num_updates 5527 | lr 2.8103e-05 | gnorm 4.742 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 4863\n",
      "2023-04-17 12:13:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:13:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:13:03 | INFO | fairseq.trainer | begin training epoch 174\n",
      "2023-04-17 12:13:03 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:13:15 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:13:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:13:15 | INFO | valid | epoch 174 | valid on 'valid' subset | loss 4.646 | nll_loss 2.696 | ppl 6.48 | wps 6683.5 | wpb 290.8 | bsz 1 | num_updates 5559 | best_loss 3.979\n",
      "2023-04-17 12:13:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 174 @ 5559 updates\n",
      "2023-04-17 12:13:15 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:13:28 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:13:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 174 @ 5559 updates, score 4.646) (writing took 12.91132577508688 seconds)\n",
      "2023-04-17 12:13:28 | INFO | fairseq_cli.train | end of epoch 174 (average epoch stats below)\n",
      "2023-04-17 12:13:28 | INFO | train | epoch 174 | loss 3.032 | nll_loss 0.905 | ppl 1.87 | wps 869.5 | ups 1.28 | wpb 678.5 | bsz 2 | num_updates 5559 | lr 2.80909e-05 | gnorm 4.603 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 4888\n",
      "2023-04-17 12:13:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:13:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:13:28 | INFO | fairseq.trainer | begin training epoch 175\n",
      "2023-04-17 12:13:28 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:13:40 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:13:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:13:40 | INFO | valid | epoch 175 | valid on 'valid' subset | loss 4.649 | nll_loss 2.709 | ppl 6.54 | wps 6717 | wpb 290.8 | bsz 1 | num_updates 5591 | best_loss 3.979\n",
      "2023-04-17 12:13:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 175 @ 5591 updates\n",
      "2023-04-17 12:13:40 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:13:53 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 12:13:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 175 @ 5591 updates, score 4.649) (writing took 13.34110277891159 seconds)\n",
      "2023-04-17 12:13:53 | INFO | fairseq_cli.train | end of epoch 175 (average epoch stats below)\n",
      "2023-04-17 12:13:53 | INFO | train | epoch 175 | loss 3.022 | nll_loss 0.894 | ppl 1.86 | wps 860.3 | ups 1.27 | wpb 678.5 | bsz 2 | num_updates 5591 | lr 2.80789e-05 | gnorm 4.634 | clip 100 | loss_scale 0.25 | train_wall 11 | gb_free 13.9 | wall 4913\n",
      "2023-04-17 12:13:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:13:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:13:53 | INFO | fairseq.trainer | begin training epoch 176\n",
      "2023-04-17 12:13:53 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:13:57 | INFO | train_inner | epoch 176:      9 / 32 loss=3.026, nll_loss=0.898, ppl=1.86, wps=855.8, ups=1.27, wpb=671.6, bsz=2, num_updates=5600, lr=2.80755e-05, gnorm=4.666, clip=100, loss_scale=0.25, train_wall=37, gb_free=13.9, wall=4917\n",
      "2023-04-17 12:14:05 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:14:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:14:05 | INFO | valid | epoch 176 | valid on 'valid' subset | loss 4.706 | nll_loss 2.791 | ppl 6.92 | wps 6573.4 | wpb 290.8 | bsz 1 | num_updates 5623 | best_loss 3.979\n",
      "2023-04-17 12:14:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 176 @ 5623 updates\n",
      "2023-04-17 12:14:05 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:14:18 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:14:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 176 @ 5623 updates, score 4.706) (writing took 13.125254406011663 seconds)\n",
      "2023-04-17 12:14:19 | INFO | fairseq_cli.train | end of epoch 176 (average epoch stats below)\n",
      "2023-04-17 12:14:19 | INFO | train | epoch 176 | loss 3.017 | nll_loss 0.887 | ppl 1.85 | wps 854.1 | ups 1.26 | wpb 678.5 | bsz 2 | num_updates 5623 | lr 2.80668e-05 | gnorm 4.841 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 4939\n",
      "2023-04-17 12:14:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:14:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:14:19 | INFO | fairseq.trainer | begin training epoch 177\n",
      "2023-04-17 12:14:19 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:14:31 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:14:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:14:31 | INFO | valid | epoch 177 | valid on 'valid' subset | loss 4.725 | nll_loss 2.762 | ppl 6.78 | wps 6645.1 | wpb 290.8 | bsz 1 | num_updates 5655 | best_loss 3.979\n",
      "2023-04-17 12:14:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 177 @ 5655 updates\n",
      "2023-04-17 12:14:31 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:14:44 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:14:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 177 @ 5655 updates, score 4.725) (writing took 13.492473497986794 seconds)\n",
      "2023-04-17 12:14:44 | INFO | fairseq_cli.train | end of epoch 177 (average epoch stats below)\n",
      "2023-04-17 12:14:44 | INFO | train | epoch 177 | loss 3 | nll_loss 0.869 | ppl 1.83 | wps 842.1 | ups 1.24 | wpb 678.5 | bsz 2 | num_updates 5655 | lr 2.80547e-05 | gnorm 4.619 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 4965\n",
      "2023-04-17 12:14:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:14:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:14:44 | INFO | fairseq.trainer | begin training epoch 178\n",
      "2023-04-17 12:14:44 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:14:56 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:14:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:14:57 | INFO | valid | epoch 178 | valid on 'valid' subset | loss 4.746 | nll_loss 2.817 | ppl 7.05 | wps 5889.4 | wpb 290.8 | bsz 1 | num_updates 5687 | best_loss 3.979\n",
      "2023-04-17 12:14:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 178 @ 5687 updates\n",
      "2023-04-17 12:14:57 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:15:10 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:15:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 178 @ 5687 updates, score 4.746) (writing took 13.430287340073846 seconds)\n",
      "2023-04-17 12:15:10 | INFO | fairseq_cli.train | end of epoch 178 (average epoch stats below)\n",
      "2023-04-17 12:15:10 | INFO | train | epoch 178 | loss 2.995 | nll_loss 0.864 | ppl 1.82 | wps 846.3 | ups 1.25 | wpb 678.5 | bsz 2 | num_updates 5687 | lr 2.80426e-05 | gnorm 4.819 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 4990\n",
      "2023-04-17 12:15:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:15:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:15:10 | INFO | fairseq.trainer | begin training epoch 179\n",
      "2023-04-17 12:15:10 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:15:15 | INFO | train_inner | epoch 179:     13 / 32 loss=3.005, nll_loss=0.874, ppl=1.83, wps=865.5, ups=1.28, wpb=676.7, bsz=2, num_updates=5700, lr=2.80377e-05, gnorm=4.732, clip=100, loss_scale=0.25, train_wall=37, gb_free=13.9, wall=4995\n",
      "2023-04-17 12:15:22 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:15:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:15:22 | INFO | valid | epoch 179 | valid on 'valid' subset | loss 4.713 | nll_loss 2.758 | ppl 6.76 | wps 6338.8 | wpb 290.8 | bsz 1 | num_updates 5719 | best_loss 3.979\n",
      "2023-04-17 12:15:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 179 @ 5719 updates\n",
      "2023-04-17 12:15:22 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:15:36 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:15:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 179 @ 5719 updates, score 4.713) (writing took 14.014203550992534 seconds)\n",
      "2023-04-17 12:15:36 | INFO | fairseq_cli.train | end of epoch 179 (average epoch stats below)\n",
      "2023-04-17 12:15:36 | INFO | train | epoch 179 | loss 2.997 | nll_loss 0.864 | ppl 1.82 | wps 832.3 | ups 1.23 | wpb 678.5 | bsz 2 | num_updates 5719 | lr 2.80306e-05 | gnorm 4.969 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 5016\n",
      "2023-04-17 12:15:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:15:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:15:36 | INFO | fairseq.trainer | begin training epoch 180\n",
      "2023-04-17 12:15:36 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:15:49 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:15:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:15:49 | INFO | valid | epoch 180 | valid on 'valid' subset | loss 4.747 | nll_loss 2.804 | ppl 6.98 | wps 5480.3 | wpb 290.8 | bsz 1 | num_updates 5751 | best_loss 3.979\n",
      "2023-04-17 12:15:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 180 @ 5751 updates\n",
      "2023-04-17 12:15:49 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 12:16:03 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:16:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 180 @ 5751 updates, score 4.747) (writing took 14.374626107048243 seconds)\n",
      "2023-04-17 12:16:03 | INFO | fairseq_cli.train | end of epoch 180 (average epoch stats below)\n",
      "2023-04-17 12:16:03 | INFO | train | epoch 180 | loss 2.984 | nll_loss 0.85 | ppl 1.8 | wps 796.6 | ups 1.17 | wpb 678.5 | bsz 2 | num_updates 5751 | lr 2.80185e-05 | gnorm 4.598 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 5044\n",
      "2023-04-17 12:16:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:16:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:16:03 | INFO | fairseq.trainer | begin training epoch 181\n",
      "2023-04-17 12:16:03 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:16:16 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:16:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:16:16 | INFO | valid | epoch 181 | valid on 'valid' subset | loss 4.784 | nll_loss 2.843 | ppl 7.18 | wps 6347.6 | wpb 290.8 | bsz 1 | num_updates 5783 | best_loss 3.979\n",
      "2023-04-17 12:16:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 181 @ 5783 updates\n",
      "2023-04-17 12:16:16 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:16:30 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:16:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 181 @ 5783 updates, score 4.784) (writing took 13.833263951004483 seconds)\n",
      "2023-04-17 12:16:30 | INFO | fairseq_cli.train | end of epoch 181 (average epoch stats below)\n",
      "2023-04-17 12:16:30 | INFO | train | epoch 181 | loss 2.968 | nll_loss 0.831 | ppl 1.78 | wps 811.3 | ups 1.2 | wpb 678.5 | bsz 2 | num_updates 5783 | lr 2.80064e-05 | gnorm 4.634 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 5070\n",
      "2023-04-17 12:16:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:16:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:16:30 | INFO | fairseq.trainer | begin training epoch 182\n",
      "2023-04-17 12:16:30 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:16:37 | INFO | train_inner | epoch 182:     17 / 32 loss=2.972, nll_loss=0.835, ppl=1.78, wps=832.7, ups=1.23, wpb=679.6, bsz=2, num_updates=5800, lr=2.8e-05, gnorm=4.705, clip=100, loss_scale=0.25, train_wall=38, gb_free=13.9, wall=5077\n",
      "2023-04-17 12:16:42 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:16:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:16:42 | INFO | valid | epoch 182 | valid on 'valid' subset | loss 4.711 | nll_loss 2.783 | ppl 6.88 | wps 7032.7 | wpb 290.8 | bsz 1 | num_updates 5815 | best_loss 3.979\n",
      "2023-04-17 12:16:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 182 @ 5815 updates\n",
      "2023-04-17 12:16:42 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:16:55 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:16:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 182 @ 5815 updates, score 4.711) (writing took 12.94123596989084 seconds)\n",
      "2023-04-17 12:16:55 | INFO | fairseq_cli.train | end of epoch 182 (average epoch stats below)\n",
      "2023-04-17 12:16:55 | INFO | train | epoch 182 | loss 2.967 | nll_loss 0.829 | ppl 1.78 | wps 864.3 | ups 1.27 | wpb 678.5 | bsz 2 | num_updates 5815 | lr 2.79943e-05 | gnorm 5.295 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 5095\n",
      "2023-04-17 12:16:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:16:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:16:55 | INFO | fairseq.trainer | begin training epoch 183\n",
      "2023-04-17 12:16:55 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:17:07 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:17:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:17:07 | INFO | valid | epoch 183 | valid on 'valid' subset | loss 4.711 | nll_loss 2.772 | ppl 6.83 | wps 6835.7 | wpb 290.8 | bsz 1 | num_updates 5847 | best_loss 3.979\n",
      "2023-04-17 12:17:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 183 @ 5847 updates\n",
      "2023-04-17 12:17:07 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:17:21 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:17:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 183 @ 5847 updates, score 4.711) (writing took 13.988527065026574 seconds)\n",
      "2023-04-17 12:17:21 | INFO | fairseq_cli.train | end of epoch 183 (average epoch stats below)\n",
      "2023-04-17 12:17:21 | INFO | train | epoch 183 | loss 2.969 | nll_loss 0.833 | ppl 1.78 | wps 831.7 | ups 1.23 | wpb 678.5 | bsz 2 | num_updates 5847 | lr 2.79823e-05 | gnorm 5.173 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 5122\n",
      "2023-04-17 12:17:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:17:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:17:21 | INFO | fairseq.trainer | begin training epoch 184\n",
      "2023-04-17 12:17:21 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:17:33 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:17:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:17:33 | INFO | valid | epoch 184 | valid on 'valid' subset | loss 4.818 | nll_loss 2.881 | ppl 7.36 | wps 6738.8 | wpb 290.8 | bsz 1 | num_updates 5879 | best_loss 3.979\n",
      "2023-04-17 12:17:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 184 @ 5879 updates\n",
      "2023-04-17 12:17:33 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:17:46 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:17:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 184 @ 5879 updates, score 4.818) (writing took 13.133378976024687 seconds)\n",
      "2023-04-17 12:17:46 | INFO | fairseq_cli.train | end of epoch 184 (average epoch stats below)\n",
      "2023-04-17 12:17:46 | INFO | train | epoch 184 | loss 2.954 | nll_loss 0.815 | ppl 1.76 | wps 864.7 | ups 1.27 | wpb 678.5 | bsz 2 | num_updates 5879 | lr 2.79702e-05 | gnorm 4.95 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 5147\n",
      "2023-04-17 12:17:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:17:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:17:46 | INFO | fairseq.trainer | begin training epoch 185\n",
      "2023-04-17 12:17:46 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:17:54 | INFO | train_inner | epoch 185:     21 / 32 loss=2.965, nll_loss=0.828, ppl=1.78, wps=870.8, ups=1.29, wpb=677.5, bsz=2, num_updates=5900, lr=2.79623e-05, gnorm=5.189, clip=100, loss_scale=0.25, train_wall=36, gb_free=13.9, wall=5155\n",
      "2023-04-17 12:17:58 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:17:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:17:58 | INFO | valid | epoch 185 | valid on 'valid' subset | loss 4.72 | nll_loss 2.792 | ppl 6.92 | wps 6872.6 | wpb 290.8 | bsz 1 | num_updates 5911 | best_loss 3.979\n",
      "2023-04-17 12:17:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 185 @ 5911 updates\n",
      "2023-04-17 12:17:59 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 12:18:13 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:18:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 185 @ 5911 updates, score 4.72) (writing took 14.677868653088808 seconds)\n",
      "2023-04-17 12:18:13 | INFO | fairseq_cli.train | end of epoch 185 (average epoch stats below)\n",
      "2023-04-17 12:18:13 | INFO | train | epoch 185 | loss 2.942 | nll_loss 0.801 | ppl 1.74 | wps 811 | ups 1.2 | wpb 678.5 | bsz 2 | num_updates 5911 | lr 2.79581e-05 | gnorm 4.746 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 5173\n",
      "2023-04-17 12:18:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:18:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:18:13 | INFO | fairseq.trainer | begin training epoch 186\n",
      "2023-04-17 12:18:13 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:18:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:18:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:18:25 | INFO | valid | epoch 186 | valid on 'valid' subset | loss 4.765 | nll_loss 2.842 | ppl 7.17 | wps 6818 | wpb 290.8 | bsz 1 | num_updates 5943 | best_loss 3.979\n",
      "2023-04-17 12:18:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 186 @ 5943 updates\n",
      "2023-04-17 12:18:25 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:18:39 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:18:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 186 @ 5943 updates, score 4.765) (writing took 13.362358535989188 seconds)\n",
      "2023-04-17 12:18:39 | INFO | fairseq_cli.train | end of epoch 186 (average epoch stats below)\n",
      "2023-04-17 12:18:39 | INFO | train | epoch 186 | loss 2.941 | nll_loss 0.801 | ppl 1.74 | wps 848 | ups 1.25 | wpb 678.5 | bsz 2 | num_updates 5943 | lr 2.7946e-05 | gnorm 4.655 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 5199\n",
      "2023-04-17 12:18:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:18:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:18:39 | INFO | fairseq.trainer | begin training epoch 187\n",
      "2023-04-17 12:18:39 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:18:51 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:18:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:18:51 | INFO | valid | epoch 187 | valid on 'valid' subset | loss 4.787 | nll_loss 2.864 | ppl 7.28 | wps 6471.5 | wpb 290.8 | bsz 1 | num_updates 5975 | best_loss 3.979\n",
      "2023-04-17 12:18:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 187 @ 5975 updates\n",
      "2023-04-17 12:18:51 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:19:05 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:19:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 187 @ 5975 updates, score 4.787) (writing took 13.970349904964678 seconds)\n",
      "2023-04-17 12:19:05 | INFO | fairseq_cli.train | end of epoch 187 (average epoch stats below)\n",
      "2023-04-17 12:19:05 | INFO | train | epoch 187 | loss 2.945 | nll_loss 0.805 | ppl 1.75 | wps 836.6 | ups 1.23 | wpb 678.5 | bsz 2 | num_updates 5975 | lr 2.7934e-05 | gnorm 5.09 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 5225\n",
      "2023-04-17 12:19:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:19:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:19:05 | INFO | fairseq.trainer | begin training epoch 188\n",
      "2023-04-17 12:19:05 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:19:14 | INFO | train_inner | epoch 188:     25 / 32 loss=2.94, nll_loss=0.8, ppl=1.74, wps=859.4, ups=1.25, wpb=686.6, bsz=2, num_updates=6000, lr=2.79245e-05, gnorm=4.759, clip=100, loss_scale=0.25, train_wall=37, gb_free=13.9, wall=5234\n",
      "2023-04-17 12:19:17 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:19:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:19:17 | INFO | valid | epoch 188 | valid on 'valid' subset | loss 4.805 | nll_loss 2.874 | ppl 7.33 | wps 6738.9 | wpb 290.8 | bsz 1 | num_updates 6007 | best_loss 3.979\n",
      "2023-04-17 12:19:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 188 @ 6007 updates\n",
      "2023-04-17 12:19:17 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:19:30 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:19:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 188 @ 6007 updates, score 4.805) (writing took 13.258518157992512 seconds)\n",
      "2023-04-17 12:19:30 | INFO | fairseq_cli.train | end of epoch 188 (average epoch stats below)\n",
      "2023-04-17 12:19:30 | INFO | train | epoch 188 | loss 2.924 | nll_loss 0.782 | ppl 1.72 | wps 850.6 | ups 1.25 | wpb 678.5 | bsz 2 | num_updates 6007 | lr 2.79219e-05 | gnorm 4.579 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 5251\n",
      "2023-04-17 12:19:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:19:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:19:30 | INFO | fairseq.trainer | begin training epoch 189\n",
      "2023-04-17 12:19:30 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:19:43 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:19:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:19:43 | INFO | valid | epoch 189 | valid on 'valid' subset | loss 4.801 | nll_loss 2.878 | ppl 7.35 | wps 6330.3 | wpb 290.8 | bsz 1 | num_updates 6039 | best_loss 3.979\n",
      "2023-04-17 12:19:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 189 @ 6039 updates\n",
      "2023-04-17 12:19:43 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:19:57 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:19:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 189 @ 6039 updates, score 4.801) (writing took 14.626663978910074 seconds)\n",
      "2023-04-17 12:19:58 | INFO | fairseq_cli.train | end of epoch 189 (average epoch stats below)\n",
      "2023-04-17 12:19:58 | INFO | train | epoch 189 | loss 2.928 | nll_loss 0.785 | ppl 1.72 | wps 793.4 | ups 1.17 | wpb 678.5 | bsz 2 | num_updates 6039 | lr 2.79098e-05 | gnorm 5.135 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 5278\n",
      "2023-04-17 12:19:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:19:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:19:58 | INFO | fairseq.trainer | begin training epoch 190\n",
      "2023-04-17 12:19:58 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:20:11 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:20:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:20:11 | INFO | valid | epoch 190 | valid on 'valid' subset | loss 4.827 | nll_loss 2.891 | ppl 7.42 | wps 4813.9 | wpb 290.8 | bsz 1 | num_updates 6071 | best_loss 3.979\n",
      "2023-04-17 12:20:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 190 @ 6071 updates\n",
      "2023-04-17 12:20:11 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:20:26 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 12:20:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 190 @ 6071 updates, score 4.827) (writing took 14.966168987913989 seconds)\n",
      "2023-04-17 12:20:26 | INFO | fairseq_cli.train | end of epoch 190 (average epoch stats below)\n",
      "2023-04-17 12:20:26 | INFO | train | epoch 190 | loss 2.912 | nll_loss 0.769 | ppl 1.7 | wps 762.4 | ups 1.12 | wpb 678.5 | bsz 2 | num_updates 6071 | lr 2.78977e-05 | gnorm 4.694 | clip 100 | loss_scale 0.25 | train_wall 13 | gb_free 13.9 | wall 5306\n",
      "2023-04-17 12:20:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:20:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:20:26 | INFO | fairseq.trainer | begin training epoch 191\n",
      "2023-04-17 12:20:26 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:20:38 | INFO | train_inner | epoch 191:     29 / 32 loss=2.917, nll_loss=0.774, ppl=1.71, wps=798.1, ups=1.19, wpb=672, bsz=2, num_updates=6100, lr=2.78868e-05, gnorm=4.85, clip=100, loss_scale=0.25, train_wall=40, gb_free=13.7, wall=5319\n",
      "2023-04-17 12:20:40 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:20:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:20:40 | INFO | valid | epoch 191 | valid on 'valid' subset | loss 4.868 | nll_loss 2.951 | ppl 7.73 | wps 6057.1 | wpb 290.8 | bsz 1 | num_updates 6103 | best_loss 3.979\n",
      "2023-04-17 12:20:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 191 @ 6103 updates\n",
      "2023-04-17 12:20:40 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:20:55 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:20:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 191 @ 6103 updates, score 4.868) (writing took 15.347682843101211 seconds)\n",
      "2023-04-17 12:20:55 | INFO | fairseq_cli.train | end of epoch 191 (average epoch stats below)\n",
      "2023-04-17 12:20:55 | INFO | train | epoch 191 | loss 2.912 | nll_loss 0.769 | ppl 1.7 | wps 745.6 | ups 1.1 | wpb 678.5 | bsz 2 | num_updates 6103 | lr 2.78857e-05 | gnorm 4.688 | clip 100 | loss_scale 0.25 | train_wall 13 | gb_free 13.9 | wall 5335\n",
      "2023-04-17 12:20:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:20:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:20:55 | INFO | fairseq.trainer | begin training epoch 192\n",
      "2023-04-17 12:20:55 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:21:08 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:21:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:21:08 | INFO | valid | epoch 192 | valid on 'valid' subset | loss 4.877 | nll_loss 2.954 | ppl 7.75 | wps 6211.2 | wpb 290.8 | bsz 1 | num_updates 6135 | best_loss 3.979\n",
      "2023-04-17 12:21:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 192 @ 6135 updates\n",
      "2023-04-17 12:21:08 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:21:22 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:21:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 192 @ 6135 updates, score 4.877) (writing took 13.848151571000926 seconds)\n",
      "2023-04-17 12:21:22 | INFO | fairseq_cli.train | end of epoch 192 (average epoch stats below)\n",
      "2023-04-17 12:21:22 | INFO | train | epoch 192 | loss 2.89 | nll_loss 0.74 | ppl 1.67 | wps 813 | ups 1.2 | wpb 678.5 | bsz 2 | num_updates 6135 | lr 2.78736e-05 | gnorm 4.807 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 5362\n",
      "2023-04-17 12:21:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:21:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:21:22 | INFO | fairseq.trainer | begin training epoch 193\n",
      "2023-04-17 12:21:22 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:21:34 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:21:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:21:35 | INFO | valid | epoch 193 | valid on 'valid' subset | loss 4.808 | nll_loss 2.885 | ppl 7.39 | wps 6708.8 | wpb 290.8 | bsz 1 | num_updates 6167 | best_loss 3.979\n",
      "2023-04-17 12:21:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 193 @ 6167 updates\n",
      "2023-04-17 12:21:35 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:21:48 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:21:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 193 @ 6167 updates, score 4.808) (writing took 13.503432215075009 seconds)\n",
      "2023-04-17 12:21:48 | INFO | fairseq_cli.train | end of epoch 193 (average epoch stats below)\n",
      "2023-04-17 12:21:48 | INFO | train | epoch 193 | loss 2.9 | nll_loss 0.756 | ppl 1.69 | wps 832.1 | ups 1.23 | wpb 678.5 | bsz 2 | num_updates 6167 | lr 2.78615e-05 | gnorm 4.845 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 5388\n",
      "2023-04-17 12:21:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:21:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:21:48 | INFO | fairseq.trainer | begin training epoch 194\n",
      "2023-04-17 12:21:48 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:22:00 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:22:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:22:01 | INFO | valid | epoch 194 | valid on 'valid' subset | loss 4.871 | nll_loss 2.954 | ppl 7.75 | wps 6397 | wpb 290.8 | bsz 1 | num_updates 6199 | best_loss 3.979\n",
      "2023-04-17 12:22:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 194 @ 6199 updates\n",
      "2023-04-17 12:22:01 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:22:15 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:22:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 194 @ 6199 updates, score 4.871) (writing took 14.284698970033787 seconds)\n",
      "2023-04-17 12:22:15 | INFO | fairseq_cli.train | end of epoch 194 (average epoch stats below)\n",
      "2023-04-17 12:22:15 | INFO | train | epoch 194 | loss 2.889 | nll_loss 0.742 | ppl 1.67 | wps 804.6 | ups 1.19 | wpb 678.5 | bsz 2 | num_updates 6199 | lr 2.78494e-05 | gnorm 7.894 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 5415\n",
      "2023-04-17 12:22:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:22:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:22:15 | INFO | fairseq.trainer | begin training epoch 195\n",
      "2023-04-17 12:22:15 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:22:15 | INFO | train_inner | epoch 195:      1 / 32 loss=2.893, nll_loss=0.746, ppl=1.68, wps=699.4, ups=1.03, wpb=678.7, bsz=2, num_updates=6200, lr=2.78491e-05, gnorm=5.807, clip=100, loss_scale=0.25, train_wall=38, gb_free=13.9, wall=5416\n",
      "2023-04-17 12:22:27 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:22:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:22:28 | INFO | valid | epoch 195 | valid on 'valid' subset | loss 4.876 | nll_loss 2.947 | ppl 7.71 | wps 6322.9 | wpb 290.8 | bsz 1 | num_updates 6231 | best_loss 3.979\n",
      "2023-04-17 12:22:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 195 @ 6231 updates\n",
      "2023-04-17 12:22:28 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 12:22:41 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:22:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 195 @ 6231 updates, score 4.876) (writing took 13.197733443928882 seconds)\n",
      "2023-04-17 12:22:41 | INFO | fairseq_cli.train | end of epoch 195 (average epoch stats below)\n",
      "2023-04-17 12:22:41 | INFO | train | epoch 195 | loss 2.905 | nll_loss 0.759 | ppl 1.69 | wps 841.1 | ups 1.24 | wpb 678.5 | bsz 2 | num_updates 6231 | lr 2.78374e-05 | gnorm 6.493 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 5441\n",
      "2023-04-17 12:22:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:22:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:22:41 | INFO | fairseq.trainer | begin training epoch 196\n",
      "2023-04-17 12:22:41 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:22:53 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:22:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:22:53 | INFO | valid | epoch 196 | valid on 'valid' subset | loss 4.891 | nll_loss 2.967 | ppl 7.82 | wps 6505.2 | wpb 290.8 | bsz 1 | num_updates 6263 | best_loss 3.979\n",
      "2023-04-17 12:22:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 196 @ 6263 updates\n",
      "2023-04-17 12:22:53 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:23:07 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:23:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 196 @ 6263 updates, score 4.891) (writing took 13.831581605947576 seconds)\n",
      "2023-04-17 12:23:07 | INFO | fairseq_cli.train | end of epoch 196 (average epoch stats below)\n",
      "2023-04-17 12:23:07 | INFO | train | epoch 196 | loss 2.937 | nll_loss 0.789 | ppl 1.73 | wps 820 | ups 1.21 | wpb 678.5 | bsz 2 | num_updates 6263 | lr 2.78253e-05 | gnorm 6.856 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 5468\n",
      "2023-04-17 12:23:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:23:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:23:07 | INFO | fairseq.trainer | begin training epoch 197\n",
      "2023-04-17 12:23:07 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:23:20 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:23:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:23:20 | INFO | valid | epoch 197 | valid on 'valid' subset | loss 4.874 | nll_loss 2.964 | ppl 7.8 | wps 6570.2 | wpb 290.8 | bsz 1 | num_updates 6295 | best_loss 3.979\n",
      "2023-04-17 12:23:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 197 @ 6295 updates\n",
      "2023-04-17 12:23:20 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:23:33 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:23:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 197 @ 6295 updates, score 4.874) (writing took 13.68386965605896 seconds)\n",
      "2023-04-17 12:23:34 | INFO | fairseq_cli.train | end of epoch 197 (average epoch stats below)\n",
      "2023-04-17 12:23:34 | INFO | train | epoch 197 | loss 2.885 | nll_loss 0.734 | ppl 1.66 | wps 826.4 | ups 1.22 | wpb 678.5 | bsz 2 | num_updates 6295 | lr 2.78132e-05 | gnorm 7.217 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 5494\n",
      "2023-04-17 12:23:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:23:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:23:34 | INFO | fairseq.trainer | begin training epoch 198\n",
      "2023-04-17 12:23:34 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:23:35 | INFO | train_inner | epoch 198:      5 / 32 loss=2.906, nll_loss=0.758, ppl=1.69, wps=851.4, ups=1.25, wpb=680.9, bsz=2, num_updates=6300, lr=2.78113e-05, gnorm=6.738, clip=100, loss_scale=0.25, train_wall=38, gb_free=13.9, wall=5496\n",
      "2023-04-17 12:23:46 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:23:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:23:46 | INFO | valid | epoch 198 | valid on 'valid' subset | loss 4.897 | nll_loss 2.991 | ppl 7.95 | wps 6568.6 | wpb 290.8 | bsz 1 | num_updates 6327 | best_loss 3.979\n",
      "2023-04-17 12:23:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 198 @ 6327 updates\n",
      "2023-04-17 12:23:46 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:23:59 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:23:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 198 @ 6327 updates, score 4.897) (writing took 13.571280194912106 seconds)\n",
      "2023-04-17 12:23:59 | INFO | fairseq_cli.train | end of epoch 198 (average epoch stats below)\n",
      "2023-04-17 12:23:59 | INFO | train | epoch 198 | loss 2.868 | nll_loss 0.717 | ppl 1.64 | wps 843.9 | ups 1.24 | wpb 678.5 | bsz 2 | num_updates 6327 | lr 2.78011e-05 | gnorm 4.813 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 5520\n",
      "2023-04-17 12:23:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:23:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:23:59 | INFO | fairseq.trainer | begin training epoch 199\n",
      "2023-04-17 12:23:59 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:24:11 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:24:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:24:11 | INFO | valid | epoch 199 | valid on 'valid' subset | loss 4.955 | nll_loss 3.043 | ppl 8.24 | wps 6838 | wpb 290.8 | bsz 1 | num_updates 6359 | best_loss 3.979\n",
      "2023-04-17 12:24:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 199 @ 6359 updates\n",
      "2023-04-17 12:24:11 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:24:24 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:24:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 199 @ 6359 updates, score 4.955) (writing took 13.16460802697111 seconds)\n",
      "2023-04-17 12:24:25 | INFO | fairseq_cli.train | end of epoch 199 (average epoch stats below)\n",
      "2023-04-17 12:24:25 | INFO | train | epoch 199 | loss 2.855 | nll_loss 0.705 | ppl 1.63 | wps 859.9 | ups 1.27 | wpb 678.5 | bsz 2 | num_updates 6359 | lr 2.77891e-05 | gnorm 10.986 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 5545\n",
      "2023-04-17 12:24:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:24:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:24:25 | INFO | fairseq.trainer | begin training epoch 200\n",
      "2023-04-17 12:24:25 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:24:37 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:24:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:24:37 | INFO | valid | epoch 200 | valid on 'valid' subset | loss 4.961 | nll_loss 3.048 | ppl 8.27 | wps 6331 | wpb 290.8 | bsz 1 | num_updates 6391 | best_loss 3.979\n",
      "2023-04-17 12:24:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 200 @ 6391 updates\n",
      "2023-04-17 12:24:37 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:24:52 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 12:24:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 200 @ 6391 updates, score 4.961) (writing took 14.51552428107243 seconds)\n",
      "2023-04-17 12:24:52 | INFO | fairseq_cli.train | end of epoch 200 (average epoch stats below)\n",
      "2023-04-17 12:24:52 | INFO | train | epoch 200 | loss 2.855 | nll_loss 0.703 | ppl 1.63 | wps 791.8 | ups 1.17 | wpb 678.5 | bsz 2 | num_updates 6391 | lr 2.7777e-05 | gnorm 4.527 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 5572\n",
      "2023-04-17 12:24:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:24:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:24:52 | INFO | fairseq.trainer | begin training epoch 201\n",
      "2023-04-17 12:24:52 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:24:56 | INFO | train_inner | epoch 201:      9 / 32 loss=2.859, nll_loss=0.707, ppl=1.63, wps=844.7, ups=1.24, wpb=679.4, bsz=2, num_updates=6400, lr=2.77736e-05, gnorm=6.703, clip=100, loss_scale=0.25, train_wall=38, gb_free=13.9, wall=5576\n",
      "2023-04-17 12:25:05 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:25:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:25:05 | INFO | valid | epoch 201 | valid on 'valid' subset | loss 4.947 | nll_loss 3.062 | ppl 8.35 | wps 5720.9 | wpb 290.8 | bsz 1 | num_updates 6423 | best_loss 3.979\n",
      "2023-04-17 12:25:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 201 @ 6423 updates\n",
      "2023-04-17 12:25:05 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:25:19 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:25:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 201 @ 6423 updates, score 4.947) (writing took 14.225907539948821 seconds)\n",
      "2023-04-17 12:25:20 | INFO | fairseq_cli.train | end of epoch 201 (average epoch stats below)\n",
      "2023-04-17 12:25:20 | INFO | train | epoch 201 | loss 2.852 | nll_loss 0.7 | ppl 1.62 | wps 784.2 | ups 1.16 | wpb 678.5 | bsz 2 | num_updates 6423 | lr 2.77649e-05 | gnorm 4.898 | clip 100 | loss_scale 0.25 | train_wall 13 | gb_free 13.9 | wall 5600\n",
      "2023-04-17 12:25:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:25:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:25:20 | INFO | fairseq.trainer | begin training epoch 202\n",
      "2023-04-17 12:25:20 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:25:32 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:25:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:25:33 | INFO | valid | epoch 202 | valid on 'valid' subset | loss 4.96 | nll_loss 3.046 | ppl 8.26 | wps 6054.5 | wpb 290.8 | bsz 1 | num_updates 6455 | best_loss 3.979\n",
      "2023-04-17 12:25:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 202 @ 6455 updates\n",
      "2023-04-17 12:25:33 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:25:46 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:25:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 202 @ 6455 updates, score 4.96) (writing took 14.07850939896889 seconds)\n",
      "2023-04-17 12:25:47 | INFO | fairseq_cli.train | end of epoch 202 (average epoch stats below)\n",
      "2023-04-17 12:25:47 | INFO | train | epoch 202 | loss 2.846 | nll_loss 0.693 | ppl 1.62 | wps 804.6 | ups 1.19 | wpb 678.5 | bsz 2 | num_updates 6455 | lr 2.77528e-05 | gnorm 5.205 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 5627\n",
      "2023-04-17 12:25:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:25:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:25:47 | INFO | fairseq.trainer | begin training epoch 203\n",
      "2023-04-17 12:25:47 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:25:59 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:25:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:25:59 | INFO | valid | epoch 203 | valid on 'valid' subset | loss 4.993 | nll_loss 3.088 | ppl 8.51 | wps 6688.3 | wpb 290.8 | bsz 1 | num_updates 6487 | best_loss 3.979\n",
      "2023-04-17 12:25:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 203 @ 6487 updates\n",
      "2023-04-17 12:25:59 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:26:09 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:26:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 203 @ 6487 updates, score 4.993) (writing took 9.874191237962805 seconds)\n",
      "2023-04-17 12:26:09 | INFO | fairseq_cli.train | end of epoch 203 (average epoch stats below)\n",
      "2023-04-17 12:26:09 | INFO | train | epoch 203 | loss 2.843 | nll_loss 0.69 | ppl 1.61 | wps 958.6 | ups 1.41 | wpb 678.5 | bsz 2 | num_updates 6487 | lr 2.77408e-05 | gnorm 4.91 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 5650\n",
      "2023-04-17 12:26:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:26:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:26:09 | INFO | fairseq.trainer | begin training epoch 204\n",
      "2023-04-17 12:26:09 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:26:14 | INFO | train_inner | epoch 204:     13 / 32 loss=2.846, nll_loss=0.693, ppl=1.62, wps=859.3, ups=1.27, wpb=675.7, bsz=2, num_updates=6500, lr=2.77358e-05, gnorm=5.093, clip=100, loss_scale=0.25, train_wall=39, gb_free=13.9, wall=5655\n",
      "2023-04-17 12:26:22 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:26:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:26:22 | INFO | valid | epoch 204 | valid on 'valid' subset | loss 4.928 | nll_loss 3 | ppl 8 | wps 6132.2 | wpb 290.8 | bsz 1 | num_updates 6519 | best_loss 3.979\n",
      "2023-04-17 12:26:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 204 @ 6519 updates\n",
      "2023-04-17 12:26:22 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:26:35 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:26:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 204 @ 6519 updates, score 4.928) (writing took 13.347688029985875 seconds)\n",
      "2023-04-17 12:26:35 | INFO | fairseq_cli.train | end of epoch 204 (average epoch stats below)\n",
      "2023-04-17 12:26:35 | INFO | train | epoch 204 | loss 2.854 | nll_loss 0.702 | ppl 1.63 | wps 834.6 | ups 1.23 | wpb 678.5 | bsz 2 | num_updates 6519 | lr 2.77287e-05 | gnorm 5.423 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 5676\n",
      "2023-04-17 12:26:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:26:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:26:35 | INFO | fairseq.trainer | begin training epoch 205\n",
      "2023-04-17 12:26:35 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:26:47 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:26:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:26:48 | INFO | valid | epoch 205 | valid on 'valid' subset | loss 4.925 | nll_loss 3.033 | ppl 8.18 | wps 6573.9 | wpb 290.8 | bsz 1 | num_updates 6551 | best_loss 3.979\n",
      "2023-04-17 12:26:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 205 @ 6551 updates\n",
      "2023-04-17 12:26:48 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 12:27:01 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:27:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 205 @ 6551 updates, score 4.925) (writing took 14.023091137059964 seconds)\n",
      "2023-04-17 12:27:02 | INFO | fairseq_cli.train | end of epoch 205 (average epoch stats below)\n",
      "2023-04-17 12:27:02 | INFO | train | epoch 205 | loss 2.831 | nll_loss 0.678 | ppl 1.6 | wps 822.6 | ups 1.21 | wpb 678.5 | bsz 2 | num_updates 6551 | lr 2.77166e-05 | gnorm 4.977 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 5702\n",
      "2023-04-17 12:27:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:27:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:27:02 | INFO | fairseq.trainer | begin training epoch 206\n",
      "2023-04-17 12:27:02 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:27:14 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:27:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:27:14 | INFO | valid | epoch 206 | valid on 'valid' subset | loss 4.94 | nll_loss 3.032 | ppl 8.18 | wps 6639.8 | wpb 290.8 | bsz 1 | num_updates 6583 | best_loss 3.979\n",
      "2023-04-17 12:27:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 206 @ 6583 updates\n",
      "2023-04-17 12:27:14 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:27:27 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:27:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 206 @ 6583 updates, score 4.94) (writing took 13.18579811998643 seconds)\n",
      "2023-04-17 12:27:27 | INFO | fairseq_cli.train | end of epoch 206 (average epoch stats below)\n",
      "2023-04-17 12:27:27 | INFO | train | epoch 206 | loss 2.826 | nll_loss 0.672 | ppl 1.59 | wps 853.9 | ups 1.26 | wpb 678.5 | bsz 2 | num_updates 6583 | lr 2.77045e-05 | gnorm 4.705 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 5727\n",
      "2023-04-17 12:27:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:27:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:27:27 | INFO | fairseq.trainer | begin training epoch 207\n",
      "2023-04-17 12:27:27 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:27:34 | INFO | train_inner | epoch 207:     17 / 32 loss=2.835, nll_loss=0.682, ppl=1.6, wps=860.5, ups=1.26, wpb=682, bsz=2, num_updates=6600, lr=2.76981e-05, gnorm=4.89, clip=100, loss_scale=0.25, train_wall=37, gb_free=13.9, wall=5734\n",
      "2023-04-17 12:27:39 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:27:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:27:39 | INFO | valid | epoch 207 | valid on 'valid' subset | loss 4.992 | nll_loss 3.071 | ppl 8.4 | wps 6535.9 | wpb 290.8 | bsz 1 | num_updates 6615 | best_loss 3.979\n",
      "2023-04-17 12:27:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 207 @ 6615 updates\n",
      "2023-04-17 12:27:39 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:27:53 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:27:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 207 @ 6615 updates, score 4.992) (writing took 13.621344082057476 seconds)\n",
      "2023-04-17 12:27:53 | INFO | fairseq_cli.train | end of epoch 207 (average epoch stats below)\n",
      "2023-04-17 12:27:53 | INFO | train | epoch 207 | loss 2.819 | nll_loss 0.666 | ppl 1.59 | wps 837 | ups 1.23 | wpb 678.5 | bsz 2 | num_updates 6615 | lr 2.76925e-05 | gnorm 4.613 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 5753\n",
      "2023-04-17 12:27:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:27:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:27:53 | INFO | fairseq.trainer | begin training epoch 208\n",
      "2023-04-17 12:27:53 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:28:05 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:28:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:28:06 | INFO | valid | epoch 208 | valid on 'valid' subset | loss 4.994 | nll_loss 3.093 | ppl 8.53 | wps 6959.3 | wpb 290.8 | bsz 1 | num_updates 6647 | best_loss 3.979\n",
      "2023-04-17 12:28:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 208 @ 6647 updates\n",
      "2023-04-17 12:28:06 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:28:19 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:28:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 208 @ 6647 updates, score 4.994) (writing took 13.188870026031509 seconds)\n",
      "2023-04-17 12:28:19 | INFO | fairseq_cli.train | end of epoch 208 (average epoch stats below)\n",
      "2023-04-17 12:28:19 | INFO | train | epoch 208 | loss 2.813 | nll_loss 0.657 | ppl 1.58 | wps 846.4 | ups 1.25 | wpb 678.5 | bsz 2 | num_updates 6647 | lr 2.76804e-05 | gnorm 4.61 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 5779\n",
      "2023-04-17 12:28:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:28:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:28:19 | INFO | fairseq.trainer | begin training epoch 209\n",
      "2023-04-17 12:28:19 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:28:31 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:28:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:28:31 | INFO | valid | epoch 209 | valid on 'valid' subset | loss 5.029 | nll_loss 3.132 | ppl 8.77 | wps 6429.3 | wpb 290.8 | bsz 1 | num_updates 6679 | best_loss 3.979\n",
      "2023-04-17 12:28:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 209 @ 6679 updates\n",
      "2023-04-17 12:28:31 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:28:45 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:28:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 209 @ 6679 updates, score 5.029) (writing took 14.412530553061515 seconds)\n",
      "2023-04-17 12:28:46 | INFO | fairseq_cli.train | end of epoch 209 (average epoch stats below)\n",
      "2023-04-17 12:28:46 | INFO | train | epoch 209 | loss 2.809 | nll_loss 0.652 | ppl 1.57 | wps 810.5 | ups 1.19 | wpb 678.5 | bsz 2 | num_updates 6679 | lr 2.76683e-05 | gnorm 4.819 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 5806\n",
      "2023-04-17 12:28:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:28:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:28:46 | INFO | fairseq.trainer | begin training epoch 210\n",
      "2023-04-17 12:28:46 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:28:54 | INFO | train_inner | epoch 210:     21 / 32 loss=2.809, nll_loss=0.652, ppl=1.57, wps=846.9, ups=1.25, wpb=675.7, bsz=2, num_updates=6700, lr=2.76604e-05, gnorm=4.757, clip=100, loss_scale=0.25, train_wall=37, gb_free=13.9, wall=5814\n",
      "2023-04-17 12:28:58 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:28:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:28:58 | INFO | valid | epoch 210 | valid on 'valid' subset | loss 5.027 | nll_loss 3.138 | ppl 8.8 | wps 6620.3 | wpb 290.8 | bsz 1 | num_updates 6711 | best_loss 3.979\n",
      "2023-04-17 12:28:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 210 @ 6711 updates\n",
      "2023-04-17 12:28:58 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 12:29:10 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:29:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 210 @ 6711 updates, score 5.027) (writing took 12.969366375007667 seconds)\n",
      "2023-04-17 12:29:11 | INFO | fairseq_cli.train | end of epoch 210 (average epoch stats below)\n",
      "2023-04-17 12:29:11 | INFO | train | epoch 210 | loss 2.808 | nll_loss 0.652 | ppl 1.57 | wps 862.1 | ups 1.27 | wpb 678.5 | bsz 2 | num_updates 6711 | lr 2.76562e-05 | gnorm 4.736 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 5831\n",
      "2023-04-17 12:29:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:29:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:29:11 | INFO | fairseq.trainer | begin training epoch 211\n",
      "2023-04-17 12:29:11 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:29:23 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:29:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:29:23 | INFO | valid | epoch 211 | valid on 'valid' subset | loss 5.013 | nll_loss 3.126 | ppl 8.73 | wps 6692.9 | wpb 290.8 | bsz 1 | num_updates 6743 | best_loss 3.979\n",
      "2023-04-17 12:29:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 211 @ 6743 updates\n",
      "2023-04-17 12:29:23 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:29:36 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:29:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 211 @ 6743 updates, score 5.013) (writing took 13.735021673026495 seconds)\n",
      "2023-04-17 12:29:37 | INFO | fairseq_cli.train | end of epoch 211 (average epoch stats below)\n",
      "2023-04-17 12:29:37 | INFO | train | epoch 211 | loss 2.799 | nll_loss 0.641 | ppl 1.56 | wps 838.6 | ups 1.24 | wpb 678.5 | bsz 2 | num_updates 6743 | lr 2.76442e-05 | gnorm 4.696 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 5857\n",
      "2023-04-17 12:29:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:29:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:29:37 | INFO | fairseq.trainer | begin training epoch 212\n",
      "2023-04-17 12:29:37 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:29:49 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:29:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:29:49 | INFO | valid | epoch 212 | valid on 'valid' subset | loss 4.966 | nll_loss 3.06 | ppl 8.34 | wps 6641.1 | wpb 290.8 | bsz 1 | num_updates 6775 | best_loss 3.979\n",
      "2023-04-17 12:29:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 212 @ 6775 updates\n",
      "2023-04-17 12:29:49 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:30:02 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:30:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 212 @ 6775 updates, score 4.966) (writing took 13.20326500700321 seconds)\n",
      "2023-04-17 12:30:02 | INFO | fairseq_cli.train | end of epoch 212 (average epoch stats below)\n",
      "2023-04-17 12:30:02 | INFO | train | epoch 212 | loss 2.793 | nll_loss 0.636 | ppl 1.55 | wps 850.1 | ups 1.25 | wpb 678.5 | bsz 2 | num_updates 6775 | lr 2.76321e-05 | gnorm 4.872 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 5882\n",
      "2023-04-17 12:30:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:30:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:30:02 | INFO | fairseq.trainer | begin training epoch 213\n",
      "2023-04-17 12:30:02 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:30:12 | INFO | train_inner | epoch 213:     25 / 32 loss=2.798, nll_loss=0.641, ppl=1.56, wps=882.1, ups=1.28, wpb=689.4, bsz=2, num_updates=6800, lr=2.76226e-05, gnorm=4.711, clip=100, loss_scale=0.25, train_wall=37, gb_free=13.9, wall=5892\n",
      "2023-04-17 12:30:14 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:30:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:30:14 | INFO | valid | epoch 213 | valid on 'valid' subset | loss 4.991 | nll_loss 3.111 | ppl 8.64 | wps 6536.1 | wpb 290.8 | bsz 1 | num_updates 6807 | best_loss 3.979\n",
      "2023-04-17 12:30:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 213 @ 6807 updates\n",
      "2023-04-17 12:30:14 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:30:28 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:30:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 213 @ 6807 updates, score 4.991) (writing took 13.861511521041393 seconds)\n",
      "2023-04-17 12:30:28 | INFO | fairseq_cli.train | end of epoch 213 (average epoch stats below)\n",
      "2023-04-17 12:30:28 | INFO | train | epoch 213 | loss 2.782 | nll_loss 0.622 | ppl 1.54 | wps 829 | ups 1.22 | wpb 678.5 | bsz 2 | num_updates 6807 | lr 2.762e-05 | gnorm 4.696 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 5909\n",
      "2023-04-17 12:30:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:30:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:30:28 | INFO | fairseq.trainer | begin training epoch 214\n",
      "2023-04-17 12:30:28 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:30:40 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:30:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:30:41 | INFO | valid | epoch 214 | valid on 'valid' subset | loss 5.008 | nll_loss 3.104 | ppl 8.6 | wps 6115.3 | wpb 290.8 | bsz 1 | num_updates 6839 | best_loss 3.979\n",
      "2023-04-17 12:30:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 214 @ 6839 updates\n",
      "2023-04-17 12:30:41 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:30:53 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:30:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 214 @ 6839 updates, score 5.008) (writing took 12.820554650039412 seconds)\n",
      "2023-04-17 12:30:53 | INFO | fairseq_cli.train | end of epoch 214 (average epoch stats below)\n",
      "2023-04-17 12:30:53 | INFO | train | epoch 214 | loss 2.785 | nll_loss 0.626 | ppl 1.54 | wps 865.6 | ups 1.28 | wpb 678.5 | bsz 2 | num_updates 6839 | lr 2.76079e-05 | gnorm 4.704 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 5934\n",
      "2023-04-17 12:30:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:30:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:30:53 | INFO | fairseq.trainer | begin training epoch 215\n",
      "2023-04-17 12:30:53 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:31:05 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:31:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:31:05 | INFO | valid | epoch 215 | valid on 'valid' subset | loss 5.001 | nll_loss 3.089 | ppl 8.51 | wps 6992.9 | wpb 290.8 | bsz 1 | num_updates 6871 | best_loss 3.979\n",
      "2023-04-17 12:31:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 215 @ 6871 updates\n",
      "2023-04-17 12:31:05 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:31:18 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 12:31:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 215 @ 6871 updates, score 5.001) (writing took 13.057736080954783 seconds)\n",
      "2023-04-17 12:31:18 | INFO | fairseq_cli.train | end of epoch 215 (average epoch stats below)\n",
      "2023-04-17 12:31:18 | INFO | train | epoch 215 | loss 2.789 | nll_loss 0.631 | ppl 1.55 | wps 871.3 | ups 1.28 | wpb 678.5 | bsz 2 | num_updates 6871 | lr 2.75958e-05 | gnorm 4.866 | clip 100 | loss_scale 0.25 | train_wall 11 | gb_free 13.9 | wall 5959\n",
      "2023-04-17 12:31:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:31:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:31:18 | INFO | fairseq.trainer | begin training epoch 216\n",
      "2023-04-17 12:31:18 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:31:30 | INFO | train_inner | epoch 216:     29 / 32 loss=2.782, nll_loss=0.623, ppl=1.54, wps=855.2, ups=1.28, wpb=667.2, bsz=2, num_updates=6900, lr=2.75849e-05, gnorm=4.712, clip=100, loss_scale=0.25, train_wall=37, gb_free=13.9, wall=5970\n",
      "2023-04-17 12:31:31 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:31:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:31:31 | INFO | valid | epoch 216 | valid on 'valid' subset | loss 5.038 | nll_loss 3.152 | ppl 8.89 | wps 6191.7 | wpb 290.8 | bsz 1 | num_updates 6903 | best_loss 3.979\n",
      "2023-04-17 12:31:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 216 @ 6903 updates\n",
      "2023-04-17 12:31:31 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:31:44 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:31:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 216 @ 6903 updates, score 5.038) (writing took 12.933931073988788 seconds)\n",
      "2023-04-17 12:31:44 | INFO | fairseq_cli.train | end of epoch 216 (average epoch stats below)\n",
      "2023-04-17 12:31:44 | INFO | train | epoch 216 | loss 2.78 | nll_loss 0.621 | ppl 1.54 | wps 849 | ups 1.25 | wpb 678.5 | bsz 2 | num_updates 6903 | lr 2.75838e-05 | gnorm 4.517 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 5984\n",
      "2023-04-17 12:31:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:31:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:31:44 | INFO | fairseq.trainer | begin training epoch 217\n",
      "2023-04-17 12:31:44 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:31:56 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:31:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:31:56 | INFO | valid | epoch 217 | valid on 'valid' subset | loss 5.006 | nll_loss 3.104 | ppl 8.6 | wps 7239 | wpb 290.8 | bsz 1 | num_updates 6935 | best_loss 3.979\n",
      "2023-04-17 12:31:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 217 @ 6935 updates\n",
      "2023-04-17 12:31:56 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:32:08 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:32:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 217 @ 6935 updates, score 5.006) (writing took 11.962554616038688 seconds)\n",
      "2023-04-17 12:32:08 | INFO | fairseq_cli.train | end of epoch 217 (average epoch stats below)\n",
      "2023-04-17 12:32:08 | INFO | train | epoch 217 | loss 2.778 | nll_loss 0.62 | ppl 1.54 | wps 890.8 | ups 1.31 | wpb 678.5 | bsz 2 | num_updates 6935 | lr 2.75717e-05 | gnorm 4.853 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 6009\n",
      "2023-04-17 12:32:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:32:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:32:08 | INFO | fairseq.trainer | begin training epoch 218\n",
      "2023-04-17 12:32:08 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:32:20 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:32:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:32:20 | INFO | valid | epoch 218 | valid on 'valid' subset | loss 5.075 | nll_loss 3.186 | ppl 9.1 | wps 7314.7 | wpb 290.8 | bsz 1 | num_updates 6967 | best_loss 3.979\n",
      "2023-04-17 12:32:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 218 @ 6967 updates\n",
      "2023-04-17 12:32:20 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:32:32 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:32:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 218 @ 6967 updates, score 5.075) (writing took 12.477198838954791 seconds)\n",
      "2023-04-17 12:32:32 | INFO | fairseq_cli.train | end of epoch 218 (average epoch stats below)\n",
      "2023-04-17 12:32:32 | INFO | train | epoch 218 | loss 2.764 | nll_loss 0.602 | ppl 1.52 | wps 902 | ups 1.33 | wpb 678.5 | bsz 2 | num_updates 6967 | lr 2.75596e-05 | gnorm 4.546 | clip 100 | loss_scale 0.25 | train_wall 11 | gb_free 13.9 | wall 6033\n",
      "2023-04-17 12:32:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:32:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:32:32 | INFO | fairseq.trainer | begin training epoch 219\n",
      "2023-04-17 12:32:32 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:32:44 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:32:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:32:45 | INFO | valid | epoch 219 | valid on 'valid' subset | loss 5.052 | nll_loss 3.155 | ppl 8.9 | wps 6365.5 | wpb 290.8 | bsz 1 | num_updates 6999 | best_loss 3.979\n",
      "2023-04-17 12:32:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 219 @ 6999 updates\n",
      "2023-04-17 12:32:45 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:32:58 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:32:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 219 @ 6999 updates, score 5.052) (writing took 13.139873676002026 seconds)\n",
      "2023-04-17 12:32:58 | INFO | fairseq_cli.train | end of epoch 219 (average epoch stats below)\n",
      "2023-04-17 12:32:58 | INFO | train | epoch 219 | loss 2.777 | nll_loss 0.618 | ppl 1.54 | wps 858 | ups 1.26 | wpb 678.5 | bsz 2 | num_updates 6999 | lr 2.75475e-05 | gnorm 4.637 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 6058\n",
      "2023-04-17 12:32:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:32:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:32:58 | INFO | fairseq.trainer | begin training epoch 220\n",
      "2023-04-17 12:32:58 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:32:58 | INFO | train_inner | epoch 220:      1 / 32 loss=2.773, nll_loss=0.614, ppl=1.53, wps=767.8, ups=1.13, wpb=678.9, bsz=2, num_updates=7000, lr=2.75472e-05, gnorm=4.681, clip=100, loss_scale=0.25, train_wall=36, gb_free=13.9, wall=6058\n",
      "2023-04-17 12:33:10 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:33:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:33:11 | INFO | valid | epoch 220 | valid on 'valid' subset | loss 5.06 | nll_loss 3.183 | ppl 9.08 | wps 6170.8 | wpb 290.8 | bsz 1 | num_updates 7031 | best_loss 3.979\n",
      "2023-04-17 12:33:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 220 @ 7031 updates\n",
      "2023-04-17 12:33:11 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 12:33:23 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:33:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 220 @ 7031 updates, score 5.06) (writing took 12.54122432693839 seconds)\n",
      "2023-04-17 12:33:23 | INFO | fairseq_cli.train | end of epoch 220 (average epoch stats below)\n",
      "2023-04-17 12:33:23 | INFO | train | epoch 220 | loss 2.761 | nll_loss 0.601 | ppl 1.52 | wps 850.3 | ups 1.25 | wpb 678.5 | bsz 2 | num_updates 7031 | lr 2.75355e-05 | gnorm 4.609 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 6083\n",
      "2023-04-17 12:33:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:33:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:33:23 | INFO | fairseq.trainer | begin training epoch 221\n",
      "2023-04-17 12:33:23 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:33:35 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:33:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:33:36 | INFO | valid | epoch 221 | valid on 'valid' subset | loss 5.044 | nll_loss 3.156 | ppl 8.91 | wps 6391.9 | wpb 290.8 | bsz 1 | num_updates 7063 | best_loss 3.979\n",
      "2023-04-17 12:33:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 221 @ 7063 updates\n",
      "2023-04-17 12:33:36 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:33:48 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:33:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 221 @ 7063 updates, score 5.044) (writing took 12.936273711035028 seconds)\n",
      "2023-04-17 12:33:49 | INFO | fairseq_cli.train | end of epoch 221 (average epoch stats below)\n",
      "2023-04-17 12:33:49 | INFO | train | epoch 221 | loss 2.746 | nll_loss 0.581 | ppl 1.5 | wps 853.4 | ups 1.26 | wpb 678.5 | bsz 2 | num_updates 7063 | lr 2.75234e-05 | gnorm 4.444 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 6109\n",
      "2023-04-17 12:33:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:33:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:33:49 | INFO | fairseq.trainer | begin training epoch 222\n",
      "2023-04-17 12:33:49 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:34:01 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:34:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:34:01 | INFO | valid | epoch 222 | valid on 'valid' subset | loss 5.043 | nll_loss 3.149 | ppl 8.87 | wps 6402 | wpb 290.8 | bsz 1 | num_updates 7095 | best_loss 3.979\n",
      "2023-04-17 12:34:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 222 @ 7095 updates\n",
      "2023-04-17 12:34:01 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:34:14 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:34:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 222 @ 7095 updates, score 5.043) (writing took 12.946717328974046 seconds)\n",
      "2023-04-17 12:34:14 | INFO | fairseq_cli.train | end of epoch 222 (average epoch stats below)\n",
      "2023-04-17 12:34:14 | INFO | train | epoch 222 | loss 2.749 | nll_loss 0.587 | ppl 1.5 | wps 857.3 | ups 1.26 | wpb 678.5 | bsz 2 | num_updates 7095 | lr 2.75113e-05 | gnorm 4.712 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 6134\n",
      "2023-04-17 12:34:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:34:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:34:14 | INFO | fairseq.trainer | begin training epoch 223\n",
      "2023-04-17 12:34:14 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:34:16 | INFO | train_inner | epoch 223:      5 / 32 loss=2.751, nll_loss=0.588, ppl=1.5, wps=872.7, ups=1.28, wpb=679.2, bsz=2, num_updates=7100, lr=2.75094e-05, gnorm=4.56, clip=100, loss_scale=0.25, train_wall=38, gb_free=13.9, wall=6136\n",
      "2023-04-17 12:34:26 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:34:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:34:27 | INFO | valid | epoch 223 | valid on 'valid' subset | loss 5.06 | nll_loss 3.168 | ppl 8.99 | wps 6349.4 | wpb 290.8 | bsz 1 | num_updates 7127 | best_loss 3.979\n",
      "2023-04-17 12:34:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 223 @ 7127 updates\n",
      "2023-04-17 12:34:27 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:34:40 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:34:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 223 @ 7127 updates, score 5.06) (writing took 13.230400447966531 seconds)\n",
      "2023-04-17 12:34:40 | INFO | fairseq_cli.train | end of epoch 223 (average epoch stats below)\n",
      "2023-04-17 12:34:40 | INFO | train | epoch 223 | loss 2.737 | nll_loss 0.573 | ppl 1.49 | wps 842.3 | ups 1.24 | wpb 678.5 | bsz 2 | num_updates 7127 | lr 2.74992e-05 | gnorm 4.377 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 6160\n",
      "2023-04-17 12:34:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:34:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:34:40 | INFO | fairseq.trainer | begin training epoch 224\n",
      "2023-04-17 12:34:40 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:34:52 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:34:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:34:53 | INFO | valid | epoch 224 | valid on 'valid' subset | loss 5.031 | nll_loss 3.152 | ppl 8.89 | wps 6398.3 | wpb 290.8 | bsz 1 | num_updates 7159 | best_loss 3.979\n",
      "2023-04-17 12:34:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 224 @ 7159 updates\n",
      "2023-04-17 12:34:53 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:35:05 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:35:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 224 @ 7159 updates, score 5.031) (writing took 12.765157588990405 seconds)\n",
      "2023-04-17 12:35:05 | INFO | fairseq_cli.train | end of epoch 224 (average epoch stats below)\n",
      "2023-04-17 12:35:05 | INFO | train | epoch 224 | loss 2.75 | nll_loss 0.588 | ppl 1.5 | wps 845.7 | ups 1.25 | wpb 678.5 | bsz 2 | num_updates 7159 | lr 2.74872e-05 | gnorm 4.96 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 6186\n",
      "2023-04-17 12:35:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:35:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:35:05 | INFO | fairseq.trainer | begin training epoch 225\n",
      "2023-04-17 12:35:05 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:35:18 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:35:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:35:18 | INFO | valid | epoch 225 | valid on 'valid' subset | loss 5.076 | nll_loss 3.193 | ppl 9.15 | wps 5368.2 | wpb 290.8 | bsz 1 | num_updates 7191 | best_loss 3.979\n",
      "2023-04-17 12:35:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 225 @ 7191 updates\n",
      "2023-04-17 12:35:18 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:35:31 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 12:35:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 225 @ 7191 updates, score 5.076) (writing took 12.981301130959764 seconds)\n",
      "2023-04-17 12:35:31 | INFO | fairseq_cli.train | end of epoch 225 (average epoch stats below)\n",
      "2023-04-17 12:35:31 | INFO | train | epoch 225 | loss 2.746 | nll_loss 0.585 | ppl 1.5 | wps 843.5 | ups 1.24 | wpb 678.5 | bsz 2 | num_updates 7191 | lr 2.74751e-05 | gnorm 4.947 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 6211\n",
      "2023-04-17 12:35:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:35:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:35:31 | INFO | fairseq.trainer | begin training epoch 226\n",
      "2023-04-17 12:35:31 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:35:35 | INFO | train_inner | epoch 226:      9 / 32 loss=2.747, nll_loss=0.585, ppl=1.5, wps=866.8, ups=1.27, wpb=682.3, bsz=2, num_updates=7200, lr=2.74717e-05, gnorm=4.8, clip=100, loss_scale=0.25, train_wall=38, gb_free=13.9, wall=6215\n",
      "2023-04-17 12:35:43 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:35:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:35:44 | INFO | valid | epoch 226 | valid on 'valid' subset | loss 5.086 | nll_loss 3.212 | ppl 9.27 | wps 6333.5 | wpb 290.8 | bsz 1 | num_updates 7223 | best_loss 3.979\n",
      "2023-04-17 12:35:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 226 @ 7223 updates\n",
      "2023-04-17 12:35:44 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:35:56 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:35:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 226 @ 7223 updates, score 5.086) (writing took 13.025237145018764 seconds)\n",
      "2023-04-17 12:35:57 | INFO | fairseq_cli.train | end of epoch 226 (average epoch stats below)\n",
      "2023-04-17 12:35:57 | INFO | train | epoch 226 | loss 2.737 | nll_loss 0.574 | ppl 1.49 | wps 855.3 | ups 1.26 | wpb 678.5 | bsz 2 | num_updates 7223 | lr 2.7463e-05 | gnorm 4.546 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 6237\n",
      "2023-04-17 12:35:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:35:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:35:57 | INFO | fairseq.trainer | begin training epoch 227\n",
      "2023-04-17 12:35:57 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:36:09 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:36:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:36:09 | INFO | valid | epoch 227 | valid on 'valid' subset | loss 5.072 | nll_loss 3.192 | ppl 9.14 | wps 6563.6 | wpb 290.8 | bsz 1 | num_updates 7255 | best_loss 3.979\n",
      "2023-04-17 12:36:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 227 @ 7255 updates\n",
      "2023-04-17 12:36:09 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:36:22 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:36:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 227 @ 7255 updates, score 5.072) (writing took 12.863606838975102 seconds)\n",
      "2023-04-17 12:36:22 | INFO | fairseq_cli.train | end of epoch 227 (average epoch stats below)\n",
      "2023-04-17 12:36:22 | INFO | train | epoch 227 | loss 2.731 | nll_loss 0.569 | ppl 1.48 | wps 859 | ups 1.27 | wpb 678.5 | bsz 2 | num_updates 7255 | lr 2.74509e-05 | gnorm 4.267 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 6262\n",
      "2023-04-17 12:36:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:36:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:36:22 | INFO | fairseq.trainer | begin training epoch 228\n",
      "2023-04-17 12:36:22 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:36:34 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:36:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:36:34 | INFO | valid | epoch 228 | valid on 'valid' subset | loss 5.078 | nll_loss 3.195 | ppl 9.16 | wps 6423.7 | wpb 290.8 | bsz 1 | num_updates 7287 | best_loss 3.979\n",
      "2023-04-17 12:36:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 228 @ 7287 updates\n",
      "2023-04-17 12:36:34 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:36:47 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:36:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 228 @ 7287 updates, score 5.078) (writing took 13.060683541931212 seconds)\n",
      "2023-04-17 12:36:47 | INFO | fairseq_cli.train | end of epoch 228 (average epoch stats below)\n",
      "2023-04-17 12:36:47 | INFO | train | epoch 228 | loss 2.732 | nll_loss 0.571 | ppl 1.49 | wps 850.8 | ups 1.25 | wpb 678.5 | bsz 2 | num_updates 7287 | lr 2.74389e-05 | gnorm 4.64 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 6288\n",
      "2023-04-17 12:36:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:36:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:36:47 | INFO | fairseq.trainer | begin training epoch 229\n",
      "2023-04-17 12:36:47 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:36:52 | INFO | train_inner | epoch 229:     13 / 32 loss=2.729, nll_loss=0.566, ppl=1.48, wps=881.3, ups=1.29, wpb=684.8, bsz=2, num_updates=7300, lr=2.7434e-05, gnorm=4.459, clip=100, loss_scale=0.25, train_wall=37, gb_free=13.9, wall=6293\n",
      "2023-04-17 12:37:00 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:37:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:37:00 | INFO | valid | epoch 229 | valid on 'valid' subset | loss 5.054 | nll_loss 3.188 | ppl 9.11 | wps 6450.8 | wpb 290.8 | bsz 1 | num_updates 7319 | best_loss 3.979\n",
      "2023-04-17 12:37:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 229 @ 7319 updates\n",
      "2023-04-17 12:37:00 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:37:12 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:37:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 229 @ 7319 updates, score 5.054) (writing took 12.775761183002032 seconds)\n",
      "2023-04-17 12:37:13 | INFO | fairseq_cli.train | end of epoch 229 (average epoch stats below)\n",
      "2023-04-17 12:37:13 | INFO | train | epoch 229 | loss 2.726 | nll_loss 0.56 | ppl 1.47 | wps 862.6 | ups 1.27 | wpb 678.5 | bsz 2 | num_updates 7319 | lr 2.74268e-05 | gnorm 4.639 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 6313\n",
      "2023-04-17 12:37:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:37:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:37:13 | INFO | fairseq.trainer | begin training epoch 230\n",
      "2023-04-17 12:37:13 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:37:24 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:37:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:37:25 | INFO | valid | epoch 230 | valid on 'valid' subset | loss 5.116 | nll_loss 3.243 | ppl 9.47 | wps 6561.7 | wpb 290.8 | bsz 1 | num_updates 7351 | best_loss 3.979\n",
      "2023-04-17 12:37:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 230 @ 7351 updates\n",
      "2023-04-17 12:37:25 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 12:37:38 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:37:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 230 @ 7351 updates, score 5.116) (writing took 13.504601313034073 seconds)\n",
      "2023-04-17 12:37:38 | INFO | fairseq_cli.train | end of epoch 230 (average epoch stats below)\n",
      "2023-04-17 12:37:38 | INFO | train | epoch 230 | loss 2.715 | nll_loss 0.551 | ppl 1.47 | wps 852.3 | ups 1.26 | wpb 678.5 | bsz 2 | num_updates 7351 | lr 2.74147e-05 | gnorm 4.87 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 6338\n",
      "2023-04-17 12:37:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:37:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:37:38 | INFO | fairseq.trainer | begin training epoch 231\n",
      "2023-04-17 12:37:38 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:37:51 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:37:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:37:51 | INFO | valid | epoch 231 | valid on 'valid' subset | loss 5.09 | nll_loss 3.213 | ppl 9.27 | wps 7322 | wpb 290.8 | bsz 1 | num_updates 7383 | best_loss 3.979\n",
      "2023-04-17 12:37:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 231 @ 7383 updates\n",
      "2023-04-17 12:37:51 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:38:03 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:38:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 231 @ 7383 updates, score 5.09) (writing took 12.670475000981241 seconds)\n",
      "2023-04-17 12:38:04 | INFO | fairseq_cli.train | end of epoch 231 (average epoch stats below)\n",
      "2023-04-17 12:38:04 | INFO | train | epoch 231 | loss 2.718 | nll_loss 0.553 | ppl 1.47 | wps 849.5 | ups 1.25 | wpb 678.5 | bsz 2 | num_updates 7383 | lr 2.74026e-05 | gnorm 4.412 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 6364\n",
      "2023-04-17 12:38:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:38:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:38:04 | INFO | fairseq.trainer | begin training epoch 232\n",
      "2023-04-17 12:38:04 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:38:10 | INFO | train_inner | epoch 232:     17 / 32 loss=2.718, nll_loss=0.553, ppl=1.47, wps=852.7, ups=1.29, wpb=661.7, bsz=2, num_updates=7400, lr=2.73962e-05, gnorm=4.693, clip=100, loss_scale=0.25, train_wall=37, gb_free=13.9, wall=6370\n",
      "2023-04-17 12:38:16 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:38:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:38:16 | INFO | valid | epoch 232 | valid on 'valid' subset | loss 5.108 | nll_loss 3.234 | ppl 9.41 | wps 6685.8 | wpb 290.8 | bsz 1 | num_updates 7415 | best_loss 3.979\n",
      "2023-04-17 12:38:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 232 @ 7415 updates\n",
      "2023-04-17 12:38:16 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:38:29 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:38:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 232 @ 7415 updates, score 5.108) (writing took 13.389607971999794 seconds)\n",
      "2023-04-17 12:38:29 | INFO | fairseq_cli.train | end of epoch 232 (average epoch stats below)\n",
      "2023-04-17 12:38:29 | INFO | train | epoch 232 | loss 2.717 | nll_loss 0.554 | ppl 1.47 | wps 840.3 | ups 1.24 | wpb 678.5 | bsz 2 | num_updates 7415 | lr 2.73906e-05 | gnorm 4.755 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 6390\n",
      "2023-04-17 12:38:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:38:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:38:29 | INFO | fairseq.trainer | begin training epoch 233\n",
      "2023-04-17 12:38:29 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:38:42 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:38:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:38:42 | INFO | valid | epoch 233 | valid on 'valid' subset | loss 5.143 | nll_loss 3.28 | ppl 9.71 | wps 6534.9 | wpb 290.8 | bsz 1 | num_updates 7447 | best_loss 3.979\n",
      "2023-04-17 12:38:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 233 @ 7447 updates\n",
      "2023-04-17 12:38:42 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:38:55 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:38:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 233 @ 7447 updates, score 5.143) (writing took 12.986845653038472 seconds)\n",
      "2023-04-17 12:38:55 | INFO | fairseq_cli.train | end of epoch 233 (average epoch stats below)\n",
      "2023-04-17 12:38:55 | INFO | train | epoch 233 | loss 2.717 | nll_loss 0.554 | ppl 1.47 | wps 840 | ups 1.24 | wpb 678.5 | bsz 2 | num_updates 7447 | lr 2.73785e-05 | gnorm 4.484 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 6416\n",
      "2023-04-17 12:38:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:38:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:38:55 | INFO | fairseq.trainer | begin training epoch 234\n",
      "2023-04-17 12:38:55 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:39:07 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:39:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:39:08 | INFO | valid | epoch 234 | valid on 'valid' subset | loss 5.198 | nll_loss 3.326 | ppl 10.03 | wps 6199.7 | wpb 290.8 | bsz 1 | num_updates 7479 | best_loss 3.979\n",
      "2023-04-17 12:39:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 234 @ 7479 updates\n",
      "2023-04-17 12:39:08 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:39:22 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:39:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 234 @ 7479 updates, score 5.198) (writing took 14.38618858391419 seconds)\n",
      "2023-04-17 12:39:22 | INFO | fairseq_cli.train | end of epoch 234 (average epoch stats below)\n",
      "2023-04-17 12:39:22 | INFO | train | epoch 234 | loss 2.702 | nll_loss 0.535 | ppl 1.45 | wps 810.9 | ups 1.2 | wpb 678.5 | bsz 2 | num_updates 7479 | lr 2.73664e-05 | gnorm 4.756 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 6442\n",
      "2023-04-17 12:39:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:39:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:39:22 | INFO | fairseq.trainer | begin training epoch 235\n",
      "2023-04-17 12:39:22 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:39:30 | INFO | train_inner | epoch 235:     21 / 32 loss=2.707, nll_loss=0.543, ppl=1.46, wps=849.7, ups=1.25, wpb=681, bsz=2, num_updates=7500, lr=2.73585e-05, gnorm=4.58, clip=100, loss_scale=0.25, train_wall=38, gb_free=13.9, wall=6450\n",
      "2023-04-17 12:39:34 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:39:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:39:34 | INFO | valid | epoch 235 | valid on 'valid' subset | loss 5.14 | nll_loss 3.264 | ppl 9.61 | wps 6538.8 | wpb 290.8 | bsz 1 | num_updates 7511 | best_loss 3.979\n",
      "2023-04-17 12:39:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 235 @ 7511 updates\n",
      "2023-04-17 12:39:34 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 12:39:47 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:39:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 235 @ 7511 updates, score 5.14) (writing took 12.745186923071742 seconds)\n",
      "2023-04-17 12:39:47 | INFO | fairseq_cli.train | end of epoch 235 (average epoch stats below)\n",
      "2023-04-17 12:39:47 | INFO | train | epoch 235 | loss 2.701 | nll_loss 0.536 | ppl 1.45 | wps 864.4 | ups 1.27 | wpb 678.5 | bsz 2 | num_updates 7511 | lr 2.73543e-05 | gnorm 4.397 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 6467\n",
      "2023-04-17 12:39:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:39:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:39:47 | INFO | fairseq.trainer | begin training epoch 236\n",
      "2023-04-17 12:39:47 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:39:59 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:39:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:39:59 | INFO | valid | epoch 236 | valid on 'valid' subset | loss 5.109 | nll_loss 3.25 | ppl 9.52 | wps 6476.9 | wpb 290.8 | bsz 1 | num_updates 7543 | best_loss 3.979\n",
      "2023-04-17 12:39:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 236 @ 7543 updates\n",
      "2023-04-17 12:39:59 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:40:12 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:40:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 236 @ 7543 updates, score 5.109) (writing took 13.068942159996368 seconds)\n",
      "2023-04-17 12:40:12 | INFO | fairseq_cli.train | end of epoch 236 (average epoch stats below)\n",
      "2023-04-17 12:40:12 | INFO | train | epoch 236 | loss 2.695 | nll_loss 0.528 | ppl 1.44 | wps 860.1 | ups 1.27 | wpb 678.5 | bsz 2 | num_updates 7543 | lr 2.73423e-05 | gnorm 4.425 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 6493\n",
      "2023-04-17 12:40:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:40:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:40:12 | INFO | fairseq.trainer | begin training epoch 237\n",
      "2023-04-17 12:40:12 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:40:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:40:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:40:25 | INFO | valid | epoch 237 | valid on 'valid' subset | loss 5.199 | nll_loss 3.326 | ppl 10.03 | wps 6469.6 | wpb 290.8 | bsz 1 | num_updates 7575 | best_loss 3.979\n",
      "2023-04-17 12:40:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 237 @ 7575 updates\n",
      "2023-04-17 12:40:25 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:40:37 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:40:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 237 @ 7575 updates, score 5.199) (writing took 12.628149085096084 seconds)\n",
      "2023-04-17 12:40:38 | INFO | fairseq_cli.train | end of epoch 237 (average epoch stats below)\n",
      "2023-04-17 12:40:38 | INFO | train | epoch 237 | loss 2.692 | nll_loss 0.526 | ppl 1.44 | wps 865.3 | ups 1.28 | wpb 678.5 | bsz 2 | num_updates 7575 | lr 2.73302e-05 | gnorm 4.353 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 6518\n",
      "2023-04-17 12:40:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:40:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:40:38 | INFO | fairseq.trainer | begin training epoch 238\n",
      "2023-04-17 12:40:38 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:40:47 | INFO | train_inner | epoch 238:     25 / 32 loss=2.696, nll_loss=0.529, ppl=1.44, wps=887.2, ups=1.3, wpb=683.5, bsz=2, num_updates=7600, lr=2.73208e-05, gnorm=4.415, clip=100, loss_scale=0.25, train_wall=37, gb_free=13.9, wall=6527\n",
      "2023-04-17 12:40:50 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:40:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:40:50 | INFO | valid | epoch 238 | valid on 'valid' subset | loss 5.252 | nll_loss 3.365 | ppl 10.3 | wps 6502.2 | wpb 290.8 | bsz 1 | num_updates 7607 | best_loss 3.979\n",
      "2023-04-17 12:40:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 238 @ 7607 updates\n",
      "2023-04-17 12:40:50 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:41:03 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:41:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 238 @ 7607 updates, score 5.252) (writing took 12.90694875002373 seconds)\n",
      "2023-04-17 12:41:03 | INFO | fairseq_cli.train | end of epoch 238 (average epoch stats below)\n",
      "2023-04-17 12:41:03 | INFO | train | epoch 238 | loss 2.692 | nll_loss 0.528 | ppl 1.44 | wps 857.3 | ups 1.26 | wpb 678.5 | bsz 2 | num_updates 7607 | lr 2.73181e-05 | gnorm 4.459 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 6543\n",
      "2023-04-17 12:41:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:41:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:41:03 | INFO | fairseq.trainer | begin training epoch 239\n",
      "2023-04-17 12:41:03 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:41:15 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:41:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:41:15 | INFO | valid | epoch 239 | valid on 'valid' subset | loss 5.246 | nll_loss 3.415 | ppl 10.67 | wps 6334.4 | wpb 290.8 | bsz 1 | num_updates 7639 | best_loss 3.979\n",
      "2023-04-17 12:41:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 239 @ 7639 updates\n",
      "2023-04-17 12:41:15 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:41:28 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:41:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 239 @ 7639 updates, score 5.246) (writing took 12.547330335946754 seconds)\n",
      "2023-04-17 12:41:28 | INFO | fairseq_cli.train | end of epoch 239 (average epoch stats below)\n",
      "2023-04-17 12:41:28 | INFO | train | epoch 239 | loss 2.691 | nll_loss 0.524 | ppl 1.44 | wps 868 | ups 1.28 | wpb 678.5 | bsz 2 | num_updates 7639 | lr 2.7306e-05 | gnorm 4.379 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 6568\n",
      "2023-04-17 12:41:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:41:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:41:28 | INFO | fairseq.trainer | begin training epoch 240\n",
      "2023-04-17 12:41:28 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:41:40 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:41:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:41:40 | INFO | valid | epoch 240 | valid on 'valid' subset | loss 5.205 | nll_loss 3.344 | ppl 10.15 | wps 6550.4 | wpb 290.8 | bsz 1 | num_updates 7671 | best_loss 3.979\n",
      "2023-04-17 12:41:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 240 @ 7671 updates\n",
      "2023-04-17 12:41:40 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:41:53 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 12:41:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 240 @ 7671 updates, score 5.205) (writing took 12.875251732999459 seconds)\n",
      "2023-04-17 12:41:53 | INFO | fairseq_cli.train | end of epoch 240 (average epoch stats below)\n",
      "2023-04-17 12:41:53 | INFO | train | epoch 240 | loss 2.695 | nll_loss 0.531 | ppl 1.45 | wps 857.3 | ups 1.26 | wpb 678.5 | bsz 2 | num_updates 7671 | lr 2.7294e-05 | gnorm 4.6 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 6593\n",
      "2023-04-17 12:41:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:41:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:41:53 | INFO | fairseq.trainer | begin training epoch 241\n",
      "2023-04-17 12:41:53 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:42:04 | INFO | train_inner | epoch 241:     29 / 32 loss=2.692, nll_loss=0.528, ppl=1.44, wps=880.4, ups=1.29, wpb=680.1, bsz=2, num_updates=7700, lr=2.7283e-05, gnorm=4.507, clip=100, loss_scale=0.25, train_wall=38, gb_free=13.9, wall=6605\n",
      "2023-04-17 12:42:05 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:42:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:42:06 | INFO | valid | epoch 241 | valid on 'valid' subset | loss 5.23 | nll_loss 3.364 | ppl 10.3 | wps 6237.9 | wpb 290.8 | bsz 1 | num_updates 7703 | best_loss 3.979\n",
      "2023-04-17 12:42:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 241 @ 7703 updates\n",
      "2023-04-17 12:42:06 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:42:18 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:42:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 241 @ 7703 updates, score 5.23) (writing took 12.761278144083917 seconds)\n",
      "2023-04-17 12:42:18 | INFO | fairseq_cli.train | end of epoch 241 (average epoch stats below)\n",
      "2023-04-17 12:42:18 | INFO | train | epoch 241 | loss 2.69 | nll_loss 0.526 | ppl 1.44 | wps 858.1 | ups 1.26 | wpb 678.5 | bsz 2 | num_updates 7703 | lr 2.72819e-05 | gnorm 4.652 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 6619\n",
      "2023-04-17 12:42:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:42:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:42:18 | INFO | fairseq.trainer | begin training epoch 242\n",
      "2023-04-17 12:42:18 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:42:31 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:42:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:42:31 | INFO | valid | epoch 242 | valid on 'valid' subset | loss 5.167 | nll_loss 3.301 | ppl 9.86 | wps 6456.6 | wpb 290.8 | bsz 1 | num_updates 7735 | best_loss 3.979\n",
      "2023-04-17 12:42:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 242 @ 7735 updates\n",
      "2023-04-17 12:42:31 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:42:44 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:42:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 242 @ 7735 updates, score 5.167) (writing took 12.89327214693185 seconds)\n",
      "2023-04-17 12:42:44 | INFO | fairseq_cli.train | end of epoch 242 (average epoch stats below)\n",
      "2023-04-17 12:42:44 | INFO | train | epoch 242 | loss 2.68 | nll_loss 0.514 | ppl 1.43 | wps 847.8 | ups 1.25 | wpb 678.5 | bsz 2 | num_updates 7735 | lr 2.72698e-05 | gnorm 4.283 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 6644\n",
      "2023-04-17 12:42:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:42:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:42:44 | INFO | fairseq.trainer | begin training epoch 243\n",
      "2023-04-17 12:42:44 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:42:57 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:42:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:42:57 | INFO | valid | epoch 243 | valid on 'valid' subset | loss 5.211 | nll_loss 3.348 | ppl 10.19 | wps 6337.5 | wpb 290.8 | bsz 1 | num_updates 7767 | best_loss 3.979\n",
      "2023-04-17 12:42:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 243 @ 7767 updates\n",
      "2023-04-17 12:42:57 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:43:10 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:43:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 243 @ 7767 updates, score 5.211) (writing took 12.708296829019673 seconds)\n",
      "2023-04-17 12:43:10 | INFO | fairseq_cli.train | end of epoch 243 (average epoch stats below)\n",
      "2023-04-17 12:43:10 | INFO | train | epoch 243 | loss 2.676 | nll_loss 0.509 | ppl 1.42 | wps 837.8 | ups 1.23 | wpb 678.5 | bsz 2 | num_updates 7767 | lr 2.72577e-05 | gnorm 4.196 | clip 100 | loss_scale 0.25 | train_wall 13 | gb_free 13.9 | wall 6670\n",
      "2023-04-17 12:43:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:43:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:43:10 | INFO | fairseq.trainer | begin training epoch 244\n",
      "2023-04-17 12:43:10 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:43:22 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:43:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:43:22 | INFO | valid | epoch 244 | valid on 'valid' subset | loss 5.189 | nll_loss 3.341 | ppl 10.13 | wps 7262.9 | wpb 290.8 | bsz 1 | num_updates 7799 | best_loss 3.979\n",
      "2023-04-17 12:43:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 244 @ 7799 updates\n",
      "2023-04-17 12:43:22 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:43:34 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:43:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 244 @ 7799 updates, score 5.189) (writing took 12.486944518983364 seconds)\n",
      "2023-04-17 12:43:34 | INFO | fairseq_cli.train | end of epoch 244 (average epoch stats below)\n",
      "2023-04-17 12:43:34 | INFO | train | epoch 244 | loss 2.676 | nll_loss 0.51 | ppl 1.42 | wps 888.1 | ups 1.31 | wpb 678.5 | bsz 2 | num_updates 7799 | lr 2.72457e-05 | gnorm 4.29 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 6695\n",
      "2023-04-17 12:43:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:43:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:43:34 | INFO | fairseq.trainer | begin training epoch 245\n",
      "2023-04-17 12:43:34 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:43:35 | INFO | train_inner | epoch 245:      1 / 32 loss=2.677, nll_loss=0.511, ppl=1.43, wps=747.6, ups=1.11, wpb=676.1, bsz=2, num_updates=7800, lr=2.72453e-05, gnorm=4.29, clip=100, loss_scale=0.25, train_wall=38, gb_free=13.9, wall=6695\n",
      "2023-04-17 12:43:46 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:43:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:43:47 | INFO | valid | epoch 245 | valid on 'valid' subset | loss 5.186 | nll_loss 3.333 | ppl 10.07 | wps 5577.6 | wpb 290.8 | bsz 1 | num_updates 7831 | best_loss 3.979\n",
      "2023-04-17 12:43:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 245 @ 7831 updates\n",
      "2023-04-17 12:43:47 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 12:44:00 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:44:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 245 @ 7831 updates, score 5.186) (writing took 14.119736231979914 seconds)\n",
      "2023-04-17 12:44:01 | INFO | fairseq_cli.train | end of epoch 245 (average epoch stats below)\n",
      "2023-04-17 12:44:01 | INFO | train | epoch 245 | loss 2.673 | nll_loss 0.507 | ppl 1.42 | wps 827.6 | ups 1.22 | wpb 678.5 | bsz 2 | num_updates 7831 | lr 2.72336e-05 | gnorm 4.285 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 6721\n",
      "2023-04-17 12:44:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:44:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:44:01 | INFO | fairseq.trainer | begin training epoch 246\n",
      "2023-04-17 12:44:01 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:44:13 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:44:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:44:13 | INFO | valid | epoch 246 | valid on 'valid' subset | loss 5.281 | nll_loss 3.436 | ppl 10.82 | wps 6541.8 | wpb 290.8 | bsz 1 | num_updates 7863 | best_loss 3.979\n",
      "2023-04-17 12:44:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 246 @ 7863 updates\n",
      "2023-04-17 12:44:13 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:44:25 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:44:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 246 @ 7863 updates, score 5.281) (writing took 12.705398258985952 seconds)\n",
      "2023-04-17 12:44:26 | INFO | fairseq_cli.train | end of epoch 246 (average epoch stats below)\n",
      "2023-04-17 12:44:26 | INFO | train | epoch 246 | loss 2.672 | nll_loss 0.505 | ppl 1.42 | wps 868 | ups 1.28 | wpb 678.5 | bsz 2 | num_updates 7863 | lr 2.72215e-05 | gnorm 4.499 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 6746\n",
      "2023-04-17 12:44:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:44:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:44:26 | INFO | fairseq.trainer | begin training epoch 247\n",
      "2023-04-17 12:44:26 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:44:37 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:44:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:44:38 | INFO | valid | epoch 247 | valid on 'valid' subset | loss 5.282 | nll_loss 3.433 | ppl 10.8 | wps 6775.3 | wpb 290.8 | bsz 1 | num_updates 7895 | best_loss 3.979\n",
      "2023-04-17 12:44:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 247 @ 7895 updates\n",
      "2023-04-17 12:44:38 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:44:51 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:44:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 247 @ 7895 updates, score 5.282) (writing took 13.169750650995411 seconds)\n",
      "2023-04-17 12:44:51 | INFO | fairseq_cli.train | end of epoch 247 (average epoch stats below)\n",
      "2023-04-17 12:44:51 | INFO | train | epoch 247 | loss 2.667 | nll_loss 0.502 | ppl 1.42 | wps 863.8 | ups 1.27 | wpb 678.5 | bsz 2 | num_updates 7895 | lr 2.72094e-05 | gnorm 4.291 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 6771\n",
      "2023-04-17 12:44:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:44:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:44:51 | INFO | fairseq.trainer | begin training epoch 248\n",
      "2023-04-17 12:44:51 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:44:53 | INFO | train_inner | epoch 248:      5 / 32 loss=2.669, nll_loss=0.502, ppl=1.42, wps=870.6, ups=1.28, wpb=678.1, bsz=2, num_updates=7900, lr=2.72075e-05, gnorm=4.337, clip=100, loss_scale=0.25, train_wall=37, gb_free=13.9, wall=6773\n",
      "2023-04-17 12:45:03 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:45:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:45:03 | INFO | valid | epoch 248 | valid on 'valid' subset | loss 5.285 | nll_loss 3.459 | ppl 10.99 | wps 6736.4 | wpb 290.8 | bsz 1 | num_updates 7927 | best_loss 3.979\n",
      "2023-04-17 12:45:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 248 @ 7927 updates\n",
      "2023-04-17 12:45:03 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:45:16 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:45:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 248 @ 7927 updates, score 5.285) (writing took 13.078906326089054 seconds)\n",
      "2023-04-17 12:45:16 | INFO | fairseq_cli.train | end of epoch 248 (average epoch stats below)\n",
      "2023-04-17 12:45:16 | INFO | train | epoch 248 | loss 2.659 | nll_loss 0.492 | ppl 1.41 | wps 866.3 | ups 1.28 | wpb 678.5 | bsz 2 | num_updates 7927 | lr 2.71974e-05 | gnorm 4.391 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 6796\n",
      "2023-04-17 12:45:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:45:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:45:16 | INFO | fairseq.trainer | begin training epoch 249\n",
      "2023-04-17 12:45:16 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:45:29 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:45:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:45:29 | INFO | valid | epoch 249 | valid on 'valid' subset | loss 5.212 | nll_loss 3.358 | ppl 10.25 | wps 6165.8 | wpb 290.8 | bsz 1 | num_updates 7959 | best_loss 3.979\n",
      "2023-04-17 12:45:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 249 @ 7959 updates\n",
      "2023-04-17 12:45:29 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:45:42 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:45:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 249 @ 7959 updates, score 5.212) (writing took 13.746200270019472 seconds)\n",
      "2023-04-17 12:45:43 | INFO | fairseq_cli.train | end of epoch 249 (average epoch stats below)\n",
      "2023-04-17 12:45:43 | INFO | train | epoch 249 | loss 2.679 | nll_loss 0.513 | ppl 1.43 | wps 810.7 | ups 1.19 | wpb 678.5 | bsz 2 | num_updates 7959 | lr 2.71853e-05 | gnorm 5.61 | clip 100 | loss_scale 0.25 | train_wall 13 | gb_free 13.9 | wall 6823\n",
      "2023-04-17 12:45:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:45:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:45:43 | INFO | fairseq.trainer | begin training epoch 250\n",
      "2023-04-17 12:45:43 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:45:56 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:45:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:45:56 | INFO | valid | epoch 250 | valid on 'valid' subset | loss 5.23 | nll_loss 3.381 | ppl 10.42 | wps 6342.7 | wpb 290.8 | bsz 1 | num_updates 7991 | best_loss 3.979\n",
      "2023-04-17 12:45:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 250 @ 7991 updates\n",
      "2023-04-17 12:45:56 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:46:10 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 12:46:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 250 @ 7991 updates, score 5.23) (writing took 13.668774861027487 seconds)\n",
      "2023-04-17 12:46:10 | INFO | fairseq_cli.train | end of epoch 250 (average epoch stats below)\n",
      "2023-04-17 12:46:10 | INFO | train | epoch 250 | loss 2.66 | nll_loss 0.494 | ppl 1.41 | wps 802.5 | ups 1.18 | wpb 678.5 | bsz 2 | num_updates 7991 | lr 2.71732e-05 | gnorm 4.225 | clip 100 | loss_scale 0.25 | train_wall 13 | gb_free 13.9 | wall 6850\n",
      "2023-04-17 12:46:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:46:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:46:10 | INFO | fairseq.trainer | begin training epoch 251\n",
      "2023-04-17 12:46:10 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:46:14 | INFO | train_inner | epoch 251:      9 / 32 loss=2.668, nll_loss=0.502, ppl=1.42, wps=843.9, ups=1.24, wpb=681.9, bsz=2, num_updates=8000, lr=2.71698e-05, gnorm=4.717, clip=100, loss_scale=0.25, train_wall=39, gb_free=13.9, wall=6854\n",
      "2023-04-17 12:46:22 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:46:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:46:22 | INFO | valid | epoch 251 | valid on 'valid' subset | loss 5.199 | nll_loss 3.343 | ppl 10.15 | wps 6364.2 | wpb 290.8 | bsz 1 | num_updates 8023 | best_loss 3.979\n",
      "2023-04-17 12:46:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 251 @ 8023 updates\n",
      "2023-04-17 12:46:22 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:46:35 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:46:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 251 @ 8023 updates, score 5.199) (writing took 13.045261602965184 seconds)\n",
      "2023-04-17 12:46:36 | INFO | fairseq_cli.train | end of epoch 251 (average epoch stats below)\n",
      "2023-04-17 12:46:36 | INFO | train | epoch 251 | loss 2.664 | nll_loss 0.499 | ppl 1.41 | wps 842.4 | ups 1.24 | wpb 678.5 | bsz 2 | num_updates 8023 | lr 2.71611e-05 | gnorm 4.202 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 6876\n",
      "2023-04-17 12:46:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:46:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:46:36 | INFO | fairseq.trainer | begin training epoch 252\n",
      "2023-04-17 12:46:36 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:46:48 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:46:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:46:48 | INFO | valid | epoch 252 | valid on 'valid' subset | loss 5.241 | nll_loss 3.399 | ppl 10.55 | wps 4992.9 | wpb 290.8 | bsz 1 | num_updates 8055 | best_loss 3.979\n",
      "2023-04-17 12:46:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 252 @ 8055 updates\n",
      "2023-04-17 12:46:48 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:47:02 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:47:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 252 @ 8055 updates, score 5.241) (writing took 14.349207100924104 seconds)\n",
      "2023-04-17 12:47:03 | INFO | fairseq_cli.train | end of epoch 252 (average epoch stats below)\n",
      "2023-04-17 12:47:03 | INFO | train | epoch 252 | loss 2.66 | nll_loss 0.493 | ppl 1.41 | wps 798.4 | ups 1.18 | wpb 678.5 | bsz 2 | num_updates 8055 | lr 2.71491e-05 | gnorm 4.361 | clip 100 | loss_scale 0.25 | train_wall 12 | gb_free 13.9 | wall 6903\n",
      "2023-04-17 12:47:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:47:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:47:03 | INFO | fairseq.trainer | begin training epoch 253\n",
      "2023-04-17 12:47:03 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:47:16 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:47:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:47:17 | INFO | valid | epoch 253 | valid on 'valid' subset | loss 5.269 | nll_loss 3.415 | ppl 10.67 | wps 5090.3 | wpb 290.8 | bsz 1 | num_updates 8087 | best_loss 3.979\n",
      "2023-04-17 12:47:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 253 @ 8087 updates\n",
      "2023-04-17 12:47:17 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:47:31 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:47:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 253 @ 8087 updates, score 5.269) (writing took 14.839644529041834 seconds)\n",
      "2023-04-17 12:47:31 | INFO | fairseq_cli.train | end of epoch 253 (average epoch stats below)\n",
      "2023-04-17 12:47:31 | INFO | train | epoch 253 | loss 2.65 | nll_loss 0.482 | ppl 1.4 | wps 754.9 | ups 1.11 | wpb 678.5 | bsz 2 | num_updates 8087 | lr 2.7137e-05 | gnorm 4.122 | clip 100 | loss_scale 0.25 | train_wall 13 | gb_free 13.9 | wall 6932\n",
      "2023-04-17 12:47:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:47:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:47:31 | INFO | fairseq.trainer | begin training epoch 254\n",
      "2023-04-17 12:47:31 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:47:37 | INFO | train_inner | epoch 254:     13 / 32 loss=2.657, nll_loss=0.49, ppl=1.4, wps=811.4, ups=1.19, wpb=679.8, bsz=2, num_updates=8100, lr=2.71321e-05, gnorm=4.247, clip=100, loss_scale=0.25, train_wall=40, gb_free=13.9, wall=6938\n",
      "2023-04-17 12:47:45 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:47:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:47:46 | INFO | valid | epoch 254 | valid on 'valid' subset | loss 5.26 | nll_loss 3.397 | ppl 10.53 | wps 5235.4 | wpb 290.8 | bsz 1 | num_updates 8119 | best_loss 3.979\n",
      "2023-04-17 12:47:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 254 @ 8119 updates\n",
      "2023-04-17 12:47:46 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:47:59 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:48:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 254 @ 8119 updates, score 5.26) (writing took 13.850322043988854 seconds)\n",
      "2023-04-17 12:48:00 | INFO | fairseq_cli.train | end of epoch 254 (average epoch stats below)\n",
      "2023-04-17 12:48:00 | INFO | train | epoch 254 | loss 2.652 | nll_loss 0.487 | ppl 1.4 | wps 774.8 | ups 1.14 | wpb 678.5 | bsz 2 | num_updates 8119 | lr 2.71249e-05 | gnorm 4.356 | clip 100 | loss_scale 0.25 | train_wall 14 | gb_free 13.9 | wall 6960\n",
      "2023-04-17 12:48:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:48:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:48:00 | INFO | fairseq.trainer | begin training epoch 255\n",
      "2023-04-17 12:48:00 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:48:13 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:48:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:48:13 | INFO | valid | epoch 255 | valid on 'valid' subset | loss 5.245 | nll_loss 3.401 | ppl 10.56 | wps 6346.3 | wpb 290.8 | bsz 1 | num_updates 8151 | best_loss 3.979\n",
      "2023-04-17 12:48:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 255 @ 8151 updates\n",
      "2023-04-17 12:48:13 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 12:48:27 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:48:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 255 @ 8151 updates, score 5.245) (writing took 14.484579246956855 seconds)\n",
      "2023-04-17 12:48:28 | INFO | fairseq_cli.train | end of epoch 255 (average epoch stats below)\n",
      "2023-04-17 12:48:28 | INFO | train | epoch 255 | loss 2.645 | nll_loss 0.477 | ppl 1.39 | wps 771.9 | ups 1.14 | wpb 678.5 | bsz 2 | num_updates 8151 | lr 2.71128e-05 | gnorm 4.269 | clip 100 | loss_scale 0.25 | train_wall 13 | gb_free 13.9 | wall 6988\n",
      "2023-04-17 12:48:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:48:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:48:28 | INFO | fairseq.trainer | begin training epoch 256\n",
      "2023-04-17 12:48:28 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:48:41 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:48:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:48:41 | INFO | valid | epoch 256 | valid on 'valid' subset | loss 5.216 | nll_loss 3.367 | ppl 10.32 | wps 5211.4 | wpb 290.8 | bsz 1 | num_updates 8183 | best_loss 3.979\n",
      "2023-04-17 12:48:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 256 @ 8183 updates\n",
      "2023-04-17 12:48:41 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:48:54 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:48:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 256 @ 8183 updates, score 5.216) (writing took 13.677107254043221 seconds)\n",
      "2023-04-17 12:48:55 | INFO | fairseq_cli.train | end of epoch 256 (average epoch stats below)\n",
      "2023-04-17 12:48:55 | INFO | train | epoch 256 | loss 2.653 | nll_loss 0.486 | ppl 1.4 | wps 804.9 | ups 1.19 | wpb 678.5 | bsz 2 | num_updates 8183 | lr 2.71008e-05 | gnorm 4.652 | clip 100 | loss_scale 0.25 | train_wall 13 | gb_free 13.9 | wall 7015\n",
      "2023-04-17 12:48:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:48:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:48:55 | INFO | fairseq.trainer | begin training epoch 257\n",
      "2023-04-17 12:48:55 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:49:02 | INFO | train_inner | epoch 257:     17 / 32 loss=2.648, nll_loss=0.481, ppl=1.4, wps=795.1, ups=1.19, wpb=670, bsz=2, num_updates=8200, lr=2.70943e-05, gnorm=4.462, clip=100, loss_scale=0.25, train_wall=41, gb_free=13.9, wall=7022\n",
      "2023-04-17 12:49:08 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:49:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:49:08 | INFO | valid | epoch 257 | valid on 'valid' subset | loss 5.226 | nll_loss 3.393 | ppl 10.5 | wps 5250.5 | wpb 290.8 | bsz 1 | num_updates 8215 | best_loss 3.979\n",
      "2023-04-17 12:49:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 257 @ 8215 updates\n",
      "2023-04-17 12:49:08 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:49:22 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:49:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 257 @ 8215 updates, score 5.226) (writing took 13.962882625986822 seconds)\n",
      "2023-04-17 12:49:22 | INFO | fairseq_cli.train | end of epoch 257 (average epoch stats below)\n",
      "2023-04-17 12:49:22 | INFO | train | epoch 257 | loss 2.64 | nll_loss 0.472 | ppl 1.39 | wps 797.5 | ups 1.18 | wpb 678.5 | bsz 2 | num_updates 8215 | lr 2.70887e-05 | gnorm 4.408 | clip 100 | loss_scale 0.5 | train_wall 13 | gb_free 13.9 | wall 7042\n",
      "2023-04-17 12:49:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:49:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:49:22 | INFO | fairseq.trainer | begin training epoch 258\n",
      "2023-04-17 12:49:22 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:49:35 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:49:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:49:35 | INFO | valid | epoch 258 | valid on 'valid' subset | loss 5.244 | nll_loss 3.404 | ppl 10.58 | wps 6430 | wpb 290.8 | bsz 1 | num_updates 8247 | best_loss 3.979\n",
      "2023-04-17 12:49:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 258 @ 8247 updates\n",
      "2023-04-17 12:49:35 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:49:48 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:49:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 258 @ 8247 updates, score 5.244) (writing took 13.592836009920575 seconds)\n",
      "2023-04-17 12:49:49 | INFO | fairseq_cli.train | end of epoch 258 (average epoch stats below)\n",
      "2023-04-17 12:49:49 | INFO | train | epoch 258 | loss 2.644 | nll_loss 0.477 | ppl 1.39 | wps 810.9 | ups 1.2 | wpb 678.5 | bsz 2 | num_updates 8247 | lr 2.70766e-05 | gnorm 4.31 | clip 100 | loss_scale 0.5 | train_wall 13 | gb_free 13.9 | wall 7069\n",
      "2023-04-17 12:49:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:49:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:49:49 | INFO | fairseq.trainer | begin training epoch 259\n",
      "2023-04-17 12:49:49 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:50:02 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:50:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:50:02 | INFO | valid | epoch 259 | valid on 'valid' subset | loss 5.23 | nll_loss 3.388 | ppl 10.47 | wps 6032.1 | wpb 290.8 | bsz 1 | num_updates 8279 | best_loss 3.979\n",
      "2023-04-17 12:50:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 259 @ 8279 updates\n",
      "2023-04-17 12:50:02 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:50:16 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:50:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 259 @ 8279 updates, score 5.23) (writing took 13.915818625013344 seconds)\n",
      "2023-04-17 12:50:16 | INFO | fairseq_cli.train | end of epoch 259 (average epoch stats below)\n",
      "2023-04-17 12:50:16 | INFO | train | epoch 259 | loss 2.639 | nll_loss 0.47 | ppl 1.38 | wps 788.1 | ups 1.16 | wpb 678.5 | bsz 2 | num_updates 8279 | lr 2.70645e-05 | gnorm 4.227 | clip 100 | loss_scale 0.5 | train_wall 13 | gb_free 13.9 | wall 7096\n",
      "2023-04-17 12:50:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:50:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:50:16 | INFO | fairseq.trainer | begin training epoch 260\n",
      "2023-04-17 12:50:16 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:50:24 | INFO | train_inner | epoch 260:     21 / 32 loss=2.637, nll_loss=0.469, ppl=1.38, wps=814, ups=1.21, wpb=674, bsz=2, num_updates=8300, lr=2.70566e-05, gnorm=4.265, clip=100, loss_scale=0.5, train_wall=40, gb_free=13.9, wall=7105\n",
      "2023-04-17 12:50:29 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:50:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:50:29 | INFO | valid | epoch 260 | valid on 'valid' subset | loss 5.302 | nll_loss 3.449 | ppl 10.92 | wps 5717.8 | wpb 290.8 | bsz 1 | num_updates 8311 | best_loss 3.979\n",
      "2023-04-17 12:50:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 260 @ 8311 updates\n",
      "2023-04-17 12:50:29 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 12:50:41 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:50:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 260 @ 8311 updates, score 5.302) (writing took 12.855121292057447 seconds)\n",
      "2023-04-17 12:50:42 | INFO | fairseq_cli.train | end of epoch 260 (average epoch stats below)\n",
      "2023-04-17 12:50:42 | INFO | train | epoch 260 | loss 2.626 | nll_loss 0.459 | ppl 1.37 | wps 851 | ups 1.25 | wpb 678.5 | bsz 2 | num_updates 8311 | lr 2.70525e-05 | gnorm 4.151 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 7122\n",
      "2023-04-17 12:50:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:50:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:50:42 | INFO | fairseq.trainer | begin training epoch 261\n",
      "2023-04-17 12:50:42 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:50:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:50:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:50:55 | INFO | valid | epoch 261 | valid on 'valid' subset | loss 5.228 | nll_loss 3.392 | ppl 10.49 | wps 5662.3 | wpb 290.8 | bsz 1 | num_updates 8343 | best_loss 3.979\n",
      "2023-04-17 12:50:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 261 @ 8343 updates\n",
      "2023-04-17 12:50:55 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:51:08 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:51:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 261 @ 8343 updates, score 5.228) (writing took 13.337850266019814 seconds)\n",
      "2023-04-17 12:51:08 | INFO | fairseq_cli.train | end of epoch 261 (average epoch stats below)\n",
      "2023-04-17 12:51:08 | INFO | train | epoch 261 | loss 2.63 | nll_loss 0.462 | ppl 1.38 | wps 817.3 | ups 1.2 | wpb 678.5 | bsz 2 | num_updates 8343 | lr 2.70404e-05 | gnorm 4.077 | clip 100 | loss_scale 0.5 | train_wall 13 | gb_free 13.9 | wall 7149\n",
      "2023-04-17 12:51:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:51:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:51:08 | INFO | fairseq.trainer | begin training epoch 262\n",
      "2023-04-17 12:51:08 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:51:21 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:51:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:51:22 | INFO | valid | epoch 262 | valid on 'valid' subset | loss 5.273 | nll_loss 3.441 | ppl 10.86 | wps 5192.3 | wpb 290.8 | bsz 1 | num_updates 8375 | best_loss 3.979\n",
      "2023-04-17 12:51:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 262 @ 8375 updates\n",
      "2023-04-17 12:51:22 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:51:35 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:51:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 262 @ 8375 updates, score 5.273) (writing took 13.555088619934395 seconds)\n",
      "2023-04-17 12:51:35 | INFO | fairseq_cli.train | end of epoch 262 (average epoch stats below)\n",
      "2023-04-17 12:51:35 | INFO | train | epoch 262 | loss 2.628 | nll_loss 0.459 | ppl 1.37 | wps 809.1 | ups 1.19 | wpb 678.5 | bsz 2 | num_updates 8375 | lr 2.70283e-05 | gnorm 4.196 | clip 100 | loss_scale 0.5 | train_wall 13 | gb_free 13.9 | wall 7175\n",
      "2023-04-17 12:51:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:51:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:51:35 | INFO | fairseq.trainer | begin training epoch 263\n",
      "2023-04-17 12:51:35 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:51:46 | INFO | train_inner | epoch 263:     25 / 32 loss=2.627, nll_loss=0.459, ppl=1.37, wps=842.7, ups=1.23, wpb=687.9, bsz=2, num_updates=8400, lr=2.70189e-05, gnorm=4.116, clip=100, loss_scale=0.5, train_wall=40, gb_free=13.9, wall=7186\n",
      "2023-04-17 12:51:49 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:51:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:51:49 | INFO | valid | epoch 263 | valid on 'valid' subset | loss 5.293 | nll_loss 3.461 | ppl 11.02 | wps 5389.8 | wpb 290.8 | bsz 1 | num_updates 8407 | best_loss 3.979\n",
      "2023-04-17 12:51:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 263 @ 8407 updates\n",
      "2023-04-17 12:51:49 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:52:04 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:52:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 263 @ 8407 updates, score 5.293) (writing took 14.919942068052478 seconds)\n",
      "2023-04-17 12:52:04 | INFO | fairseq_cli.train | end of epoch 263 (average epoch stats below)\n",
      "2023-04-17 12:52:04 | INFO | train | epoch 263 | loss 2.619 | nll_loss 0.452 | ppl 1.37 | wps 744 | ups 1.1 | wpb 678.5 | bsz 2 | num_updates 8407 | lr 2.70162e-05 | gnorm 4.158 | clip 100 | loss_scale 0.5 | train_wall 14 | gb_free 13.9 | wall 7205\n",
      "2023-04-17 12:52:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:52:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:52:04 | INFO | fairseq.trainer | begin training epoch 264\n",
      "2023-04-17 12:52:04 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:52:18 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:52:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:52:18 | INFO | valid | epoch 264 | valid on 'valid' subset | loss 5.277 | nll_loss 3.43 | ppl 10.78 | wps 5645.8 | wpb 290.8 | bsz 1 | num_updates 8439 | best_loss 3.979\n",
      "2023-04-17 12:52:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 264 @ 8439 updates\n",
      "2023-04-17 12:52:18 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:52:32 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:52:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 264 @ 8439 updates, score 5.277) (writing took 14.703342847060412 seconds)\n",
      "2023-04-17 12:52:33 | INFO | fairseq_cli.train | end of epoch 264 (average epoch stats below)\n",
      "2023-04-17 12:52:33 | INFO | train | epoch 264 | loss 2.633 | nll_loss 0.468 | ppl 1.38 | wps 759.7 | ups 1.12 | wpb 678.5 | bsz 2 | num_updates 8439 | lr 2.70042e-05 | gnorm 4.262 | clip 100 | loss_scale 0.5 | train_wall 13 | gb_free 13.9 | wall 7233\n",
      "2023-04-17 12:52:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:52:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:52:33 | INFO | fairseq.trainer | begin training epoch 265\n",
      "2023-04-17 12:52:33 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:52:47 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:52:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:52:47 | INFO | valid | epoch 265 | valid on 'valid' subset | loss 5.317 | nll_loss 3.497 | ppl 11.29 | wps 6447.8 | wpb 290.8 | bsz 1 | num_updates 8471 | best_loss 3.979\n",
      "2023-04-17 12:52:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 265 @ 8471 updates\n",
      "2023-04-17 12:52:47 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:53:01 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 12:53:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 265 @ 8471 updates, score 5.317) (writing took 14.904997979989275 seconds)\n",
      "2023-04-17 12:53:02 | INFO | fairseq_cli.train | end of epoch 265 (average epoch stats below)\n",
      "2023-04-17 12:53:02 | INFO | train | epoch 265 | loss 2.618 | nll_loss 0.448 | ppl 1.36 | wps 750.8 | ups 1.11 | wpb 678.5 | bsz 2 | num_updates 8471 | lr 2.69921e-05 | gnorm 4.078 | clip 100 | loss_scale 0.5 | train_wall 14 | gb_free 13.9 | wall 7262\n",
      "2023-04-17 12:53:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:53:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:53:02 | INFO | fairseq.trainer | begin training epoch 266\n",
      "2023-04-17 12:53:02 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:53:14 | INFO | train_inner | epoch 266:     29 / 32 loss=2.624, nll_loss=0.457, ppl=1.37, wps=774.9, ups=1.14, wpb=679.2, bsz=2, num_updates=8500, lr=2.69811e-05, gnorm=4.182, clip=100, loss_scale=0.5, train_wall=42, gb_free=13.9, wall=7274\n",
      "2023-04-17 12:53:15 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:53:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:53:15 | INFO | valid | epoch 266 | valid on 'valid' subset | loss 5.253 | nll_loss 3.417 | ppl 10.68 | wps 6117.4 | wpb 290.8 | bsz 1 | num_updates 8503 | best_loss 3.979\n",
      "2023-04-17 12:53:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 266 @ 8503 updates\n",
      "2023-04-17 12:53:15 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:53:30 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:53:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 266 @ 8503 updates, score 5.253) (writing took 15.31535858893767 seconds)\n",
      "2023-04-17 12:53:30 | INFO | fairseq_cli.train | end of epoch 266 (average epoch stats below)\n",
      "2023-04-17 12:53:30 | INFO | train | epoch 266 | loss 2.622 | nll_loss 0.455 | ppl 1.37 | wps 760.2 | ups 1.12 | wpb 678.5 | bsz 2 | num_updates 8503 | lr 2.698e-05 | gnorm 4.163 | clip 100 | loss_scale 0.5 | train_wall 13 | gb_free 13.9 | wall 7291\n",
      "2023-04-17 12:53:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:53:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:53:30 | INFO | fairseq.trainer | begin training epoch 267\n",
      "2023-04-17 12:53:30 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:53:44 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:53:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:53:44 | INFO | valid | epoch 267 | valid on 'valid' subset | loss 5.312 | nll_loss 3.473 | ppl 11.1 | wps 5877.3 | wpb 290.8 | bsz 1 | num_updates 8535 | best_loss 3.979\n",
      "2023-04-17 12:53:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 267 @ 8535 updates\n",
      "2023-04-17 12:53:44 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:53:58 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:53:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 267 @ 8535 updates, score 5.312) (writing took 14.702837817952968 seconds)\n",
      "2023-04-17 12:53:58 | INFO | fairseq_cli.train | end of epoch 267 (average epoch stats below)\n",
      "2023-04-17 12:53:58 | INFO | train | epoch 267 | loss 2.622 | nll_loss 0.456 | ppl 1.37 | wps 771.8 | ups 1.14 | wpb 678.5 | bsz 2 | num_updates 8535 | lr 2.69679e-05 | gnorm 4.149 | clip 100 | loss_scale 0.5 | train_wall 13 | gb_free 13.9 | wall 7319\n",
      "2023-04-17 12:53:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:53:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:53:58 | INFO | fairseq.trainer | begin training epoch 268\n",
      "2023-04-17 12:53:58 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:54:11 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:54:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:54:12 | INFO | valid | epoch 268 | valid on 'valid' subset | loss 5.27 | nll_loss 3.441 | ppl 10.86 | wps 6975.4 | wpb 290.8 | bsz 1 | num_updates 8567 | best_loss 3.979\n",
      "2023-04-17 12:54:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 268 @ 8567 updates\n",
      "2023-04-17 12:54:12 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:54:25 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:54:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 268 @ 8567 updates, score 5.27) (writing took 13.585094499052502 seconds)\n",
      "2023-04-17 12:54:25 | INFO | fairseq_cli.train | end of epoch 268 (average epoch stats below)\n",
      "2023-04-17 12:54:25 | INFO | train | epoch 268 | loss 2.613 | nll_loss 0.446 | ppl 1.36 | wps 812 | ups 1.2 | wpb 678.5 | bsz 2 | num_updates 8567 | lr 2.69558e-05 | gnorm 4.065 | clip 100 | loss_scale 0.5 | train_wall 13 | gb_free 13.9 | wall 7345\n",
      "2023-04-17 12:54:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:54:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:54:25 | INFO | fairseq.trainer | begin training epoch 269\n",
      "2023-04-17 12:54:25 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:54:38 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:54:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:54:38 | INFO | valid | epoch 269 | valid on 'valid' subset | loss 5.287 | nll_loss 3.442 | ppl 10.87 | wps 5809.9 | wpb 290.8 | bsz 1 | num_updates 8599 | best_loss 3.979\n",
      "2023-04-17 12:54:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 269 @ 8599 updates\n",
      "2023-04-17 12:54:38 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:54:52 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:54:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 269 @ 8599 updates, score 5.287) (writing took 13.953088706010021 seconds)\n",
      "2023-04-17 12:54:52 | INFO | fairseq_cli.train | end of epoch 269 (average epoch stats below)\n",
      "2023-04-17 12:54:52 | INFO | train | epoch 269 | loss 2.623 | nll_loss 0.455 | ppl 1.37 | wps 802.3 | ups 1.18 | wpb 678.5 | bsz 2 | num_updates 8599 | lr 2.69438e-05 | gnorm 4.55 | clip 100 | loss_scale 0.5 | train_wall 13 | gb_free 13.9 | wall 7373\n",
      "2023-04-17 12:54:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:54:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:54:52 | INFO | fairseq.trainer | begin training epoch 270\n",
      "2023-04-17 12:54:52 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:54:53 | INFO | train_inner | epoch 270:      1 / 32 loss=2.619, nll_loss=0.452, ppl=1.37, wps=683.4, ups=1.01, wpb=676.8, bsz=2, num_updates=8600, lr=2.69434e-05, gnorm=4.255, clip=100, loss_scale=0.5, train_wall=40, gb_free=13.9, wall=7373\n",
      "2023-04-17 12:55:05 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:55:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:55:05 | INFO | valid | epoch 270 | valid on 'valid' subset | loss 5.356 | nll_loss 3.537 | ppl 11.61 | wps 6139.9 | wpb 290.8 | bsz 1 | num_updates 8631 | best_loss 3.979\n",
      "2023-04-17 12:55:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 270 @ 8631 updates\n",
      "2023-04-17 12:55:05 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 12:55:19 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:55:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 270 @ 8631 updates, score 5.356) (writing took 14.04215064109303 seconds)\n",
      "2023-04-17 12:55:19 | INFO | fairseq_cli.train | end of epoch 270 (average epoch stats below)\n",
      "2023-04-17 12:55:19 | INFO | train | epoch 270 | loss 2.608 | nll_loss 0.44 | ppl 1.36 | wps 803 | ups 1.18 | wpb 678.5 | bsz 2 | num_updates 8631 | lr 2.69317e-05 | gnorm 3.921 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 7400\n",
      "2023-04-17 12:55:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:55:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:55:19 | INFO | fairseq.trainer | begin training epoch 271\n",
      "2023-04-17 12:55:19 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:55:32 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:55:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:55:33 | INFO | valid | epoch 271 | valid on 'valid' subset | loss 5.355 | nll_loss 3.532 | ppl 11.57 | wps 5699.1 | wpb 290.8 | bsz 1 | num_updates 8663 | best_loss 3.979\n",
      "2023-04-17 12:55:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 271 @ 8663 updates\n",
      "2023-04-17 12:55:33 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:55:46 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:55:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 271 @ 8663 updates, score 5.355) (writing took 13.693806850002147 seconds)\n",
      "2023-04-17 12:55:46 | INFO | fairseq_cli.train | end of epoch 271 (average epoch stats below)\n",
      "2023-04-17 12:55:46 | INFO | train | epoch 271 | loss 2.612 | nll_loss 0.445 | ppl 1.36 | wps 803.1 | ups 1.18 | wpb 678.5 | bsz 2 | num_updates 8663 | lr 2.69196e-05 | gnorm 4.069 | clip 100 | loss_scale 0.5 | train_wall 13 | gb_free 13.9 | wall 7427\n",
      "2023-04-17 12:55:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:55:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:55:46 | INFO | fairseq.trainer | begin training epoch 272\n",
      "2023-04-17 12:55:46 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:55:59 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:55:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:56:00 | INFO | valid | epoch 272 | valid on 'valid' subset | loss 5.351 | nll_loss 3.542 | ppl 11.65 | wps 5390.6 | wpb 290.8 | bsz 1 | num_updates 8695 | best_loss 3.979\n",
      "2023-04-17 12:56:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 272 @ 8695 updates\n",
      "2023-04-17 12:56:00 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:56:14 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:56:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 272 @ 8695 updates, score 5.351) (writing took 14.347065843991004 seconds)\n",
      "2023-04-17 12:56:14 | INFO | fairseq_cli.train | end of epoch 272 (average epoch stats below)\n",
      "2023-04-17 12:56:14 | INFO | train | epoch 272 | loss 2.61 | nll_loss 0.443 | ppl 1.36 | wps 786 | ups 1.16 | wpb 678.5 | bsz 2 | num_updates 8695 | lr 2.69075e-05 | gnorm 4.163 | clip 100 | loss_scale 0.5 | train_wall 13 | gb_free 13.9 | wall 7454\n",
      "2023-04-17 12:56:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:56:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:56:14 | INFO | fairseq.trainer | begin training epoch 273\n",
      "2023-04-17 12:56:14 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:56:16 | INFO | train_inner | epoch 273:      5 / 32 loss=2.61, nll_loss=0.443, ppl=1.36, wps=815, ups=1.2, wpb=678.4, bsz=2, num_updates=8700, lr=2.69057e-05, gnorm=4.083, clip=100, loss_scale=0.5, train_wall=40, gb_free=13.9, wall=7456\n",
      "2023-04-17 12:56:27 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:56:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:56:28 | INFO | valid | epoch 273 | valid on 'valid' subset | loss 5.386 | nll_loss 3.554 | ppl 11.75 | wps 4361.4 | wpb 290.8 | bsz 1 | num_updates 8727 | best_loss 3.979\n",
      "2023-04-17 12:56:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 273 @ 8727 updates\n",
      "2023-04-17 12:56:28 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:56:42 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:56:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 273 @ 8727 updates, score 5.386) (writing took 14.640525897033513 seconds)\n",
      "2023-04-17 12:56:42 | INFO | fairseq_cli.train | end of epoch 273 (average epoch stats below)\n",
      "2023-04-17 12:56:42 | INFO | train | epoch 273 | loss 2.605 | nll_loss 0.437 | ppl 1.35 | wps 766.1 | ups 1.13 | wpb 678.5 | bsz 2 | num_updates 8727 | lr 2.68955e-05 | gnorm 4.241 | clip 100 | loss_scale 0.5 | train_wall 13 | gb_free 13.9 | wall 7483\n",
      "2023-04-17 12:56:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:56:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:56:42 | INFO | fairseq.trainer | begin training epoch 274\n",
      "2023-04-17 12:56:42 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:56:56 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:56:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:56:56 | INFO | valid | epoch 274 | valid on 'valid' subset | loss 5.35 | nll_loss 3.529 | ppl 11.54 | wps 5983.7 | wpb 290.8 | bsz 1 | num_updates 8759 | best_loss 3.979\n",
      "2023-04-17 12:56:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 274 @ 8759 updates\n",
      "2023-04-17 12:56:56 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:57:10 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:57:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 274 @ 8759 updates, score 5.35) (writing took 14.304731823969632 seconds)\n",
      "2023-04-17 12:57:10 | INFO | fairseq_cli.train | end of epoch 274 (average epoch stats below)\n",
      "2023-04-17 12:57:10 | INFO | train | epoch 274 | loss 2.601 | nll_loss 0.431 | ppl 1.35 | wps 771.5 | ups 1.14 | wpb 678.5 | bsz 2 | num_updates 8759 | lr 2.68834e-05 | gnorm 4.244 | clip 100 | loss_scale 0.5 | train_wall 13 | gb_free 13.9 | wall 7511\n",
      "2023-04-17 12:57:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:57:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:57:10 | INFO | fairseq.trainer | begin training epoch 275\n",
      "2023-04-17 12:57:10 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:57:24 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:57:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:57:25 | INFO | valid | epoch 275 | valid on 'valid' subset | loss 5.381 | nll_loss 3.546 | ppl 11.68 | wps 4678.5 | wpb 290.8 | bsz 1 | num_updates 8791 | best_loss 3.979\n",
      "2023-04-17 12:57:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 275 @ 8791 updates\n",
      "2023-04-17 12:57:25 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:57:38 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 12:57:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 275 @ 8791 updates, score 5.381) (writing took 13.995088721974753 seconds)\n",
      "2023-04-17 12:57:39 | INFO | fairseq_cli.train | end of epoch 275 (average epoch stats below)\n",
      "2023-04-17 12:57:39 | INFO | train | epoch 275 | loss 2.601 | nll_loss 0.433 | ppl 1.35 | wps 773.7 | ups 1.14 | wpb 678.5 | bsz 2 | num_updates 8791 | lr 2.68713e-05 | gnorm 4.164 | clip 100 | loss_scale 0.5 | train_wall 13 | gb_free 13.9 | wall 7539\n",
      "2023-04-17 12:57:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:57:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:57:39 | INFO | fairseq.trainer | begin training epoch 276\n",
      "2023-04-17 12:57:39 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:57:42 | INFO | train_inner | epoch 276:      9 / 32 loss=2.601, nll_loss=0.432, ppl=1.35, wps=788.9, ups=1.16, wpb=680.2, bsz=2, num_updates=8800, lr=2.68679e-05, gnorm=4.154, clip=100, loss_scale=0.5, train_wall=42, gb_free=13.9, wall=7542\n",
      "2023-04-17 12:57:51 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:57:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:57:52 | INFO | valid | epoch 276 | valid on 'valid' subset | loss 5.327 | nll_loss 3.503 | ppl 11.34 | wps 5184 | wpb 290.8 | bsz 1 | num_updates 8823 | best_loss 3.979\n",
      "2023-04-17 12:57:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 276 @ 8823 updates\n",
      "2023-04-17 12:57:52 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:58:06 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:58:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 276 @ 8823 updates, score 5.327) (writing took 14.848707583034411 seconds)\n",
      "2023-04-17 12:58:07 | INFO | fairseq_cli.train | end of epoch 276 (average epoch stats below)\n",
      "2023-04-17 12:58:07 | INFO | train | epoch 276 | loss 2.598 | nll_loss 0.431 | ppl 1.35 | wps 773.2 | ups 1.14 | wpb 678.5 | bsz 2 | num_updates 8823 | lr 2.68592e-05 | gnorm 4.248 | clip 100 | loss_scale 0.5 | train_wall 13 | gb_free 13.9 | wall 7567\n",
      "2023-04-17 12:58:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:58:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:58:07 | INFO | fairseq.trainer | begin training epoch 277\n",
      "2023-04-17 12:58:07 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:58:20 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:58:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:58:20 | INFO | valid | epoch 277 | valid on 'valid' subset | loss 5.38 | nll_loss 3.542 | ppl 11.65 | wps 6445 | wpb 290.8 | bsz 1 | num_updates 8855 | best_loss 3.979\n",
      "2023-04-17 12:58:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 277 @ 8855 updates\n",
      "2023-04-17 12:58:20 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:58:33 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:58:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 277 @ 8855 updates, score 5.38) (writing took 13.632077057030983 seconds)\n",
      "2023-04-17 12:58:34 | INFO | fairseq_cli.train | end of epoch 277 (average epoch stats below)\n",
      "2023-04-17 12:58:34 | INFO | train | epoch 277 | loss 2.603 | nll_loss 0.436 | ppl 1.35 | wps 805.9 | ups 1.19 | wpb 678.5 | bsz 2 | num_updates 8855 | lr 2.68472e-05 | gnorm 4.17 | clip 100 | loss_scale 0.5 | train_wall 13 | gb_free 13.9 | wall 7594\n",
      "2023-04-17 12:58:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:58:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:58:34 | INFO | fairseq.trainer | begin training epoch 278\n",
      "2023-04-17 12:58:34 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:58:47 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:58:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:58:48 | INFO | valid | epoch 278 | valid on 'valid' subset | loss 5.393 | nll_loss 3.566 | ppl 11.85 | wps 6100.1 | wpb 290.8 | bsz 1 | num_updates 8887 | best_loss 3.979\n",
      "2023-04-17 12:58:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 278 @ 8887 updates\n",
      "2023-04-17 12:58:48 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:58:59 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:58:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 278 @ 8887 updates, score 5.393) (writing took 11.334116832935251 seconds)\n",
      "2023-04-17 12:58:59 | INFO | fairseq_cli.train | end of epoch 278 (average epoch stats below)\n",
      "2023-04-17 12:58:59 | INFO | train | epoch 278 | loss 2.6 | nll_loss 0.433 | ppl 1.35 | wps 857.4 | ups 1.26 | wpb 678.5 | bsz 2 | num_updates 8887 | lr 2.68351e-05 | gnorm 4.256 | clip 100 | loss_scale 0.5 | train_wall 13 | gb_free 13.9 | wall 7619\n",
      "2023-04-17 12:58:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:58:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:58:59 | INFO | fairseq.trainer | begin training epoch 279\n",
      "2023-04-17 12:58:59 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:59:05 | INFO | train_inner | epoch 279:     13 / 32 loss=2.6, nll_loss=0.433, ppl=1.35, wps=818.9, ups=1.21, wpb=674.1, bsz=2, num_updates=8900, lr=2.68302e-05, gnorm=4.343, clip=100, loss_scale=0.5, train_wall=41, gb_free=13.9, wall=7625\n",
      "2023-04-17 12:59:12 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:59:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:59:13 | INFO | valid | epoch 279 | valid on 'valid' subset | loss 5.372 | nll_loss 3.558 | ppl 11.78 | wps 5619.1 | wpb 290.8 | bsz 1 | num_updates 8919 | best_loss 3.979\n",
      "2023-04-17 12:59:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 279 @ 8919 updates\n",
      "2023-04-17 12:59:13 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:59:28 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:59:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 279 @ 8919 updates, score 5.372) (writing took 16.217243592021987 seconds)\n",
      "2023-04-17 12:59:29 | INFO | fairseq_cli.train | end of epoch 279 (average epoch stats below)\n",
      "2023-04-17 12:59:29 | INFO | train | epoch 279 | loss 2.602 | nll_loss 0.435 | ppl 1.35 | wps 726.9 | ups 1.07 | wpb 678.5 | bsz 2 | num_updates 8919 | lr 2.6823e-05 | gnorm 4.789 | clip 100 | loss_scale 0.5 | train_wall 13 | gb_free 13.9 | wall 7649\n",
      "2023-04-17 12:59:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:59:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:59:29 | INFO | fairseq.trainer | begin training epoch 280\n",
      "2023-04-17 12:59:29 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 12:59:41 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 12:59:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:59:42 | INFO | valid | epoch 280 | valid on 'valid' subset | loss 5.374 | nll_loss 3.562 | ppl 11.81 | wps 6278.2 | wpb 290.8 | bsz 1 | num_updates 8951 | best_loss 3.979\n",
      "2023-04-17 12:59:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 280 @ 8951 updates\n",
      "2023-04-17 12:59:42 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 12:59:57 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 12:59:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 280 @ 8951 updates, score 5.374) (writing took 15.784273740951903 seconds)\n",
      "2023-04-17 12:59:57 | INFO | fairseq_cli.train | end of epoch 280 (average epoch stats below)\n",
      "2023-04-17 12:59:57 | INFO | train | epoch 280 | loss 2.602 | nll_loss 0.434 | ppl 1.35 | wps 756.9 | ups 1.12 | wpb 678.5 | bsz 2 | num_updates 8951 | lr 2.68109e-05 | gnorm 4.405 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 7678\n",
      "2023-04-17 12:59:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 12:59:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 12:59:57 | INFO | fairseq.trainer | begin training epoch 281\n",
      "2023-04-17 12:59:57 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:00:10 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:00:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:00:11 | INFO | valid | epoch 281 | valid on 'valid' subset | loss 5.375 | nll_loss 3.546 | ppl 11.68 | wps 5403.1 | wpb 290.8 | bsz 1 | num_updates 8983 | best_loss 3.979\n",
      "2023-04-17 13:00:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 281 @ 8983 updates\n",
      "2023-04-17 13:00:11 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:00:25 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:00:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 281 @ 8983 updates, score 5.375) (writing took 14.906805451028049 seconds)\n",
      "2023-04-17 13:00:25 | INFO | fairseq_cli.train | end of epoch 281 (average epoch stats below)\n",
      "2023-04-17 13:00:25 | INFO | train | epoch 281 | loss 2.595 | nll_loss 0.429 | ppl 1.35 | wps 775.1 | ups 1.14 | wpb 678.5 | bsz 2 | num_updates 8983 | lr 2.67989e-05 | gnorm 4.06 | clip 100 | loss_scale 0.5 | train_wall 13 | gb_free 13.9 | wall 7706\n",
      "2023-04-17 13:00:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:00:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:00:25 | INFO | fairseq.trainer | begin training epoch 282\n",
      "2023-04-17 13:00:25 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:00:32 | INFO | train_inner | epoch 282:     17 / 32 loss=2.6, nll_loss=0.432, ppl=1.35, wps=774.7, ups=1.14, wpb=681.3, bsz=2, num_updates=9000, lr=2.67925e-05, gnorm=4.251, clip=100, loss_scale=0.5, train_wall=40, gb_free=13.9, wall=7713\n",
      "2023-04-17 13:00:38 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:00:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:00:39 | INFO | valid | epoch 282 | valid on 'valid' subset | loss 5.375 | nll_loss 3.556 | ppl 11.76 | wps 4780 | wpb 290.8 | bsz 1 | num_updates 9015 | best_loss 3.979\n",
      "2023-04-17 13:00:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 282 @ 9015 updates\n",
      "2023-04-17 13:00:39 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:00:53 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:00:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 282 @ 9015 updates, score 5.375) (writing took 14.830564445001073 seconds)\n",
      "2023-04-17 13:00:54 | INFO | fairseq_cli.train | end of epoch 282 (average epoch stats below)\n",
      "2023-04-17 13:00:54 | INFO | train | epoch 282 | loss 2.592 | nll_loss 0.424 | ppl 1.34 | wps 770.4 | ups 1.14 | wpb 678.5 | bsz 2 | num_updates 9015 | lr 2.67868e-05 | gnorm 3.849 | clip 100 | loss_scale 0.5 | train_wall 13 | gb_free 13.9 | wall 7734\n",
      "2023-04-17 13:00:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:00:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:00:54 | INFO | fairseq.trainer | begin training epoch 283\n",
      "2023-04-17 13:00:54 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:01:07 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:01:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:01:08 | INFO | valid | epoch 283 | valid on 'valid' subset | loss 5.425 | nll_loss 3.608 | ppl 12.19 | wps 6379.8 | wpb 290.8 | bsz 1 | num_updates 9047 | best_loss 3.979\n",
      "2023-04-17 13:01:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 283 @ 9047 updates\n",
      "2023-04-17 13:01:08 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:01:21 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:01:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 283 @ 9047 updates, score 5.425) (writing took 13.999044643016532 seconds)\n",
      "2023-04-17 13:01:22 | INFO | fairseq_cli.train | end of epoch 283 (average epoch stats below)\n",
      "2023-04-17 13:01:22 | INFO | train | epoch 283 | loss 2.585 | nll_loss 0.418 | ppl 1.34 | wps 777.7 | ups 1.15 | wpb 678.5 | bsz 2 | num_updates 9047 | lr 2.67747e-05 | gnorm 3.826 | clip 100 | loss_scale 0.5 | train_wall 13 | gb_free 13.9 | wall 7762\n",
      "2023-04-17 13:01:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:01:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:01:22 | INFO | fairseq.trainer | begin training epoch 284\n",
      "2023-04-17 13:01:22 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:01:34 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:01:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:01:35 | INFO | valid | epoch 284 | valid on 'valid' subset | loss 5.403 | nll_loss 3.587 | ppl 12.01 | wps 5536.4 | wpb 290.8 | bsz 1 | num_updates 9079 | best_loss 3.979\n",
      "2023-04-17 13:01:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 284 @ 9079 updates\n",
      "2023-04-17 13:01:35 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:01:48 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:01:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 284 @ 9079 updates, score 5.403) (writing took 13.432858598069288 seconds)\n",
      "2023-04-17 13:01:48 | INFO | fairseq_cli.train | end of epoch 284 (average epoch stats below)\n",
      "2023-04-17 13:01:48 | INFO | train | epoch 284 | loss 2.58 | nll_loss 0.413 | ppl 1.33 | wps 815.5 | ups 1.2 | wpb 678.5 | bsz 2 | num_updates 9079 | lr 2.67626e-05 | gnorm 3.8 | clip 100 | loss_scale 0.5 | train_wall 13 | gb_free 13.9 | wall 7788\n",
      "2023-04-17 13:01:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:01:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:01:48 | INFO | fairseq.trainer | begin training epoch 285\n",
      "2023-04-17 13:01:48 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:01:57 | INFO | train_inner | epoch 285:     21 / 32 loss=2.583, nll_loss=0.415, ppl=1.33, wps=802.8, ups=1.19, wpb=677.2, bsz=2, num_updates=9100, lr=2.67547e-05, gnorm=3.893, clip=100, loss_scale=0.5, train_wall=40, gb_free=13.9, wall=7797\n",
      "2023-04-17 13:02:02 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:02:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:02:02 | INFO | valid | epoch 285 | valid on 'valid' subset | loss 5.395 | nll_loss 3.573 | ppl 11.9 | wps 4726 | wpb 290.8 | bsz 1 | num_updates 9111 | best_loss 3.979\n",
      "2023-04-17 13:02:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 285 @ 9111 updates\n",
      "2023-04-17 13:02:02 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 13:02:16 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:02:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 285 @ 9111 updates, score 5.395) (writing took 14.350030235946178 seconds)\n",
      "2023-04-17 13:02:16 | INFO | fairseq_cli.train | end of epoch 285 (average epoch stats below)\n",
      "2023-04-17 13:02:16 | INFO | train | epoch 285 | loss 2.578 | nll_loss 0.41 | ppl 1.33 | wps 773.3 | ups 1.14 | wpb 678.5 | bsz 2 | num_updates 9111 | lr 2.67506e-05 | gnorm 4.048 | clip 100 | loss_scale 0.5 | train_wall 13 | gb_free 13.9 | wall 7817\n",
      "2023-04-17 13:02:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:02:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:02:16 | INFO | fairseq.trainer | begin training epoch 286\n",
      "2023-04-17 13:02:16 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:02:29 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:02:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:02:29 | INFO | valid | epoch 286 | valid on 'valid' subset | loss 5.406 | nll_loss 3.596 | ppl 12.09 | wps 5301.9 | wpb 290.8 | bsz 1 | num_updates 9143 | best_loss 3.979\n",
      "2023-04-17 13:02:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 286 @ 9143 updates\n",
      "2023-04-17 13:02:29 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:02:45 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:02:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 286 @ 9143 updates, score 5.406) (writing took 15.528451988939196 seconds)\n",
      "2023-04-17 13:02:45 | INFO | fairseq_cli.train | end of epoch 286 (average epoch stats below)\n",
      "2023-04-17 13:02:45 | INFO | train | epoch 286 | loss 2.577 | nll_loss 0.407 | ppl 1.33 | wps 756.3 | ups 1.11 | wpb 678.5 | bsz 2 | num_updates 9143 | lr 2.67385e-05 | gnorm 3.974 | clip 100 | loss_scale 0.5 | train_wall 13 | gb_free 13.9 | wall 7845\n",
      "2023-04-17 13:02:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:02:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:02:45 | INFO | fairseq.trainer | begin training epoch 287\n",
      "2023-04-17 13:02:45 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:02:58 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:02:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:02:59 | INFO | valid | epoch 287 | valid on 'valid' subset | loss 5.382 | nll_loss 3.569 | ppl 11.87 | wps 5837.8 | wpb 290.8 | bsz 1 | num_updates 9175 | best_loss 3.979\n",
      "2023-04-17 13:02:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 287 @ 9175 updates\n",
      "2023-04-17 13:02:59 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:03:12 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:03:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 287 @ 9175 updates, score 5.382) (writing took 13.27526860893704 seconds)\n",
      "2023-04-17 13:03:12 | INFO | fairseq_cli.train | end of epoch 287 (average epoch stats below)\n",
      "2023-04-17 13:03:12 | INFO | train | epoch 287 | loss 2.571 | nll_loss 0.405 | ppl 1.32 | wps 809.8 | ups 1.19 | wpb 678.5 | bsz 2 | num_updates 9175 | lr 2.67264e-05 | gnorm 3.75 | clip 100 | loss_scale 0.5 | train_wall 13 | gb_free 13.9 | wall 7872\n",
      "2023-04-17 13:03:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:03:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:03:12 | INFO | fairseq.trainer | begin training epoch 288\n",
      "2023-04-17 13:03:12 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:03:21 | INFO | train_inner | epoch 288:     25 / 32 loss=2.575, nll_loss=0.407, ppl=1.33, wps=810.1, ups=1.19, wpb=681.9, bsz=2, num_updates=9200, lr=2.6717e-05, gnorm=3.947, clip=100, loss_scale=0.5, train_wall=39, gb_free=13.9, wall=7881\n",
      "2023-04-17 13:03:23 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:03:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:03:24 | INFO | valid | epoch 288 | valid on 'valid' subset | loss 5.422 | nll_loss 3.635 | ppl 12.42 | wps 6621.1 | wpb 290.8 | bsz 1 | num_updates 9207 | best_loss 3.979\n",
      "2023-04-17 13:03:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 288 @ 9207 updates\n",
      "2023-04-17 13:03:24 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:03:36 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:03:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 288 @ 9207 updates, score 5.422) (writing took 12.56691585702356 seconds)\n",
      "2023-04-17 13:03:36 | INFO | fairseq_cli.train | end of epoch 288 (average epoch stats below)\n",
      "2023-04-17 13:03:36 | INFO | train | epoch 288 | loss 2.577 | nll_loss 0.409 | ppl 1.33 | wps 889.5 | ups 1.31 | wpb 678.5 | bsz 2 | num_updates 9207 | lr 2.67143e-05 | gnorm 4.197 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 7896\n",
      "2023-04-17 13:03:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:03:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:03:36 | INFO | fairseq.trainer | begin training epoch 289\n",
      "2023-04-17 13:03:36 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:03:48 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:03:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:03:49 | INFO | valid | epoch 289 | valid on 'valid' subset | loss 5.42 | nll_loss 3.597 | ppl 12.1 | wps 6509.3 | wpb 290.8 | bsz 1 | num_updates 9239 | best_loss 3.979\n",
      "2023-04-17 13:03:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 289 @ 9239 updates\n",
      "2023-04-17 13:03:49 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:04:01 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:04:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 289 @ 9239 updates, score 5.42) (writing took 12.838656948064454 seconds)\n",
      "2023-04-17 13:04:01 | INFO | fairseq_cli.train | end of epoch 289 (average epoch stats below)\n",
      "2023-04-17 13:04:01 | INFO | train | epoch 289 | loss 2.575 | nll_loss 0.407 | ppl 1.33 | wps 861.3 | ups 1.27 | wpb 678.5 | bsz 2 | num_updates 9239 | lr 2.67023e-05 | gnorm 4.066 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 7922\n",
      "2023-04-17 13:04:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:04:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:04:01 | INFO | fairseq.trainer | begin training epoch 290\n",
      "2023-04-17 13:04:01 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:04:14 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:04:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:04:14 | INFO | valid | epoch 290 | valid on 'valid' subset | loss 5.413 | nll_loss 3.604 | ppl 12.16 | wps 6546 | wpb 290.8 | bsz 1 | num_updates 9271 | best_loss 3.979\n",
      "2023-04-17 13:04:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 290 @ 9271 updates\n",
      "2023-04-17 13:04:14 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:04:27 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 13:04:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 290 @ 9271 updates, score 5.413) (writing took 13.337904401007108 seconds)\n",
      "2023-04-17 13:04:27 | INFO | fairseq_cli.train | end of epoch 290 (average epoch stats below)\n",
      "2023-04-17 13:04:27 | INFO | train | epoch 290 | loss 2.577 | nll_loss 0.412 | ppl 1.33 | wps 844.6 | ups 1.24 | wpb 678.5 | bsz 2 | num_updates 9271 | lr 2.66902e-05 | gnorm 4.125 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 7947\n",
      "2023-04-17 13:04:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:04:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:04:27 | INFO | fairseq.trainer | begin training epoch 291\n",
      "2023-04-17 13:04:27 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:04:38 | INFO | train_inner | epoch 291:     29 / 32 loss=2.578, nll_loss=0.411, ppl=1.33, wps=878, ups=1.29, wpb=679.2, bsz=2, num_updates=9300, lr=2.66792e-05, gnorm=4.144, clip=100, loss_scale=0.5, train_wall=37, gb_free=13.9, wall=7959\n",
      "2023-04-17 13:04:39 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:04:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:04:40 | INFO | valid | epoch 291 | valid on 'valid' subset | loss 5.357 | nll_loss 3.541 | ppl 11.64 | wps 6636.1 | wpb 290.8 | bsz 1 | num_updates 9303 | best_loss 3.979\n",
      "2023-04-17 13:04:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 291 @ 9303 updates\n",
      "2023-04-17 13:04:40 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:04:52 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:04:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 291 @ 9303 updates, score 5.357) (writing took 12.65272178594023 seconds)\n",
      "2023-04-17 13:04:52 | INFO | fairseq_cli.train | end of epoch 291 (average epoch stats below)\n",
      "2023-04-17 13:04:52 | INFO | train | epoch 291 | loss 2.58 | nll_loss 0.413 | ppl 1.33 | wps 864 | ups 1.27 | wpb 678.5 | bsz 2 | num_updates 9303 | lr 2.66781e-05 | gnorm 4.205 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 7972\n",
      "2023-04-17 13:04:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:04:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:04:52 | INFO | fairseq.trainer | begin training epoch 292\n",
      "2023-04-17 13:04:52 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:05:04 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:05:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:05:04 | INFO | valid | epoch 292 | valid on 'valid' subset | loss 5.453 | nll_loss 3.644 | ppl 12.5 | wps 6582.1 | wpb 290.8 | bsz 1 | num_updates 9335 | best_loss 3.979\n",
      "2023-04-17 13:05:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 292 @ 9335 updates\n",
      "2023-04-17 13:05:04 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:05:17 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:05:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 292 @ 9335 updates, score 5.453) (writing took 12.795068289968185 seconds)\n",
      "2023-04-17 13:05:17 | INFO | fairseq_cli.train | end of epoch 292 (average epoch stats below)\n",
      "2023-04-17 13:05:17 | INFO | train | epoch 292 | loss 2.569 | nll_loss 0.401 | ppl 1.32 | wps 876.4 | ups 1.29 | wpb 678.5 | bsz 2 | num_updates 9335 | lr 2.6666e-05 | gnorm 3.903 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 7997\n",
      "2023-04-17 13:05:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:05:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:05:17 | INFO | fairseq.trainer | begin training epoch 293\n",
      "2023-04-17 13:05:17 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:05:29 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:05:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:05:29 | INFO | valid | epoch 293 | valid on 'valid' subset | loss 5.402 | nll_loss 3.577 | ppl 11.93 | wps 6429.1 | wpb 290.8 | bsz 1 | num_updates 9367 | best_loss 3.979\n",
      "2023-04-17 13:05:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 293 @ 9367 updates\n",
      "2023-04-17 13:05:29 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:05:42 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:05:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 293 @ 9367 updates, score 5.402) (writing took 12.710122327087447 seconds)\n",
      "2023-04-17 13:05:42 | INFO | fairseq_cli.train | end of epoch 293 (average epoch stats below)\n",
      "2023-04-17 13:05:42 | INFO | train | epoch 293 | loss 2.57 | nll_loss 0.403 | ppl 1.32 | wps 864.1 | ups 1.27 | wpb 678.5 | bsz 2 | num_updates 9367 | lr 2.6654e-05 | gnorm 4.446 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 8022\n",
      "2023-04-17 13:05:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:05:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:05:42 | INFO | fairseq.trainer | begin training epoch 294\n",
      "2023-04-17 13:05:42 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:05:54 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:05:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:05:55 | INFO | valid | epoch 294 | valid on 'valid' subset | loss 5.432 | nll_loss 3.622 | ppl 12.31 | wps 6591.6 | wpb 290.8 | bsz 1 | num_updates 9399 | best_loss 3.979\n",
      "2023-04-17 13:05:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 294 @ 9399 updates\n",
      "2023-04-17 13:05:55 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:06:07 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:06:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 294 @ 9399 updates, score 5.432) (writing took 12.534477308043279 seconds)\n",
      "2023-04-17 13:06:07 | INFO | fairseq_cli.train | end of epoch 294 (average epoch stats below)\n",
      "2023-04-17 13:06:07 | INFO | train | epoch 294 | loss 2.564 | nll_loss 0.396 | ppl 1.32 | wps 871.1 | ups 1.28 | wpb 678.5 | bsz 2 | num_updates 9399 | lr 2.66419e-05 | gnorm 4.516 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 8047\n",
      "2023-04-17 13:06:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:06:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:06:07 | INFO | fairseq.trainer | begin training epoch 295\n",
      "2023-04-17 13:06:07 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:06:07 | INFO | train_inner | epoch 295:      1 / 32 loss=2.568, nll_loss=0.4, ppl=1.32, wps=760.5, ups=1.12, wpb=677.9, bsz=2, num_updates=9400, lr=2.66415e-05, gnorm=4.271, clip=100, loss_scale=0.5, train_wall=37, gb_free=13.9, wall=8048\n",
      "2023-04-17 13:06:19 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:06:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:06:20 | INFO | valid | epoch 295 | valid on 'valid' subset | loss 5.428 | nll_loss 3.62 | ppl 12.3 | wps 6569.1 | wpb 290.8 | bsz 1 | num_updates 9431 | best_loss 3.979\n",
      "2023-04-17 13:06:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 295 @ 9431 updates\n",
      "2023-04-17 13:06:20 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 13:06:32 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:06:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 295 @ 9431 updates, score 5.428) (writing took 12.649534801021218 seconds)\n",
      "2023-04-17 13:06:32 | INFO | fairseq_cli.train | end of epoch 295 (average epoch stats below)\n",
      "2023-04-17 13:06:32 | INFO | train | epoch 295 | loss 2.568 | nll_loss 0.401 | ppl 1.32 | wps 864.8 | ups 1.27 | wpb 678.5 | bsz 2 | num_updates 9431 | lr 2.66298e-05 | gnorm 4.177 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 8072\n",
      "2023-04-17 13:06:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:06:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:06:32 | INFO | fairseq.trainer | begin training epoch 296\n",
      "2023-04-17 13:06:32 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:06:44 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:06:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:06:44 | INFO | valid | epoch 296 | valid on 'valid' subset | loss 5.29 | nll_loss 3.458 | ppl 10.99 | wps 6401.8 | wpb 290.8 | bsz 1 | num_updates 9463 | best_loss 3.979\n",
      "2023-04-17 13:06:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 296 @ 9463 updates\n",
      "2023-04-17 13:06:44 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:06:57 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:06:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 296 @ 9463 updates, score 5.29) (writing took 13.061826065997593 seconds)\n",
      "2023-04-17 13:06:58 | INFO | fairseq_cli.train | end of epoch 296 (average epoch stats below)\n",
      "2023-04-17 13:06:58 | INFO | train | epoch 296 | loss 2.634 | nll_loss 0.475 | ppl 1.39 | wps 856.6 | ups 1.26 | wpb 678.5 | bsz 2 | num_updates 9463 | lr 2.66177e-05 | gnorm 6.024 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 8098\n",
      "2023-04-17 13:06:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:06:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:06:58 | INFO | fairseq.trainer | begin training epoch 297\n",
      "2023-04-17 13:06:58 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:07:09 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:07:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:07:09 | INFO | valid | epoch 297 | valid on 'valid' subset | loss 5.386 | nll_loss 3.583 | ppl 11.99 | wps 6817.3 | wpb 290.8 | bsz 1 | num_updates 9495 | best_loss 3.979\n",
      "2023-04-17 13:07:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 297 @ 9495 updates\n",
      "2023-04-17 13:07:09 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:07:22 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:07:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 297 @ 9495 updates, score 5.386) (writing took 12.607897078967653 seconds)\n",
      "2023-04-17 13:07:22 | INFO | fairseq_cli.train | end of epoch 297 (average epoch stats below)\n",
      "2023-04-17 13:07:22 | INFO | train | epoch 297 | loss 2.568 | nll_loss 0.401 | ppl 1.32 | wps 890.1 | ups 1.31 | wpb 678.5 | bsz 2 | num_updates 9495 | lr 2.66057e-05 | gnorm 3.977 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 8122\n",
      "2023-04-17 13:07:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:07:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:07:22 | INFO | fairseq.trainer | begin training epoch 298\n",
      "2023-04-17 13:07:22 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:07:24 | INFO | train_inner | epoch 298:      5 / 32 loss=2.589, nll_loss=0.425, ppl=1.34, wps=883.6, ups=1.31, wpb=674.3, bsz=2, num_updates=9500, lr=2.66038e-05, gnorm=4.701, clip=100, loss_scale=0.5, train_wall=37, gb_free=13.9, wall=8124\n",
      "2023-04-17 13:07:34 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:07:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:07:34 | INFO | valid | epoch 298 | valid on 'valid' subset | loss 5.414 | nll_loss 3.603 | ppl 12.15 | wps 6863 | wpb 290.8 | bsz 1 | num_updates 9527 | best_loss 3.979\n",
      "2023-04-17 13:07:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 298 @ 9527 updates\n",
      "2023-04-17 13:07:34 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:07:46 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:07:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 298 @ 9527 updates, score 5.414) (writing took 12.642500409972854 seconds)\n",
      "2023-04-17 13:07:47 | INFO | fairseq_cli.train | end of epoch 298 (average epoch stats below)\n",
      "2023-04-17 13:07:47 | INFO | train | epoch 298 | loss 2.564 | nll_loss 0.396 | ppl 1.32 | wps 880.8 | ups 1.3 | wpb 678.5 | bsz 2 | num_updates 9527 | lr 2.65936e-05 | gnorm 4.002 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 8147\n",
      "2023-04-17 13:07:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:07:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:07:47 | INFO | fairseq.trainer | begin training epoch 299\n",
      "2023-04-17 13:07:47 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:07:59 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:07:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:08:00 | INFO | valid | epoch 299 | valid on 'valid' subset | loss 5.475 | nll_loss 3.659 | ppl 12.63 | wps 6400.9 | wpb 290.8 | bsz 1 | num_updates 9559 | best_loss 3.979\n",
      "2023-04-17 13:08:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 299 @ 9559 updates\n",
      "2023-04-17 13:08:00 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:08:12 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:08:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 299 @ 9559 updates, score 5.475) (writing took 12.688324889051728 seconds)\n",
      "2023-04-17 13:08:12 | INFO | fairseq_cli.train | end of epoch 299 (average epoch stats below)\n",
      "2023-04-17 13:08:12 | INFO | train | epoch 299 | loss 2.562 | nll_loss 0.394 | ppl 1.31 | wps 839.6 | ups 1.24 | wpb 678.5 | bsz 2 | num_updates 9559 | lr 2.65815e-05 | gnorm 4.113 | clip 100 | loss_scale 0.5 | train_wall 13 | gb_free 13.9 | wall 8173\n",
      "2023-04-17 13:08:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:08:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:08:12 | INFO | fairseq.trainer | begin training epoch 300\n",
      "2023-04-17 13:08:12 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:08:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:08:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:08:25 | INFO | valid | epoch 300 | valid on 'valid' subset | loss 5.453 | nll_loss 3.64 | ppl 12.46 | wps 6583.9 | wpb 290.8 | bsz 1 | num_updates 9591 | best_loss 3.979\n",
      "2023-04-17 13:08:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 300 @ 9591 updates\n",
      "2023-04-17 13:08:25 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:08:37 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 13:08:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 300 @ 9591 updates, score 5.453) (writing took 12.689484875067137 seconds)\n",
      "2023-04-17 13:08:37 | INFO | fairseq_cli.train | end of epoch 300 (average epoch stats below)\n",
      "2023-04-17 13:08:37 | INFO | train | epoch 300 | loss 2.565 | nll_loss 0.399 | ppl 1.32 | wps 868.2 | ups 1.28 | wpb 678.5 | bsz 2 | num_updates 9591 | lr 2.65694e-05 | gnorm 3.957 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 8198\n",
      "2023-04-17 13:08:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:08:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:08:37 | INFO | fairseq.trainer | begin training epoch 301\n",
      "2023-04-17 13:08:37 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:08:41 | INFO | train_inner | epoch 301:      9 / 32 loss=2.564, nll_loss=0.396, ppl=1.32, wps=886.9, ups=1.3, wpb=683.8, bsz=2, num_updates=9600, lr=2.6566e-05, gnorm=4.001, clip=100, loss_scale=0.5, train_wall=38, gb_free=13.9, wall=8201\n",
      "2023-04-17 13:08:49 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:08:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:08:50 | INFO | valid | epoch 301 | valid on 'valid' subset | loss 5.535 | nll_loss 3.735 | ppl 13.31 | wps 6561 | wpb 290.8 | bsz 1 | num_updates 9623 | best_loss 3.979\n",
      "2023-04-17 13:08:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 301 @ 9623 updates\n",
      "2023-04-17 13:08:50 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:09:02 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:09:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 301 @ 9623 updates, score 5.535) (writing took 12.362697529955767 seconds)\n",
      "2023-04-17 13:09:02 | INFO | fairseq_cli.train | end of epoch 301 (average epoch stats below)\n",
      "2023-04-17 13:09:02 | INFO | train | epoch 301 | loss 2.576 | nll_loss 0.409 | ppl 1.33 | wps 883.1 | ups 1.3 | wpb 678.5 | bsz 2 | num_updates 9623 | lr 2.65574e-05 | gnorm 4.046 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 8222\n",
      "2023-04-17 13:09:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:09:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:09:02 | INFO | fairseq.trainer | begin training epoch 302\n",
      "2023-04-17 13:09:02 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:09:14 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:09:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:09:15 | INFO | valid | epoch 302 | valid on 'valid' subset | loss 5.369 | nll_loss 3.552 | ppl 11.73 | wps 6370.6 | wpb 290.8 | bsz 1 | num_updates 9655 | best_loss 3.979\n",
      "2023-04-17 13:09:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 302 @ 9655 updates\n",
      "2023-04-17 13:09:15 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:09:27 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:09:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 302 @ 9655 updates, score 5.369) (writing took 13.050305305980146 seconds)\n",
      "2023-04-17 13:09:28 | INFO | fairseq_cli.train | end of epoch 302 (average epoch stats below)\n",
      "2023-04-17 13:09:28 | INFO | train | epoch 302 | loss 2.621 | nll_loss 0.458 | ppl 1.37 | wps 850.7 | ups 1.25 | wpb 678.5 | bsz 2 | num_updates 9655 | lr 2.65453e-05 | gnorm 5.711 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 8248\n",
      "2023-04-17 13:09:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:09:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:09:28 | INFO | fairseq.trainer | begin training epoch 303\n",
      "2023-04-17 13:09:28 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:09:40 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:09:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:09:40 | INFO | valid | epoch 303 | valid on 'valid' subset | loss 5.542 | nll_loss 3.755 | ppl 13.5 | wps 6479.7 | wpb 290.8 | bsz 1 | num_updates 9687 | best_loss 3.979\n",
      "2023-04-17 13:09:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 303 @ 9687 updates\n",
      "2023-04-17 13:09:40 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:09:52 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:09:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 303 @ 9687 updates, score 5.542) (writing took 12.314659176045097 seconds)\n",
      "2023-04-17 13:09:52 | INFO | fairseq_cli.train | end of epoch 303 (average epoch stats below)\n",
      "2023-04-17 13:09:52 | INFO | train | epoch 303 | loss 2.573 | nll_loss 0.406 | ppl 1.32 | wps 879.3 | ups 1.3 | wpb 678.5 | bsz 2 | num_updates 9687 | lr 2.65332e-05 | gnorm 4.612 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 8273\n",
      "2023-04-17 13:09:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:09:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:09:52 | INFO | fairseq.trainer | begin training epoch 304\n",
      "2023-04-17 13:09:52 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:09:57 | INFO | train_inner | epoch 304:     13 / 32 loss=2.591, nll_loss=0.426, ppl=1.34, wps=885.9, ups=1.31, wpb=676.1, bsz=2, num_updates=9700, lr=2.65283e-05, gnorm=4.826, clip=100, loss_scale=0.5, train_wall=37, gb_free=13.9, wall=8277\n",
      "2023-04-17 13:10:04 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:10:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:10:05 | INFO | valid | epoch 304 | valid on 'valid' subset | loss 5.492 | nll_loss 3.688 | ppl 12.88 | wps 6217.9 | wpb 290.8 | bsz 1 | num_updates 9719 | best_loss 3.979\n",
      "2023-04-17 13:10:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 304 @ 9719 updates\n",
      "2023-04-17 13:10:05 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:10:17 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:10:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 304 @ 9719 updates, score 5.492) (writing took 12.712114154943265 seconds)\n",
      "2023-04-17 13:10:17 | INFO | fairseq_cli.train | end of epoch 304 (average epoch stats below)\n",
      "2023-04-17 13:10:17 | INFO | train | epoch 304 | loss 2.605 | nll_loss 0.441 | ppl 1.36 | wps 868.8 | ups 1.28 | wpb 678.5 | bsz 2 | num_updates 9719 | lr 2.65211e-05 | gnorm 4.358 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 8297\n",
      "2023-04-17 13:10:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:10:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:10:17 | INFO | fairseq.trainer | begin training epoch 305\n",
      "2023-04-17 13:10:17 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:10:29 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:10:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:10:30 | INFO | valid | epoch 305 | valid on 'valid' subset | loss 5.526 | nll_loss 3.729 | ppl 13.26 | wps 6488.1 | wpb 290.8 | bsz 1 | num_updates 9751 | best_loss 3.979\n",
      "2023-04-17 13:10:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 305 @ 9751 updates\n",
      "2023-04-17 13:10:30 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 13:10:42 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:10:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 305 @ 9751 updates, score 5.526) (writing took 12.609920743969269 seconds)\n",
      "2023-04-17 13:10:42 | INFO | fairseq_cli.train | end of epoch 305 (average epoch stats below)\n",
      "2023-04-17 13:10:42 | INFO | train | epoch 305 | loss 2.58 | nll_loss 0.413 | ppl 1.33 | wps 867.3 | ups 1.28 | wpb 678.5 | bsz 2 | num_updates 9751 | lr 2.65091e-05 | gnorm 4.151 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 8323\n",
      "2023-04-17 13:10:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:10:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:10:42 | INFO | fairseq.trainer | begin training epoch 306\n",
      "2023-04-17 13:10:42 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:10:54 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:10:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:10:55 | INFO | valid | epoch 306 | valid on 'valid' subset | loss 5.422 | nll_loss 3.62 | ppl 12.3 | wps 6463.3 | wpb 290.8 | bsz 1 | num_updates 9783 | best_loss 3.979\n",
      "2023-04-17 13:10:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 306 @ 9783 updates\n",
      "2023-04-17 13:10:55 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:11:07 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:11:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 306 @ 9783 updates, score 5.422) (writing took 12.861655337968841 seconds)\n",
      "2023-04-17 13:11:08 | INFO | fairseq_cli.train | end of epoch 306 (average epoch stats below)\n",
      "2023-04-17 13:11:08 | INFO | train | epoch 306 | loss 2.598 | nll_loss 0.43 | ppl 1.35 | wps 857.4 | ups 1.26 | wpb 678.5 | bsz 2 | num_updates 9783 | lr 2.6497e-05 | gnorm 5.036 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 8348\n",
      "2023-04-17 13:11:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:11:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:11:08 | INFO | fairseq.trainer | begin training epoch 307\n",
      "2023-04-17 13:11:08 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:11:14 | INFO | train_inner | epoch 307:     17 / 32 loss=2.595, nll_loss=0.429, ppl=1.35, wps=888.4, ups=1.3, wpb=683.7, bsz=2, num_updates=9800, lr=2.64906e-05, gnorm=4.498, clip=100, loss_scale=0.5, train_wall=37, gb_free=13.9, wall=8354\n",
      "2023-04-17 13:11:20 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:11:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:11:20 | INFO | valid | epoch 307 | valid on 'valid' subset | loss 5.469 | nll_loss 3.649 | ppl 12.55 | wps 6508.6 | wpb 290.8 | bsz 1 | num_updates 9815 | best_loss 3.979\n",
      "2023-04-17 13:11:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 307 @ 9815 updates\n",
      "2023-04-17 13:11:20 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:11:34 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:11:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 307 @ 9815 updates, score 5.469) (writing took 14.09705587697681 seconds)\n",
      "2023-04-17 13:11:34 | INFO | fairseq_cli.train | end of epoch 307 (average epoch stats below)\n",
      "2023-04-17 13:11:34 | INFO | train | epoch 307 | loss 2.589 | nll_loss 0.423 | ppl 1.34 | wps 820.7 | ups 1.21 | wpb 678.5 | bsz 2 | num_updates 9815 | lr 2.64849e-05 | gnorm 4.293 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 8374\n",
      "2023-04-17 13:11:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:11:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:11:34 | INFO | fairseq.trainer | begin training epoch 308\n",
      "2023-04-17 13:11:34 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:11:46 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:11:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:11:47 | INFO | valid | epoch 308 | valid on 'valid' subset | loss 5.458 | nll_loss 3.658 | ppl 12.62 | wps 6441.6 | wpb 290.8 | bsz 1 | num_updates 9847 | best_loss 3.979\n",
      "2023-04-17 13:11:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 308 @ 9847 updates\n",
      "2023-04-17 13:11:47 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:11:59 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:11:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 308 @ 9847 updates, score 5.458) (writing took 12.712113979971036 seconds)\n",
      "2023-04-17 13:11:59 | INFO | fairseq_cli.train | end of epoch 308 (average epoch stats below)\n",
      "2023-04-17 13:11:59 | INFO | train | epoch 308 | loss 2.572 | nll_loss 0.402 | ppl 1.32 | wps 860.7 | ups 1.27 | wpb 678.5 | bsz 2 | num_updates 9847 | lr 2.64728e-05 | gnorm 4.049 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 8400\n",
      "2023-04-17 13:11:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:11:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:11:59 | INFO | fairseq.trainer | begin training epoch 309\n",
      "2023-04-17 13:11:59 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:12:12 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:12:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:12:12 | INFO | valid | epoch 309 | valid on 'valid' subset | loss 5.458 | nll_loss 3.657 | ppl 12.62 | wps 5520 | wpb 290.8 | bsz 1 | num_updates 9879 | best_loss 3.979\n",
      "2023-04-17 13:12:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 309 @ 9879 updates\n",
      "2023-04-17 13:12:12 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:12:25 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:12:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 309 @ 9879 updates, score 5.458) (writing took 13.055432328023016 seconds)\n",
      "2023-04-17 13:12:25 | INFO | fairseq_cli.train | end of epoch 309 (average epoch stats below)\n",
      "2023-04-17 13:12:25 | INFO | train | epoch 309 | loss 2.599 | nll_loss 0.434 | ppl 1.35 | wps 828.4 | ups 1.22 | wpb 678.5 | bsz 2 | num_updates 9879 | lr 2.64608e-05 | gnorm 6.027 | clip 100 | loss_scale 0.5 | train_wall 13 | gb_free 13.9 | wall 8426\n",
      "2023-04-17 13:12:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:12:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:12:26 | INFO | fairseq.trainer | begin training epoch 310\n",
      "2023-04-17 13:12:26 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:12:34 | INFO | train_inner | epoch 310:     21 / 32 loss=2.579, nll_loss=0.411, ppl=1.33, wps=841.2, ups=1.26, wpb=668.8, bsz=2, num_updates=9900, lr=2.64528e-05, gnorm=4.639, clip=100, loss_scale=0.5, train_wall=38, gb_free=13.9, wall=8434\n",
      "2023-04-17 13:12:38 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:12:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:12:38 | INFO | valid | epoch 310 | valid on 'valid' subset | loss 5.529 | nll_loss 3.735 | ppl 13.32 | wps 6376.4 | wpb 290.8 | bsz 1 | num_updates 9911 | best_loss 3.979\n",
      "2023-04-17 13:12:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 310 @ 9911 updates\n",
      "2023-04-17 13:12:38 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 13:12:50 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:12:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 310 @ 9911 updates, score 5.529) (writing took 12.508884494076483 seconds)\n",
      "2023-04-17 13:12:51 | INFO | fairseq_cli.train | end of epoch 310 (average epoch stats below)\n",
      "2023-04-17 13:12:51 | INFO | train | epoch 310 | loss 2.551 | nll_loss 0.385 | ppl 1.31 | wps 868.2 | ups 1.28 | wpb 678.5 | bsz 2 | num_updates 9911 | lr 2.64487e-05 | gnorm 3.751 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 8451\n",
      "2023-04-17 13:12:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:12:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:12:51 | INFO | fairseq.trainer | begin training epoch 311\n",
      "2023-04-17 13:12:51 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:13:02 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:13:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:13:03 | INFO | valid | epoch 311 | valid on 'valid' subset | loss 5.459 | nll_loss 3.684 | ppl 12.86 | wps 6577.9 | wpb 290.8 | bsz 1 | num_updates 9943 | best_loss 3.979\n",
      "2023-04-17 13:13:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 311 @ 9943 updates\n",
      "2023-04-17 13:13:03 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:13:15 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:13:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 311 @ 9943 updates, score 5.459) (writing took 12.40428830194287 seconds)\n",
      "2023-04-17 13:13:15 | INFO | fairseq_cli.train | end of epoch 311 (average epoch stats below)\n",
      "2023-04-17 13:13:15 | INFO | train | epoch 311 | loss 2.564 | nll_loss 0.394 | ppl 1.31 | wps 887.6 | ups 1.31 | wpb 678.5 | bsz 2 | num_updates 9943 | lr 2.64366e-05 | gnorm 4.215 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 8475\n",
      "2023-04-17 13:13:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:13:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:13:15 | INFO | fairseq.trainer | begin training epoch 312\n",
      "2023-04-17 13:13:15 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:13:27 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:13:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:13:27 | INFO | valid | epoch 312 | valid on 'valid' subset | loss 5.484 | nll_loss 3.674 | ppl 12.76 | wps 7045.4 | wpb 290.8 | bsz 1 | num_updates 9975 | best_loss 3.979\n",
      "2023-04-17 13:13:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 312 @ 9975 updates\n",
      "2023-04-17 13:13:27 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:13:40 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:13:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 312 @ 9975 updates, score 5.484) (writing took 13.182302328990772 seconds)\n",
      "2023-04-17 13:13:40 | INFO | fairseq_cli.train | end of epoch 312 (average epoch stats below)\n",
      "2023-04-17 13:13:40 | INFO | train | epoch 312 | loss 2.561 | nll_loss 0.394 | ppl 1.31 | wps 863.6 | ups 1.27 | wpb 678.5 | bsz 2 | num_updates 9975 | lr 2.64245e-05 | gnorm 3.728 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 8500\n",
      "2023-04-17 13:13:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:13:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:13:40 | INFO | fairseq.trainer | begin training epoch 313\n",
      "2023-04-17 13:13:40 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:13:50 | INFO | train_inner | epoch 313:     25 / 32 loss=2.557, nll_loss=0.389, ppl=1.31, wps=903.1, ups=1.31, wpb=686.9, bsz=2, num_updates=10000, lr=2.64151e-05, gnorm=3.876, clip=100, loss_scale=0.5, train_wall=37, gb_free=13.9, wall=8510\n",
      "2023-04-17 13:13:50 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:13:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:13:50 | INFO | valid | epoch 313 | valid on 'valid' subset | loss 5.538 | nll_loss 3.736 | ppl 13.33 | wps 5633.7 | wpb 290.8 | bsz 1 | num_updates 10000 | best_loss 3.979\n",
      "2023-04-17 13:13:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 313 @ 10000 updates\n",
      "2023-04-17 13:13:50 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_313_10000.pt\n",
      "2023-04-17 13:13:58 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_313_10000.pt\n",
      "2023-04-17 13:14:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_313_10000.pt (epoch 313 @ 10000 updates, score 5.538) (writing took 16.271047297981568 seconds)\n",
      "2023-04-17 13:14:09 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:14:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:14:09 | INFO | valid | epoch 313 | valid on 'valid' subset | loss 5.495 | nll_loss 3.705 | ppl 13.04 | wps 6319.2 | wpb 290.8 | bsz 1 | num_updates 10007 | best_loss 3.979\n",
      "2023-04-17 13:14:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 313 @ 10007 updates\n",
      "2023-04-17 13:14:09 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:14:22 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:14:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 313 @ 10007 updates, score 5.495) (writing took 13.52463000593707 seconds)\n",
      "2023-04-17 13:14:23 | INFO | fairseq_cli.train | end of epoch 313 (average epoch stats below)\n",
      "2023-04-17 13:14:23 | INFO | train | epoch 313 | loss 2.55 | nll_loss 0.38 | ppl 1.3 | wps 510.3 | ups 0.75 | wpb 678.5 | bsz 2 | num_updates 10007 | lr 2.64125e-05 | gnorm 3.759 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 8543\n",
      "2023-04-17 13:14:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:14:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:14:23 | INFO | fairseq.trainer | begin training epoch 314\n",
      "2023-04-17 13:14:23 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:14:34 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:14:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:14:35 | INFO | valid | epoch 314 | valid on 'valid' subset | loss 5.541 | nll_loss 3.751 | ppl 13.46 | wps 6534 | wpb 290.8 | bsz 1 | num_updates 10039 | best_loss 3.979\n",
      "2023-04-17 13:14:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 314 @ 10039 updates\n",
      "2023-04-17 13:14:35 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:14:48 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:14:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 314 @ 10039 updates, score 5.541) (writing took 13.862598767038435 seconds)\n",
      "2023-04-17 13:14:49 | INFO | fairseq_cli.train | end of epoch 314 (average epoch stats below)\n",
      "2023-04-17 13:14:49 | INFO | train | epoch 314 | loss 2.552 | nll_loss 0.383 | ppl 1.3 | wps 838.4 | ups 1.24 | wpb 678.5 | bsz 2 | num_updates 10039 | lr 2.64004e-05 | gnorm 3.751 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 8569\n",
      "2023-04-17 13:14:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 13:14:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:14:49 | INFO | fairseq.trainer | begin training epoch 315\n",
      "2023-04-17 13:14:49 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:15:00 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:15:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:15:01 | INFO | valid | epoch 315 | valid on 'valid' subset | loss 5.488 | nll_loss 3.694 | ppl 12.94 | wps 7360.4 | wpb 290.8 | bsz 1 | num_updates 10071 | best_loss 3.979\n",
      "2023-04-17 13:15:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 315 @ 10071 updates\n",
      "2023-04-17 13:15:01 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:15:14 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:15:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 315 @ 10071 updates, score 5.488) (writing took 13.448254657909274 seconds)\n",
      "2023-04-17 13:15:14 | INFO | fairseq_cli.train | end of epoch 315 (average epoch stats below)\n",
      "2023-04-17 13:15:14 | INFO | train | epoch 315 | loss 2.549 | nll_loss 0.381 | ppl 1.3 | wps 850.4 | ups 1.25 | wpb 678.5 | bsz 2 | num_updates 10071 | lr 2.63883e-05 | gnorm 3.558 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 8594\n",
      "2023-04-17 13:15:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:15:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:15:14 | INFO | fairseq.trainer | begin training epoch 316\n",
      "2023-04-17 13:15:14 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:15:25 | INFO | train_inner | epoch 316:     29 / 32 loss=2.551, nll_loss=0.382, ppl=1.3, wps=709.2, ups=1.05, wpb=675, bsz=2, num_updates=10100, lr=2.63774e-05, gnorm=3.719, clip=100, loss_scale=0.5, train_wall=37, gb_free=13.9, wall=8605\n",
      "2023-04-17 13:15:26 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:15:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:15:26 | INFO | valid | epoch 316 | valid on 'valid' subset | loss 5.545 | nll_loss 3.744 | ppl 13.4 | wps 7216.9 | wpb 290.8 | bsz 1 | num_updates 10103 | best_loss 3.979\n",
      "2023-04-17 13:15:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 316 @ 10103 updates\n",
      "2023-04-17 13:15:26 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:15:38 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:15:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 316 @ 10103 updates, score 5.545) (writing took 12.445576151018031 seconds)\n",
      "2023-04-17 13:15:39 | INFO | fairseq_cli.train | end of epoch 316 (average epoch stats below)\n",
      "2023-04-17 13:15:39 | INFO | train | epoch 316 | loss 2.553 | nll_loss 0.386 | ppl 1.31 | wps 885.5 | ups 1.3 | wpb 678.5 | bsz 2 | num_updates 10103 | lr 2.63762e-05 | gnorm 4.348 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 8619\n",
      "2023-04-17 13:15:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:15:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:15:39 | INFO | fairseq.trainer | begin training epoch 317\n",
      "2023-04-17 13:15:39 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:15:51 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:15:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:15:51 | INFO | valid | epoch 317 | valid on 'valid' subset | loss 5.507 | nll_loss 3.73 | ppl 13.27 | wps 7133.6 | wpb 290.8 | bsz 1 | num_updates 10135 | best_loss 3.979\n",
      "2023-04-17 13:15:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 317 @ 10135 updates\n",
      "2023-04-17 13:15:51 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:16:04 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:16:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 317 @ 10135 updates, score 5.507) (writing took 12.942231888999231 seconds)\n",
      "2023-04-17 13:16:04 | INFO | fairseq_cli.train | end of epoch 317 (average epoch stats below)\n",
      "2023-04-17 13:16:04 | INFO | train | epoch 317 | loss 2.593 | nll_loss 0.426 | ppl 1.34 | wps 860.1 | ups 1.27 | wpb 678.5 | bsz 2 | num_updates 10135 | lr 2.63642e-05 | gnorm 11.426 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 8644\n",
      "2023-04-17 13:16:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:16:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:16:04 | INFO | fairseq.trainer | begin training epoch 318\n",
      "2023-04-17 13:16:04 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:16:16 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:16:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:16:16 | INFO | valid | epoch 318 | valid on 'valid' subset | loss 5.486 | nll_loss 3.695 | ppl 12.95 | wps 7361.2 | wpb 290.8 | bsz 1 | num_updates 10167 | best_loss 3.979\n",
      "2023-04-17 13:16:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 318 @ 10167 updates\n",
      "2023-04-17 13:16:16 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:17:19 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:17:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 320 @ 10231 updates, score 5.575) (writing took 12.551314108073711 seconds)\n",
      "2023-04-17 13:17:19 | INFO | fairseq_cli.train | end of epoch 320 (average epoch stats below)\n",
      "2023-04-17 13:17:19 | INFO | train | epoch 320 | loss 2.55 | nll_loss 0.382 | ppl 1.3 | wps 867.2 | ups 1.28 | wpb 678.5 | bsz 2 | num_updates 10231 | lr 2.63279e-05 | gnorm 4.015 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 8720\n",
      "2023-04-17 13:17:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:17:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:17:19 | INFO | fairseq.trainer | begin training epoch 321\n",
      "2023-04-17 13:17:19 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:17:31 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:17:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:17:31 | INFO | valid | epoch 321 | valid on 'valid' subset | loss 5.495 | nll_loss 3.701 | ppl 13.01 | wps 6104.7 | wpb 290.8 | bsz 1 | num_updates 10263 | best_loss 3.979\n",
      "2023-04-17 13:17:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 321 @ 10263 updates\n",
      "2023-04-17 13:17:31 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:17:44 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:17:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 321 @ 10263 updates, score 5.495) (writing took 13.389085532049648 seconds)\n",
      "2023-04-17 13:17:45 | INFO | fairseq_cli.train | end of epoch 321 (average epoch stats below)\n",
      "2023-04-17 13:17:45 | INFO | train | epoch 321 | loss 2.548 | nll_loss 0.377 | ppl 1.3 | wps 861.5 | ups 1.27 | wpb 678.5 | bsz 2 | num_updates 10263 | lr 2.63158e-05 | gnorm 3.736 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 8745\n",
      "2023-04-17 13:17:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:17:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:17:45 | INFO | fairseq.trainer | begin training epoch 322\n",
      "2023-04-17 13:17:45 | INFO | fairseq_cli.train | Start iterating over samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 13:17:56 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:17:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:17:56 | INFO | valid | epoch 322 | valid on 'valid' subset | loss 5.554 | nll_loss 3.756 | ppl 13.51 | wps 7320 | wpb 290.8 | bsz 1 | num_updates 10295 | best_loss 3.979\n",
      "2023-04-17 13:17:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 322 @ 10295 updates\n",
      "2023-04-17 13:17:56 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:18:09 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:18:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 322 @ 10295 updates, score 5.554) (writing took 12.698039820999838 seconds)\n",
      "2023-04-17 13:18:09 | INFO | fairseq_cli.train | end of epoch 322 (average epoch stats below)\n",
      "2023-04-17 13:18:09 | INFO | train | epoch 322 | loss 2.542 | nll_loss 0.375 | ppl 1.3 | wps 885.3 | ups 1.3 | wpb 678.5 | bsz 2 | num_updates 10295 | lr 2.63038e-05 | gnorm 5.647 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 8769\n",
      "2023-04-17 13:18:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:18:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:18:09 | INFO | fairseq.trainer | begin training epoch 323\n",
      "2023-04-17 13:18:09 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:18:11 | INFO | train_inner | epoch 323:      5 / 32 loss=2.547, nll_loss=0.377, ppl=1.3, wps=892, ups=1.31, wpb=679.2, bsz=2, num_updates=10300, lr=2.63019e-05, gnorm=4.729, clip=100, loss_scale=0.5, train_wall=36, gb_free=13.9, wall=8771\n",
      "2023-04-17 13:18:20 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:18:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:18:21 | INFO | valid | epoch 323 | valid on 'valid' subset | loss 5.457 | nll_loss 3.671 | ppl 12.74 | wps 7305.7 | wpb 290.8 | bsz 1 | num_updates 10327 | best_loss 3.979\n",
      "2023-04-17 13:18:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 323 @ 10327 updates\n",
      "2023-04-17 13:18:21 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:18:33 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:18:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 323 @ 10327 updates, score 5.457) (writing took 12.521747869090177 seconds)\n",
      "2023-04-17 13:18:33 | INFO | fairseq_cli.train | end of epoch 323 (average epoch stats below)\n",
      "2023-04-17 13:18:33 | INFO | train | epoch 323 | loss 2.543 | nll_loss 0.374 | ppl 1.3 | wps 906.4 | ups 1.34 | wpb 678.5 | bsz 2 | num_updates 10327 | lr 2.62917e-05 | gnorm 5.148 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 8793\n",
      "2023-04-17 13:18:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:18:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:18:33 | INFO | fairseq.trainer | begin training epoch 324\n",
      "2023-04-17 13:18:33 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:18:45 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:18:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:18:45 | INFO | valid | epoch 324 | valid on 'valid' subset | loss 5.582 | nll_loss 3.797 | ppl 13.9 | wps 7264.7 | wpb 290.8 | bsz 1 | num_updates 10359 | best_loss 3.979\n",
      "2023-04-17 13:18:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 324 @ 10359 updates\n",
      "2023-04-17 13:18:45 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:18:59 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:18:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 324 @ 10359 updates, score 5.582) (writing took 14.137641230016015 seconds)\n",
      "2023-04-17 13:18:59 | INFO | fairseq_cli.train | end of epoch 324 (average epoch stats below)\n",
      "2023-04-17 13:18:59 | INFO | train | epoch 324 | loss 2.524 | nll_loss 0.356 | ppl 1.28 | wps 823.9 | ups 1.21 | wpb 678.5 | bsz 2 | num_updates 10359 | lr 2.62796e-05 | gnorm 3.747 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 8820\n",
      "2023-04-17 13:18:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:18:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:18:59 | INFO | fairseq.trainer | begin training epoch 325\n",
      "2023-04-17 13:18:59 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:19:10 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:19:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:19:10 | INFO | valid | epoch 325 | valid on 'valid' subset | loss 5.48 | nll_loss 3.691 | ppl 12.91 | wps 7270.6 | wpb 290.8 | bsz 1 | num_updates 10391 | best_loss 3.979\n",
      "2023-04-17 13:19:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 325 @ 10391 updates\n",
      "2023-04-17 13:19:10 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:19:23 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:19:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 325 @ 10391 updates, score 5.48) (writing took 13.03351376100909 seconds)\n",
      "2023-04-17 13:19:23 | INFO | fairseq_cli.train | end of epoch 325 (average epoch stats below)\n",
      "2023-04-17 13:19:23 | INFO | train | epoch 325 | loss 2.527 | nll_loss 0.359 | ppl 1.28 | wps 902.8 | ups 1.33 | wpb 678.5 | bsz 2 | num_updates 10391 | lr 2.62675e-05 | gnorm 3.907 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 8844\n",
      "2023-04-17 13:19:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:19:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:19:23 | INFO | fairseq.trainer | begin training epoch 326\n",
      "2023-04-17 13:19:23 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:19:27 | INFO | train_inner | epoch 326:      9 / 32 loss=2.53, nll_loss=0.363, ppl=1.29, wps=904, ups=1.32, wpb=685.5, bsz=2, num_updates=10400, lr=2.62642e-05, gnorm=3.949, clip=100, loss_scale=0.5, train_wall=35, gb_free=13.9, wall=8847\n",
      "2023-04-17 13:19:35 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:19:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:19:35 | INFO | valid | epoch 326 | valid on 'valid' subset | loss 5.455 | nll_loss 3.679 | ppl 12.81 | wps 7164.6 | wpb 290.8 | bsz 1 | num_updates 10423 | best_loss 3.979\n",
      "2023-04-17 13:19:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 326 @ 10423 updates\n",
      "2023-04-17 13:19:35 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:19:49 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:19:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 326 @ 10423 updates, score 5.455) (writing took 14.883872259990312 seconds)\n",
      "2023-04-17 13:19:50 | INFO | fairseq_cli.train | end of epoch 326 (average epoch stats below)\n",
      "2023-04-17 13:19:50 | INFO | train | epoch 326 | loss 2.527 | nll_loss 0.359 | ppl 1.28 | wps 826.5 | ups 1.22 | wpb 678.5 | bsz 2 | num_updates 10423 | lr 2.62555e-05 | gnorm 3.72 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 8870\n",
      "2023-04-17 13:19:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:19:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:19:50 | INFO | fairseq.trainer | begin training epoch 327\n",
      "2023-04-17 13:19:50 | INFO | fairseq_cli.train | Start iterating over samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 13:20:02 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:20:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:20:02 | INFO | valid | epoch 327 | valid on 'valid' subset | loss 5.539 | nll_loss 3.769 | ppl 13.63 | wps 6967 | wpb 290.8 | bsz 1 | num_updates 10455 | best_loss 3.979\n",
      "2023-04-17 13:20:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 327 @ 10455 updates\n",
      "2023-04-17 13:20:02 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:20:16 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:20:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 327 @ 10455 updates, score 5.539) (writing took 14.22057255601976 seconds)\n",
      "2023-04-17 13:20:16 | INFO | fairseq_cli.train | end of epoch 327 (average epoch stats below)\n",
      "2023-04-17 13:20:16 | INFO | train | epoch 327 | loss 2.522 | nll_loss 0.356 | ppl 1.28 | wps 820.6 | ups 1.21 | wpb 678.5 | bsz 2 | num_updates 10455 | lr 2.62434e-05 | gnorm 3.814 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 8896\n",
      "2023-04-17 13:20:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:20:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:20:16 | INFO | fairseq.trainer | begin training epoch 328\n",
      "2023-04-17 13:20:16 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:20:28 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:20:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:20:28 | INFO | valid | epoch 328 | valid on 'valid' subset | loss 5.514 | nll_loss 3.739 | ppl 13.35 | wps 7036.3 | wpb 290.8 | bsz 1 | num_updates 10487 | best_loss 3.979\n",
      "2023-04-17 13:20:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 328 @ 10487 updates\n",
      "2023-04-17 13:20:28 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:20:42 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:20:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 328 @ 10487 updates, score 5.514) (writing took 14.032649534987286 seconds)\n",
      "2023-04-17 13:20:42 | INFO | fairseq_cli.train | end of epoch 328 (average epoch stats below)\n",
      "2023-04-17 13:20:42 | INFO | train | epoch 328 | loss 2.526 | nll_loss 0.358 | ppl 1.28 | wps 830 | ups 1.22 | wpb 678.5 | bsz 2 | num_updates 10487 | lr 2.62313e-05 | gnorm 3.89 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 8923\n",
      "2023-04-17 13:20:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:20:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:20:42 | INFO | fairseq.trainer | begin training epoch 329\n",
      "2023-04-17 13:20:42 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:20:48 | INFO | train_inner | epoch 329:     13 / 32 loss=2.526, nll_loss=0.359, ppl=1.28, wps=843.9, ups=1.24, wpb=682.7, bsz=2, num_updates=10500, lr=2.62264e-05, gnorm=3.78, clip=100, loss_scale=0.5, train_wall=37, gb_free=13.9, wall=8928\n",
      "2023-04-17 13:20:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:20:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:20:55 | INFO | valid | epoch 329 | valid on 'valid' subset | loss 5.501 | nll_loss 3.721 | ppl 13.18 | wps 7131.1 | wpb 290.8 | bsz 1 | num_updates 10519 | best_loss 3.979\n",
      "2023-04-17 13:20:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 329 @ 10519 updates\n",
      "2023-04-17 13:20:55 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:21:08 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:21:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 329 @ 10519 updates, score 5.501) (writing took 13.134482874069363 seconds)\n",
      "2023-04-17 13:21:08 | INFO | fairseq_cli.train | end of epoch 329 (average epoch stats below)\n",
      "2023-04-17 13:21:08 | INFO | train | epoch 329 | loss 2.533 | nll_loss 0.367 | ppl 1.29 | wps 834.2 | ups 1.23 | wpb 678.5 | bsz 2 | num_updates 10519 | lr 2.62192e-05 | gnorm 4.079 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 8949\n",
      "2023-04-17 13:21:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:21:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:21:08 | INFO | fairseq.trainer | begin training epoch 330\n",
      "2023-04-17 13:21:08 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:21:20 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:21:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:21:20 | INFO | valid | epoch 330 | valid on 'valid' subset | loss 5.544 | nll_loss 3.777 | ppl 13.7 | wps 6095.4 | wpb 290.8 | bsz 1 | num_updates 10551 | best_loss 3.979\n",
      "2023-04-17 13:21:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 330 @ 10551 updates\n",
      "2023-04-17 13:21:20 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:21:34 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:21:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 330 @ 10551 updates, score 5.544) (writing took 13.693737145047635 seconds)\n",
      "2023-04-17 13:21:34 | INFO | fairseq_cli.train | end of epoch 330 (average epoch stats below)\n",
      "2023-04-17 13:21:34 | INFO | train | epoch 330 | loss 2.531 | nll_loss 0.366 | ppl 1.29 | wps 847.1 | ups 1.25 | wpb 678.5 | bsz 2 | num_updates 10551 | lr 2.62072e-05 | gnorm 3.909 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 8974\n",
      "2023-04-17 13:21:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:21:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:21:34 | INFO | fairseq.trainer | begin training epoch 331\n",
      "2023-04-17 13:21:34 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:21:46 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:21:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:21:46 | INFO | valid | epoch 331 | valid on 'valid' subset | loss 5.626 | nll_loss 3.859 | ppl 14.51 | wps 6852.8 | wpb 290.8 | bsz 1 | num_updates 10583 | best_loss 3.979\n",
      "2023-04-17 13:21:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 331 @ 10583 updates\n",
      "2023-04-17 13:21:46 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:21:59 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:21:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 331 @ 10583 updates, score 5.626) (writing took 13.267259616055526 seconds)\n",
      "2023-04-17 13:21:59 | INFO | fairseq_cli.train | end of epoch 331 (average epoch stats below)\n",
      "2023-04-17 13:21:59 | INFO | train | epoch 331 | loss 2.513 | nll_loss 0.346 | ppl 1.27 | wps 864.8 | ups 1.27 | wpb 678.5 | bsz 2 | num_updates 10583 | lr 2.61951e-05 | gnorm 3.524 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 8999\n",
      "2023-04-17 13:21:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:21:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:21:59 | INFO | fairseq.trainer | begin training epoch 332\n",
      "2023-04-17 13:21:59 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:22:05 | INFO | train_inner | epoch 332:     17 / 32 loss=2.523, nll_loss=0.357, ppl=1.28, wps=847.9, ups=1.29, wpb=656.2, bsz=2, num_updates=10600, lr=2.61887e-05, gnorm=3.964, clip=100, loss_scale=0.5, train_wall=36, gb_free=13.9, wall=9005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 13:22:10 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:22:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:22:10 | INFO | valid | epoch 332 | valid on 'valid' subset | loss 5.524 | nll_loss 3.745 | ppl 13.4 | wps 7110.6 | wpb 290.8 | bsz 1 | num_updates 10615 | best_loss 3.979\n",
      "2023-04-17 13:22:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 332 @ 10615 updates\n",
      "2023-04-17 13:22:10 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:22:24 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:22:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 332 @ 10615 updates, score 5.524) (writing took 13.829959249007516 seconds)\n",
      "2023-04-17 13:22:24 | INFO | fairseq_cli.train | end of epoch 332 (average epoch stats below)\n",
      "2023-04-17 13:22:24 | INFO | train | epoch 332 | loss 2.522 | nll_loss 0.357 | ppl 1.28 | wps 870.2 | ups 1.28 | wpb 678.5 | bsz 2 | num_updates 10615 | lr 2.6183e-05 | gnorm 4.21 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 9024\n",
      "2023-04-17 13:22:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:22:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:22:24 | INFO | fairseq.trainer | begin training epoch 333\n",
      "2023-04-17 13:22:24 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:22:35 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:22:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:22:36 | INFO | valid | epoch 333 | valid on 'valid' subset | loss 5.571 | nll_loss 3.801 | ppl 13.94 | wps 7197.2 | wpb 290.8 | bsz 1 | num_updates 10647 | best_loss 3.979\n",
      "2023-04-17 13:22:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 333 @ 10647 updates\n",
      "2023-04-17 13:22:36 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:22:49 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:22:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 333 @ 10647 updates, score 5.571) (writing took 13.155524756992236 seconds)\n",
      "2023-04-17 13:22:49 | INFO | fairseq_cli.train | end of epoch 333 (average epoch stats below)\n",
      "2023-04-17 13:22:49 | INFO | train | epoch 333 | loss 2.516 | nll_loss 0.35 | ppl 1.27 | wps 881.7 | ups 1.3 | wpb 678.5 | bsz 2 | num_updates 10647 | lr 2.61709e-05 | gnorm 3.727 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 9049\n",
      "2023-04-17 13:22:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:22:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:22:49 | INFO | fairseq.trainer | begin training epoch 334\n",
      "2023-04-17 13:22:49 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:23:00 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:23:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:23:00 | INFO | valid | epoch 334 | valid on 'valid' subset | loss 5.468 | nll_loss 3.681 | ppl 12.83 | wps 7274.6 | wpb 290.8 | bsz 1 | num_updates 10679 | best_loss 3.979\n",
      "2023-04-17 13:23:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 334 @ 10679 updates\n",
      "2023-04-17 13:23:00 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:23:15 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:23:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 334 @ 10679 updates, score 5.468) (writing took 14.52269646700006 seconds)\n",
      "2023-04-17 13:23:15 | INFO | fairseq_cli.train | end of epoch 334 (average epoch stats below)\n",
      "2023-04-17 13:23:15 | INFO | train | epoch 334 | loss 2.519 | nll_loss 0.353 | ppl 1.28 | wps 827.2 | ups 1.22 | wpb 678.5 | bsz 2 | num_updates 10679 | lr 2.61589e-05 | gnorm 3.482 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 9075\n",
      "2023-04-17 13:23:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:23:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:23:15 | INFO | fairseq.trainer | begin training epoch 335\n",
      "2023-04-17 13:23:15 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:23:23 | INFO | train_inner | epoch 335:     21 / 32 loss=2.517, nll_loss=0.351, ppl=1.28, wps=883.6, ups=1.29, wpb=686, bsz=2, num_updates=10700, lr=2.61509e-05, gnorm=3.638, clip=100, loss_scale=0.5, train_wall=35, gb_free=13.9, wall=9083\n",
      "2023-04-17 13:23:27 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:23:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:23:27 | INFO | valid | epoch 335 | valid on 'valid' subset | loss 5.601 | nll_loss 3.828 | ppl 14.2 | wps 7309.6 | wpb 290.8 | bsz 1 | num_updates 10711 | best_loss 3.979\n",
      "2023-04-17 13:23:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 335 @ 10711 updates\n",
      "2023-04-17 13:23:27 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:23:40 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:23:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 335 @ 10711 updates, score 5.601) (writing took 13.009592980961315 seconds)\n",
      "2023-04-17 13:23:40 | INFO | fairseq_cli.train | end of epoch 335 (average epoch stats below)\n",
      "2023-04-17 13:23:40 | INFO | train | epoch 335 | loss 2.513 | nll_loss 0.347 | ppl 1.27 | wps 872 | ups 1.29 | wpb 678.5 | bsz 2 | num_updates 10711 | lr 2.61468e-05 | gnorm 3.53 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 9100\n",
      "2023-04-17 13:23:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:23:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:23:40 | INFO | fairseq.trainer | begin training epoch 336\n",
      "2023-04-17 13:23:40 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:23:51 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:23:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:23:52 | INFO | valid | epoch 336 | valid on 'valid' subset | loss 5.509 | nll_loss 3.729 | ppl 13.26 | wps 6209.9 | wpb 290.8 | bsz 1 | num_updates 10743 | best_loss 3.979\n",
      "2023-04-17 13:23:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 336 @ 10743 updates\n",
      "2023-04-17 13:23:52 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:24:04 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:24:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 336 @ 10743 updates, score 5.509) (writing took 13.068378183990717 seconds)\n",
      "2023-04-17 13:24:05 | INFO | fairseq_cli.train | end of epoch 336 (average epoch stats below)\n",
      "2023-04-17 13:24:05 | INFO | train | epoch 336 | loss 2.516 | nll_loss 0.35 | ppl 1.27 | wps 877.5 | ups 1.29 | wpb 678.5 | bsz 2 | num_updates 10743 | lr 2.61347e-05 | gnorm 3.687 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 9125\n",
      "2023-04-17 13:24:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:24:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:24:05 | INFO | fairseq.trainer | begin training epoch 337\n",
      "2023-04-17 13:24:05 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:24:16 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:24:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 13:24:16 | INFO | valid | epoch 337 | valid on 'valid' subset | loss 5.556 | nll_loss 3.788 | ppl 13.82 | wps 6975.5 | wpb 290.8 | bsz 1 | num_updates 10775 | best_loss 3.979\n",
      "2023-04-17 13:24:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 337 @ 10775 updates\n",
      "2023-04-17 13:24:16 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:24:29 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:24:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 337 @ 10775 updates, score 5.556) (writing took 13.3295110440813 seconds)\n",
      "2023-04-17 13:24:30 | INFO | fairseq_cli.train | end of epoch 337 (average epoch stats below)\n",
      "2023-04-17 13:24:30 | INFO | train | epoch 337 | loss 2.512 | nll_loss 0.345 | ppl 1.27 | wps 870.5 | ups 1.28 | wpb 678.5 | bsz 2 | num_updates 10775 | lr 2.61226e-05 | gnorm 3.722 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 9150\n",
      "2023-04-17 13:24:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:24:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:24:30 | INFO | fairseq.trainer | begin training epoch 338\n",
      "2023-04-17 13:24:30 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:24:38 | INFO | train_inner | epoch 338:     25 / 32 loss=2.516, nll_loss=0.349, ppl=1.27, wps=909.6, ups=1.32, wpb=688.3, bsz=2, num_updates=10800, lr=2.61132e-05, gnorm=3.74, clip=100, loss_scale=0.5, train_wall=35, gb_free=13.9, wall=9159\n",
      "2023-04-17 13:24:41 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:24:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:24:41 | INFO | valid | epoch 338 | valid on 'valid' subset | loss 5.529 | nll_loss 3.764 | ppl 13.58 | wps 6284 | wpb 290.8 | bsz 1 | num_updates 10807 | best_loss 3.979\n",
      "2023-04-17 13:24:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 338 @ 10807 updates\n",
      "2023-04-17 13:24:41 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:24:54 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:24:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 338 @ 10807 updates, score 5.529) (writing took 13.378008891013451 seconds)\n",
      "2023-04-17 13:24:54 | INFO | fairseq_cli.train | end of epoch 338 (average epoch stats below)\n",
      "2023-04-17 13:24:54 | INFO | train | epoch 338 | loss 2.518 | nll_loss 0.35 | ppl 1.27 | wps 870.9 | ups 1.28 | wpb 678.5 | bsz 2 | num_updates 10807 | lr 2.61106e-05 | gnorm 3.77 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 9175\n",
      "2023-04-17 13:24:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:24:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:24:54 | INFO | fairseq.trainer | begin training epoch 339\n",
      "2023-04-17 13:24:54 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:25:06 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:25:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:25:06 | INFO | valid | epoch 339 | valid on 'valid' subset | loss 5.559 | nll_loss 3.794 | ppl 13.87 | wps 6454.7 | wpb 290.8 | bsz 1 | num_updates 10839 | best_loss 3.979\n",
      "2023-04-17 13:25:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 339 @ 10839 updates\n",
      "2023-04-17 13:25:06 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:25:19 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:25:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 339 @ 10839 updates, score 5.559) (writing took 12.896658933954313 seconds)\n",
      "2023-04-17 13:25:19 | INFO | fairseq_cli.train | end of epoch 339 (average epoch stats below)\n",
      "2023-04-17 13:25:19 | INFO | train | epoch 339 | loss 2.511 | nll_loss 0.343 | ppl 1.27 | wps 873.2 | ups 1.29 | wpb 678.5 | bsz 2 | num_updates 10839 | lr 2.60985e-05 | gnorm 3.56 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 9200\n",
      "2023-04-17 13:25:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:25:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:25:19 | INFO | fairseq.trainer | begin training epoch 340\n",
      "2023-04-17 13:25:19 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:25:31 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:25:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:25:31 | INFO | valid | epoch 340 | valid on 'valid' subset | loss 5.585 | nll_loss 3.797 | ppl 13.9 | wps 6225.4 | wpb 290.8 | bsz 1 | num_updates 10871 | best_loss 3.979\n",
      "2023-04-17 13:25:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 340 @ 10871 updates\n",
      "2023-04-17 13:25:31 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:25:44 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:25:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 340 @ 10871 updates, score 5.585) (writing took 13.17546694399789 seconds)\n",
      "2023-04-17 13:25:44 | INFO | fairseq_cli.train | end of epoch 340 (average epoch stats below)\n",
      "2023-04-17 13:25:44 | INFO | train | epoch 340 | loss 2.514 | nll_loss 0.35 | ppl 1.27 | wps 881.1 | ups 1.3 | wpb 678.5 | bsz 2 | num_updates 10871 | lr 2.60864e-05 | gnorm 3.738 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 9224\n",
      "2023-04-17 13:25:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:25:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:25:44 | INFO | fairseq.trainer | begin training epoch 341\n",
      "2023-04-17 13:25:44 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:25:55 | INFO | train_inner | epoch 341:     29 / 32 loss=2.512, nll_loss=0.346, ppl=1.27, wps=885.9, ups=1.31, wpb=676.6, bsz=2, num_updates=10900, lr=2.60755e-05, gnorm=3.595, clip=100, loss_scale=0.5, train_wall=36, gb_free=13.9, wall=9235\n",
      "2023-04-17 13:25:56 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:25:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:25:56 | INFO | valid | epoch 341 | valid on 'valid' subset | loss 5.608 | nll_loss 3.84 | ppl 14.32 | wps 7261.6 | wpb 290.8 | bsz 1 | num_updates 10903 | best_loss 3.979\n",
      "2023-04-17 13:25:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 341 @ 10903 updates\n",
      "2023-04-17 13:25:56 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:26:09 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:26:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 341 @ 10903 updates, score 5.608) (writing took 12.845066639012657 seconds)\n",
      "2023-04-17 13:26:09 | INFO | fairseq_cli.train | end of epoch 341 (average epoch stats below)\n",
      "2023-04-17 13:26:09 | INFO | train | epoch 341 | loss 2.508 | nll_loss 0.342 | ppl 1.27 | wps 877.9 | ups 1.29 | wpb 678.5 | bsz 2 | num_updates 10903 | lr 2.60743e-05 | gnorm 3.521 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 9249\n",
      "2023-04-17 13:26:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:26:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:26:09 | INFO | fairseq.trainer | begin training epoch 342\n",
      "2023-04-17 13:26:09 | INFO | fairseq_cli.train | Start iterating over samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 13:26:20 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:26:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:26:20 | INFO | valid | epoch 342 | valid on 'valid' subset | loss 5.605 | nll_loss 3.843 | ppl 14.35 | wps 7184.9 | wpb 290.8 | bsz 1 | num_updates 10935 | best_loss 3.979\n",
      "2023-04-17 13:26:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 342 @ 10935 updates\n",
      "2023-04-17 13:26:20 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:26:33 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:26:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 342 @ 10935 updates, score 5.605) (writing took 13.159873118042015 seconds)\n",
      "2023-04-17 13:26:33 | INFO | fairseq_cli.train | end of epoch 342 (average epoch stats below)\n",
      "2023-04-17 13:26:33 | INFO | train | epoch 342 | loss 2.507 | nll_loss 0.34 | ppl 1.27 | wps 881 | ups 1.3 | wpb 678.5 | bsz 2 | num_updates 10935 | lr 2.60623e-05 | gnorm 3.871 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 9274\n",
      "2023-04-17 13:26:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:26:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:26:33 | INFO | fairseq.trainer | begin training epoch 343\n",
      "2023-04-17 13:26:33 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:26:45 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:26:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:26:45 | INFO | valid | epoch 343 | valid on 'valid' subset | loss 5.618 | nll_loss 3.84 | ppl 14.32 | wps 6189 | wpb 290.8 | bsz 1 | num_updates 10967 | best_loss 3.979\n",
      "2023-04-17 13:26:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 343 @ 10967 updates\n",
      "2023-04-17 13:26:45 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:26:58 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:26:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 343 @ 10967 updates, score 5.618) (writing took 13.245796942035668 seconds)\n",
      "2023-04-17 13:26:58 | INFO | fairseq_cli.train | end of epoch 343 (average epoch stats below)\n",
      "2023-04-17 13:26:58 | INFO | train | epoch 343 | loss 2.506 | nll_loss 0.339 | ppl 1.26 | wps 871.4 | ups 1.28 | wpb 678.5 | bsz 2 | num_updates 10967 | lr 2.60502e-05 | gnorm 3.901 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 9299\n",
      "2023-04-17 13:26:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:26:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:26:58 | INFO | fairseq.trainer | begin training epoch 344\n",
      "2023-04-17 13:26:58 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:27:09 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:27:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:27:10 | INFO | valid | epoch 344 | valid on 'valid' subset | loss 5.567 | nll_loss 3.78 | ppl 13.74 | wps 7149.8 | wpb 290.8 | bsz 1 | num_updates 10999 | best_loss 3.979\n",
      "2023-04-17 13:27:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 344 @ 10999 updates\n",
      "2023-04-17 13:27:10 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:27:22 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:27:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 344 @ 10999 updates, score 5.567) (writing took 12.984807049040683 seconds)\n",
      "2023-04-17 13:27:23 | INFO | fairseq_cli.train | end of epoch 344 (average epoch stats below)\n",
      "2023-04-17 13:27:23 | INFO | train | epoch 344 | loss 2.507 | nll_loss 0.343 | ppl 1.27 | wps 895 | ups 1.32 | wpb 678.5 | bsz 2 | num_updates 10999 | lr 2.60381e-05 | gnorm 3.822 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 9323\n",
      "2023-04-17 13:27:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:27:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:27:23 | INFO | fairseq.trainer | begin training epoch 345\n",
      "2023-04-17 13:27:23 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:27:23 | INFO | train_inner | epoch 345:      1 / 32 loss=2.506, nll_loss=0.34, ppl=1.27, wps=765.4, ups=1.13, wpb=675.2, bsz=2, num_updates=11000, lr=2.60377e-05, gnorm=3.862, clip=100, loss_scale=0.5, train_wall=35, gb_free=13.9, wall=9323\n",
      "2023-04-17 13:27:34 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:27:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:27:34 | INFO | valid | epoch 345 | valid on 'valid' subset | loss 5.584 | nll_loss 3.816 | ppl 14.08 | wps 7354.4 | wpb 290.8 | bsz 1 | num_updates 11031 | best_loss 3.979\n",
      "2023-04-17 13:27:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 345 @ 11031 updates\n",
      "2023-04-17 13:27:34 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:27:46 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:27:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 345 @ 11031 updates, score 5.584) (writing took 12.842612331034616 seconds)\n",
      "2023-04-17 13:27:47 | INFO | fairseq_cli.train | end of epoch 345 (average epoch stats below)\n",
      "2023-04-17 13:27:47 | INFO | train | epoch 345 | loss 2.505 | nll_loss 0.339 | ppl 1.26 | wps 899.8 | ups 1.33 | wpb 678.5 | bsz 2 | num_updates 11031 | lr 2.6026e-05 | gnorm 3.557 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 9347\n",
      "2023-04-17 13:27:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:27:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:27:47 | INFO | fairseq.trainer | begin training epoch 346\n",
      "2023-04-17 13:27:47 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:27:58 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:27:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:27:58 | INFO | valid | epoch 346 | valid on 'valid' subset | loss 5.61 | nll_loss 3.853 | ppl 14.45 | wps 7348.5 | wpb 290.8 | bsz 1 | num_updates 11063 | best_loss 3.979\n",
      "2023-04-17 13:27:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 346 @ 11063 updates\n",
      "2023-04-17 13:27:58 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:28:11 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:28:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 346 @ 11063 updates, score 5.61) (writing took 12.842003321973607 seconds)\n",
      "2023-04-17 13:28:11 | INFO | fairseq_cli.train | end of epoch 346 (average epoch stats below)\n",
      "2023-04-17 13:28:11 | INFO | train | epoch 346 | loss 2.508 | nll_loss 0.341 | ppl 1.27 | wps 900.1 | ups 1.33 | wpb 678.5 | bsz 2 | num_updates 11063 | lr 2.6014e-05 | gnorm 4.104 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 9371\n",
      "2023-04-17 13:28:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:28:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:28:11 | INFO | fairseq.trainer | begin training epoch 347\n",
      "2023-04-17 13:28:11 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:28:22 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:28:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 13:28:22 | INFO | valid | epoch 347 | valid on 'valid' subset | loss 5.55 | nll_loss 3.779 | ppl 13.72 | wps 7151.6 | wpb 290.8 | bsz 1 | num_updates 11095 | best_loss 3.979\n",
      "2023-04-17 13:28:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 347 @ 11095 updates\n",
      "2023-04-17 13:28:22 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:28:35 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:28:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 347 @ 11095 updates, score 5.55) (writing took 12.777172048925422 seconds)\n",
      "2023-04-17 13:28:35 | INFO | fairseq_cli.train | end of epoch 347 (average epoch stats below)\n",
      "2023-04-17 13:28:35 | INFO | train | epoch 347 | loss 2.502 | nll_loss 0.338 | ppl 1.26 | wps 902.3 | ups 1.33 | wpb 678.5 | bsz 2 | num_updates 11095 | lr 2.60019e-05 | gnorm 3.557 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 9395\n",
      "2023-04-17 13:28:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:28:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:28:35 | INFO | fairseq.trainer | begin training epoch 348\n",
      "2023-04-17 13:28:35 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:28:37 | INFO | train_inner | epoch 348:      5 / 32 loss=2.505, nll_loss=0.34, ppl=1.27, wps=930.6, ups=1.36, wpb=686.3, bsz=2, num_updates=11100, lr=2.6e-05, gnorm=3.72, clip=100, loss_scale=0.5, train_wall=34, gb_free=13.9, wall=9397\n",
      "2023-04-17 13:28:46 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:28:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:28:46 | INFO | valid | epoch 348 | valid on 'valid' subset | loss 5.591 | nll_loss 3.826 | ppl 14.18 | wps 7082.4 | wpb 290.8 | bsz 1 | num_updates 11127 | best_loss 3.979\n",
      "2023-04-17 13:28:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 348 @ 11127 updates\n",
      "2023-04-17 13:28:46 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:28:59 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:28:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 348 @ 11127 updates, score 5.591) (writing took 12.945530571974814 seconds)\n",
      "2023-04-17 13:28:59 | INFO | fairseq_cli.train | end of epoch 348 (average epoch stats below)\n",
      "2023-04-17 13:28:59 | INFO | train | epoch 348 | loss 2.502 | nll_loss 0.337 | ppl 1.26 | wps 889 | ups 1.31 | wpb 678.5 | bsz 2 | num_updates 11127 | lr 2.59898e-05 | gnorm 3.75 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 9420\n",
      "2023-04-17 13:28:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:28:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:28:59 | INFO | fairseq.trainer | begin training epoch 349\n",
      "2023-04-17 13:28:59 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:29:10 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:29:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:29:11 | INFO | valid | epoch 349 | valid on 'valid' subset | loss 5.581 | nll_loss 3.81 | ppl 14.03 | wps 7249.4 | wpb 290.8 | bsz 1 | num_updates 11159 | best_loss 3.979\n",
      "2023-04-17 13:29:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 349 @ 11159 updates\n",
      "2023-04-17 13:29:11 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:29:23 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:29:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 349 @ 11159 updates, score 5.581) (writing took 12.765698653063737 seconds)\n",
      "2023-04-17 13:29:23 | INFO | fairseq_cli.train | end of epoch 349 (average epoch stats below)\n",
      "2023-04-17 13:29:23 | INFO | train | epoch 349 | loss 2.499 | nll_loss 0.335 | ppl 1.26 | wps 898.9 | ups 1.32 | wpb 678.5 | bsz 2 | num_updates 11159 | lr 2.59777e-05 | gnorm 3.637 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 9444\n",
      "2023-04-17 13:29:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:29:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:29:23 | INFO | fairseq.trainer | begin training epoch 350\n",
      "2023-04-17 13:29:23 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:29:35 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:29:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:29:35 | INFO | valid | epoch 350 | valid on 'valid' subset | loss 5.658 | nll_loss 3.905 | ppl 14.98 | wps 7204.7 | wpb 290.8 | bsz 1 | num_updates 11191 | best_loss 3.979\n",
      "2023-04-17 13:29:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 350 @ 11191 updates\n",
      "2023-04-17 13:29:35 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:29:48 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:29:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 350 @ 11191 updates, score 5.658) (writing took 12.989457512972876 seconds)\n",
      "2023-04-17 13:29:48 | INFO | fairseq_cli.train | end of epoch 350 (average epoch stats below)\n",
      "2023-04-17 13:29:48 | INFO | train | epoch 350 | loss 2.503 | nll_loss 0.338 | ppl 1.26 | wps 893.2 | ups 1.32 | wpb 678.5 | bsz 2 | num_updates 11191 | lr 2.59657e-05 | gnorm 3.914 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 9468\n",
      "2023-04-17 13:29:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:29:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:29:48 | INFO | fairseq.trainer | begin training epoch 351\n",
      "2023-04-17 13:29:48 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:29:51 | INFO | train_inner | epoch 351:      9 / 32 loss=2.501, nll_loss=0.336, ppl=1.26, wps=905.5, ups=1.35, wpb=672.9, bsz=2, num_updates=11200, lr=2.59623e-05, gnorm=3.79, clip=100, loss_scale=0.5, train_wall=34, gb_free=13.9, wall=9471\n",
      "2023-04-17 13:29:59 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:29:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:29:59 | INFO | valid | epoch 351 | valid on 'valid' subset | loss 5.541 | nll_loss 3.759 | ppl 13.54 | wps 7295.9 | wpb 290.8 | bsz 1 | num_updates 11223 | best_loss 3.979\n",
      "2023-04-17 13:29:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 351 @ 11223 updates\n",
      "2023-04-17 13:29:59 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:30:12 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:30:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 351 @ 11223 updates, score 5.541) (writing took 12.79397559503559 seconds)\n",
      "2023-04-17 13:30:12 | INFO | fairseq_cli.train | end of epoch 351 (average epoch stats below)\n",
      "2023-04-17 13:30:12 | INFO | train | epoch 351 | loss 2.501 | nll_loss 0.335 | ppl 1.26 | wps 898.7 | ups 1.32 | wpb 678.5 | bsz 2 | num_updates 11223 | lr 2.59536e-05 | gnorm 3.883 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 9492\n",
      "2023-04-17 13:30:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:30:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:30:12 | INFO | fairseq.trainer | begin training epoch 352\n",
      "2023-04-17 13:30:12 | INFO | fairseq_cli.train | Start iterating over samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 13:30:23 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:30:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:30:23 | INFO | valid | epoch 352 | valid on 'valid' subset | loss 5.588 | nll_loss 3.827 | ppl 14.2 | wps 7274.6 | wpb 290.8 | bsz 1 | num_updates 11255 | best_loss 3.979\n",
      "2023-04-17 13:30:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 352 @ 11255 updates\n",
      "2023-04-17 13:30:23 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:30:36 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:30:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 352 @ 11255 updates, score 5.588) (writing took 13.003666214994155 seconds)\n",
      "2023-04-17 13:30:36 | INFO | fairseq_cli.train | end of epoch 352 (average epoch stats below)\n",
      "2023-04-17 13:30:36 | INFO | train | epoch 352 | loss 2.5 | nll_loss 0.334 | ppl 1.26 | wps 892 | ups 1.31 | wpb 678.5 | bsz 2 | num_updates 11255 | lr 2.59415e-05 | gnorm 3.889 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 9516\n",
      "2023-04-17 13:30:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:30:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:30:36 | INFO | fairseq.trainer | begin training epoch 353\n",
      "2023-04-17 13:30:36 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:30:47 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:30:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:30:48 | INFO | valid | epoch 353 | valid on 'valid' subset | loss 5.61 | nll_loss 3.849 | ppl 14.41 | wps 7331.3 | wpb 290.8 | bsz 1 | num_updates 11287 | best_loss 3.979\n",
      "2023-04-17 13:30:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 353 @ 11287 updates\n",
      "2023-04-17 13:30:48 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:31:00 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:31:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 353 @ 11287 updates, score 5.61) (writing took 12.77835173998028 seconds)\n",
      "2023-04-17 13:31:00 | INFO | fairseq_cli.train | end of epoch 353 (average epoch stats below)\n",
      "2023-04-17 13:31:00 | INFO | train | epoch 353 | loss 2.498 | nll_loss 0.334 | ppl 1.26 | wps 900.9 | ups 1.33 | wpb 678.5 | bsz 2 | num_updates 11287 | lr 2.59294e-05 | gnorm 3.534 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 9541\n",
      "2023-04-17 13:31:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:31:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:31:00 | INFO | fairseq.trainer | begin training epoch 354\n",
      "2023-04-17 13:31:00 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:31:05 | INFO | train_inner | epoch 354:     13 / 32 loss=2.5, nll_loss=0.335, ppl=1.26, wps=927, ups=1.35, wpb=685.7, bsz=2, num_updates=11300, lr=2.59245e-05, gnorm=3.746, clip=100, loss_scale=0.5, train_wall=34, gb_free=13.9, wall=9545\n",
      "2023-04-17 13:31:11 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:31:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:31:12 | INFO | valid | epoch 354 | valid on 'valid' subset | loss 5.654 | nll_loss 3.895 | ppl 14.87 | wps 7261.2 | wpb 290.8 | bsz 1 | num_updates 11319 | best_loss 3.979\n",
      "2023-04-17 13:31:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 354 @ 11319 updates\n",
      "2023-04-17 13:31:12 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:31:24 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:31:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 354 @ 11319 updates, score 5.654) (writing took 12.93307550100144 seconds)\n",
      "2023-04-17 13:31:25 | INFO | fairseq_cli.train | end of epoch 354 (average epoch stats below)\n",
      "2023-04-17 13:31:25 | INFO | train | epoch 354 | loss 2.5 | nll_loss 0.335 | ppl 1.26 | wps 896.3 | ups 1.32 | wpb 678.5 | bsz 2 | num_updates 11319 | lr 2.59174e-05 | gnorm 3.684 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 9565\n",
      "2023-04-17 13:31:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:31:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:31:25 | INFO | fairseq.trainer | begin training epoch 355\n",
      "2023-04-17 13:31:25 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:31:36 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:31:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:31:36 | INFO | valid | epoch 355 | valid on 'valid' subset | loss 5.645 | nll_loss 3.875 | ppl 14.68 | wps 7230.1 | wpb 290.8 | bsz 1 | num_updates 11351 | best_loss 3.979\n",
      "2023-04-17 13:31:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 355 @ 11351 updates\n",
      "2023-04-17 13:31:36 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:31:49 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:31:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 355 @ 11351 updates, score 5.645) (writing took 12.924002435058355 seconds)\n",
      "2023-04-17 13:31:49 | INFO | fairseq_cli.train | end of epoch 355 (average epoch stats below)\n",
      "2023-04-17 13:31:49 | INFO | train | epoch 355 | loss 2.501 | nll_loss 0.338 | ppl 1.26 | wps 892 | ups 1.31 | wpb 678.5 | bsz 2 | num_updates 11351 | lr 2.59053e-05 | gnorm 3.809 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 9589\n",
      "2023-04-17 13:31:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:31:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:31:49 | INFO | fairseq.trainer | begin training epoch 356\n",
      "2023-04-17 13:31:49 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:32:00 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:32:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:32:00 | INFO | valid | epoch 356 | valid on 'valid' subset | loss 5.608 | nll_loss 3.851 | ppl 14.43 | wps 7329.2 | wpb 290.8 | bsz 1 | num_updates 11383 | best_loss 3.979\n",
      "2023-04-17 13:32:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 356 @ 11383 updates\n",
      "2023-04-17 13:32:00 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:32:13 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:32:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 356 @ 11383 updates, score 5.608) (writing took 12.876830248977058 seconds)\n",
      "2023-04-17 13:32:13 | INFO | fairseq_cli.train | end of epoch 356 (average epoch stats below)\n",
      "2023-04-17 13:32:13 | INFO | train | epoch 356 | loss 2.493 | nll_loss 0.326 | ppl 1.25 | wps 896.7 | ups 1.32 | wpb 678.5 | bsz 2 | num_updates 11383 | lr 2.58932e-05 | gnorm 3.531 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 9613\n",
      "2023-04-17 13:32:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:32:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:32:13 | INFO | fairseq.trainer | begin training epoch 357\n",
      "2023-04-17 13:32:13 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:32:19 | INFO | train_inner | epoch 357:     17 / 32 loss=2.497, nll_loss=0.332, ppl=1.26, wps=900.6, ups=1.35, wpb=668.3, bsz=2, num_updates=11400, lr=2.58868e-05, gnorm=3.675, clip=100, loss_scale=0.5, train_wall=34, gb_free=13.9, wall=9619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 13:32:24 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:32:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:32:24 | INFO | valid | epoch 357 | valid on 'valid' subset | loss 5.613 | nll_loss 3.854 | ppl 14.46 | wps 7109.4 | wpb 290.8 | bsz 1 | num_updates 11415 | best_loss 3.979\n",
      "2023-04-17 13:32:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 357 @ 11415 updates\n",
      "2023-04-17 13:32:24 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:32:37 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:32:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 357 @ 11415 updates, score 5.613) (writing took 13.049197206972167 seconds)\n",
      "2023-04-17 13:32:38 | INFO | fairseq_cli.train | end of epoch 357 (average epoch stats below)\n",
      "2023-04-17 13:32:38 | INFO | train | epoch 357 | loss 2.49 | nll_loss 0.325 | ppl 1.25 | wps 889.5 | ups 1.31 | wpb 678.5 | bsz 2 | num_updates 11415 | lr 2.58811e-05 | gnorm 3.506 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 9638\n",
      "2023-04-17 13:32:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:32:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:32:38 | INFO | fairseq.trainer | begin training epoch 358\n",
      "2023-04-17 13:32:38 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:32:49 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:32:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:32:49 | INFO | valid | epoch 358 | valid on 'valid' subset | loss 5.643 | nll_loss 3.887 | ppl 14.79 | wps 7043.7 | wpb 290.8 | bsz 1 | num_updates 11447 | best_loss 3.979\n",
      "2023-04-17 13:32:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 358 @ 11447 updates\n",
      "2023-04-17 13:32:49 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:33:02 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:33:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 358 @ 11447 updates, score 5.643) (writing took 13.001162475091405 seconds)\n",
      "2023-04-17 13:33:02 | INFO | fairseq_cli.train | end of epoch 358 (average epoch stats below)\n",
      "2023-04-17 13:33:02 | INFO | train | epoch 358 | loss 2.486 | nll_loss 0.321 | ppl 1.25 | wps 890.7 | ups 1.31 | wpb 678.5 | bsz 2 | num_updates 11447 | lr 2.58691e-05 | gnorm 3.414 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 9662\n",
      "2023-04-17 13:33:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:33:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:33:02 | INFO | fairseq.trainer | begin training epoch 359\n",
      "2023-04-17 13:33:02 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:33:14 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:33:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:33:14 | INFO | valid | epoch 359 | valid on 'valid' subset | loss 5.611 | nll_loss 3.856 | ppl 14.48 | wps 6262.3 | wpb 290.8 | bsz 1 | num_updates 11479 | best_loss 3.979\n",
      "2023-04-17 13:33:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 359 @ 11479 updates\n",
      "2023-04-17 13:33:14 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:33:27 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:33:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 359 @ 11479 updates, score 5.611) (writing took 12.85745552496519 seconds)\n",
      "2023-04-17 13:33:27 | INFO | fairseq_cli.train | end of epoch 359 (average epoch stats below)\n",
      "2023-04-17 13:33:27 | INFO | train | epoch 359 | loss 2.494 | nll_loss 0.33 | ppl 1.26 | wps 868.5 | ups 1.28 | wpb 678.5 | bsz 2 | num_updates 11479 | lr 2.5857e-05 | gnorm 3.458 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 9687\n",
      "2023-04-17 13:33:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:33:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:33:27 | INFO | fairseq.trainer | begin training epoch 360\n",
      "2023-04-17 13:33:27 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:33:34 | INFO | train_inner | epoch 360:     21 / 32 loss=2.49, nll_loss=0.325, ppl=1.25, wps=908.7, ups=1.33, wpb=683.4, bsz=2, num_updates=11500, lr=2.58491e-05, gnorm=3.461, clip=100, loss_scale=0.5, train_wall=35, gb_free=13.9, wall=9695\n",
      "2023-04-17 13:33:38 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:33:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:33:38 | INFO | valid | epoch 360 | valid on 'valid' subset | loss 5.69 | nll_loss 3.942 | ppl 15.37 | wps 7079.6 | wpb 290.8 | bsz 1 | num_updates 11511 | best_loss 3.979\n",
      "2023-04-17 13:33:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 360 @ 11511 updates\n",
      "2023-04-17 13:33:38 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:33:51 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:33:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 360 @ 11511 updates, score 5.69) (writing took 12.95855662599206 seconds)\n",
      "2023-04-17 13:33:51 | INFO | fairseq_cli.train | end of epoch 360 (average epoch stats below)\n",
      "2023-04-17 13:33:51 | INFO | train | epoch 360 | loss 2.493 | nll_loss 0.329 | ppl 1.26 | wps 892.7 | ups 1.32 | wpb 678.5 | bsz 2 | num_updates 11511 | lr 2.58449e-05 | gnorm 4.407 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 9711\n",
      "2023-04-17 13:33:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:33:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:33:51 | INFO | fairseq.trainer | begin training epoch 361\n",
      "2023-04-17 13:33:51 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:34:02 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:34:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:34:03 | INFO | valid | epoch 361 | valid on 'valid' subset | loss 5.665 | nll_loss 3.917 | ppl 15.1 | wps 7002.7 | wpb 290.8 | bsz 1 | num_updates 11543 | best_loss 3.979\n",
      "2023-04-17 13:34:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 361 @ 11543 updates\n",
      "2023-04-17 13:34:03 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:34:15 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:34:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 361 @ 11543 updates, score 5.665) (writing took 12.85861744300928 seconds)\n",
      "2023-04-17 13:34:15 | INFO | fairseq_cli.train | end of epoch 361 (average epoch stats below)\n",
      "2023-04-17 13:34:15 | INFO | train | epoch 361 | loss 2.486 | nll_loss 0.319 | ppl 1.25 | wps 897 | ups 1.32 | wpb 678.5 | bsz 2 | num_updates 11543 | lr 2.58328e-05 | gnorm 4.572 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 9736\n",
      "2023-04-17 13:34:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:34:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:34:15 | INFO | fairseq.trainer | begin training epoch 362\n",
      "2023-04-17 13:34:15 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:34:27 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:34:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 13:34:27 | INFO | valid | epoch 362 | valid on 'valid' subset | loss 5.71 | nll_loss 3.952 | ppl 15.48 | wps 7317.5 | wpb 290.8 | bsz 1 | num_updates 11575 | best_loss 3.979\n",
      "2023-04-17 13:34:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 362 @ 11575 updates\n",
      "2023-04-17 13:34:27 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:34:40 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:34:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 362 @ 11575 updates, score 5.71) (writing took 13.07487735291943 seconds)\n",
      "2023-04-17 13:34:40 | INFO | fairseq_cli.train | end of epoch 362 (average epoch stats below)\n",
      "2023-04-17 13:34:40 | INFO | train | epoch 362 | loss 2.486 | nll_loss 0.32 | ppl 1.25 | wps 891.1 | ups 1.31 | wpb 678.5 | bsz 2 | num_updates 11575 | lr 2.58208e-05 | gnorm 3.771 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 9760\n",
      "2023-04-17 13:34:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:34:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:34:40 | INFO | fairseq.trainer | begin training epoch 363\n",
      "2023-04-17 13:34:40 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:34:49 | INFO | train_inner | epoch 363:     25 / 32 loss=2.488, nll_loss=0.322, ppl=1.25, wps=913, ups=1.35, wpb=678.3, bsz=2, num_updates=11600, lr=2.58113e-05, gnorm=4.28, clip=100, loss_scale=0.5, train_wall=34, gb_free=13.9, wall=9769\n",
      "2023-04-17 13:34:51 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:34:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:34:51 | INFO | valid | epoch 363 | valid on 'valid' subset | loss 5.646 | nll_loss 3.891 | ppl 14.83 | wps 7310.8 | wpb 290.8 | bsz 1 | num_updates 11607 | best_loss 3.979\n",
      "2023-04-17 13:34:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 363 @ 11607 updates\n",
      "2023-04-17 13:34:51 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:35:04 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:35:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 363 @ 11607 updates, score 5.646) (writing took 12.836495113093406 seconds)\n",
      "2023-04-17 13:35:04 | INFO | fairseq_cli.train | end of epoch 363 (average epoch stats below)\n",
      "2023-04-17 13:35:04 | INFO | train | epoch 363 | loss 2.487 | nll_loss 0.321 | ppl 1.25 | wps 899 | ups 1.32 | wpb 678.5 | bsz 2 | num_updates 11607 | lr 2.58087e-05 | gnorm 3.681 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 9784\n",
      "2023-04-17 13:35:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:35:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:35:04 | INFO | fairseq.trainer | begin training epoch 364\n",
      "2023-04-17 13:35:04 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:35:15 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:35:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:35:15 | INFO | valid | epoch 364 | valid on 'valid' subset | loss 5.718 | nll_loss 3.978 | ppl 15.75 | wps 7056.5 | wpb 290.8 | bsz 1 | num_updates 11639 | best_loss 3.979\n",
      "2023-04-17 13:35:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 364 @ 11639 updates\n",
      "2023-04-17 13:35:15 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:35:28 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:35:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 364 @ 11639 updates, score 5.718) (writing took 12.825212609954178 seconds)\n",
      "2023-04-17 13:35:28 | INFO | fairseq_cli.train | end of epoch 364 (average epoch stats below)\n",
      "2023-04-17 13:35:28 | INFO | train | epoch 364 | loss 2.49 | nll_loss 0.323 | ppl 1.25 | wps 895.3 | ups 1.32 | wpb 678.5 | bsz 2 | num_updates 11639 | lr 2.57966e-05 | gnorm 4.124 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 9808\n",
      "2023-04-17 13:35:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:35:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:35:28 | INFO | fairseq.trainer | begin training epoch 365\n",
      "2023-04-17 13:35:28 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:35:39 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:35:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:35:40 | INFO | valid | epoch 365 | valid on 'valid' subset | loss 5.643 | nll_loss 3.887 | ppl 14.8 | wps 7343.9 | wpb 290.8 | bsz 1 | num_updates 11671 | best_loss 3.979\n",
      "2023-04-17 13:35:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 365 @ 11671 updates\n",
      "2023-04-17 13:35:40 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:35:52 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:35:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 365 @ 11671 updates, score 5.643) (writing took 12.636045770021155 seconds)\n",
      "2023-04-17 13:35:52 | INFO | fairseq_cli.train | end of epoch 365 (average epoch stats below)\n",
      "2023-04-17 13:35:52 | INFO | train | epoch 365 | loss 2.493 | nll_loss 0.329 | ppl 1.26 | wps 907.2 | ups 1.34 | wpb 678.5 | bsz 2 | num_updates 11671 | lr 2.57845e-05 | gnorm 3.842 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 9832\n",
      "2023-04-17 13:35:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:35:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:35:52 | INFO | fairseq.trainer | begin training epoch 366\n",
      "2023-04-17 13:35:52 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:36:02 | INFO | train_inner | epoch 366:     29 / 32 loss=2.488, nll_loss=0.324, ppl=1.25, wps=923.2, ups=1.36, wpb=680.2, bsz=2, num_updates=11700, lr=2.57736e-05, gnorm=3.826, clip=100, loss_scale=0.5, train_wall=34, gb_free=13.9, wall=9843\n",
      "2023-04-17 13:36:03 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:36:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:36:03 | INFO | valid | epoch 366 | valid on 'valid' subset | loss 5.672 | nll_loss 3.92 | ppl 15.14 | wps 7349.9 | wpb 290.8 | bsz 1 | num_updates 11703 | best_loss 3.979\n",
      "2023-04-17 13:36:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 366 @ 11703 updates\n",
      "2023-04-17 13:36:03 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:36:16 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:36:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 366 @ 11703 updates, score 5.672) (writing took 12.988660800969228 seconds)\n",
      "2023-04-17 13:36:16 | INFO | fairseq_cli.train | end of epoch 366 (average epoch stats below)\n",
      "2023-04-17 13:36:16 | INFO | train | epoch 366 | loss 2.486 | nll_loss 0.321 | ppl 1.25 | wps 894.1 | ups 1.32 | wpb 678.5 | bsz 2 | num_updates 11703 | lr 2.57725e-05 | gnorm 3.723 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 9857\n",
      "2023-04-17 13:36:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:36:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:36:16 | INFO | fairseq.trainer | begin training epoch 367\n",
      "2023-04-17 13:36:16 | INFO | fairseq_cli.train | Start iterating over samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 13:36:28 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:36:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:36:28 | INFO | valid | epoch 367 | valid on 'valid' subset | loss 5.712 | nll_loss 3.947 | ppl 15.43 | wps 7319.2 | wpb 290.8 | bsz 1 | num_updates 11735 | best_loss 3.979\n",
      "2023-04-17 13:36:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 367 @ 11735 updates\n",
      "2023-04-17 13:36:28 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:36:41 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:36:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 367 @ 11735 updates, score 5.712) (writing took 12.723502507898957 seconds)\n",
      "2023-04-17 13:36:41 | INFO | fairseq_cli.train | end of epoch 367 (average epoch stats below)\n",
      "2023-04-17 13:36:41 | INFO | train | epoch 367 | loss 2.483 | nll_loss 0.32 | ppl 1.25 | wps 894.6 | ups 1.32 | wpb 678.5 | bsz 2 | num_updates 11735 | lr 2.57604e-05 | gnorm 3.387 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 9881\n",
      "2023-04-17 13:36:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:36:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:36:41 | INFO | fairseq.trainer | begin training epoch 368\n",
      "2023-04-17 13:36:41 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:36:52 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:36:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:36:52 | INFO | valid | epoch 368 | valid on 'valid' subset | loss 5.667 | nll_loss 3.92 | ppl 15.14 | wps 7075.5 | wpb 290.8 | bsz 1 | num_updates 11767 | best_loss 3.979\n",
      "2023-04-17 13:36:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 368 @ 11767 updates\n",
      "2023-04-17 13:36:52 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:37:05 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:37:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 368 @ 11767 updates, score 5.667) (writing took 13.032229921896942 seconds)\n",
      "2023-04-17 13:37:05 | INFO | fairseq_cli.train | end of epoch 368 (average epoch stats below)\n",
      "2023-04-17 13:37:05 | INFO | train | epoch 368 | loss 2.484 | nll_loss 0.318 | ppl 1.25 | wps 888.9 | ups 1.31 | wpb 678.5 | bsz 2 | num_updates 11767 | lr 2.57483e-05 | gnorm 3.681 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 9905\n",
      "2023-04-17 13:37:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:37:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:37:05 | INFO | fairseq.trainer | begin training epoch 369\n",
      "2023-04-17 13:37:05 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:37:16 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:37:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:37:17 | INFO | valid | epoch 369 | valid on 'valid' subset | loss 5.637 | nll_loss 3.885 | ppl 14.77 | wps 7297.1 | wpb 290.8 | bsz 1 | num_updates 11799 | best_loss 3.979\n",
      "2023-04-17 13:37:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 369 @ 11799 updates\n",
      "2023-04-17 13:37:17 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:37:29 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:37:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 369 @ 11799 updates, score 5.637) (writing took 12.413467988953926 seconds)\n",
      "2023-04-17 13:37:29 | INFO | fairseq_cli.train | end of epoch 369 (average epoch stats below)\n",
      "2023-04-17 13:37:29 | INFO | train | epoch 369 | loss 2.48 | nll_loss 0.316 | ppl 1.24 | wps 912.4 | ups 1.34 | wpb 678.5 | bsz 2 | num_updates 11799 | lr 2.57362e-05 | gnorm 3.441 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 9929\n",
      "2023-04-17 13:37:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:37:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:37:29 | INFO | fairseq.trainer | begin training epoch 370\n",
      "2023-04-17 13:37:29 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:37:29 | INFO | train_inner | epoch 370:      1 / 32 loss=2.483, nll_loss=0.318, ppl=1.25, wps=774.8, ups=1.15, wpb=674.2, bsz=2, num_updates=11800, lr=2.57358e-05, gnorm=3.553, clip=100, loss_scale=0.5, train_wall=35, gb_free=13.9, wall=9930\n",
      "2023-04-17 13:37:40 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:37:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:37:40 | INFO | valid | epoch 370 | valid on 'valid' subset | loss 5.756 | nll_loss 4.009 | ppl 16.1 | wps 6984.6 | wpb 290.8 | bsz 1 | num_updates 11831 | best_loss 3.979\n",
      "2023-04-17 13:37:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 370 @ 11831 updates\n",
      "2023-04-17 13:37:40 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:37:53 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:37:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 370 @ 11831 updates, score 5.756) (writing took 13.077652755076997 seconds)\n",
      "2023-04-17 13:37:53 | INFO | fairseq_cli.train | end of epoch 370 (average epoch stats below)\n",
      "2023-04-17 13:37:53 | INFO | train | epoch 370 | loss 2.477 | nll_loss 0.312 | ppl 1.24 | wps 890.9 | ups 1.31 | wpb 678.5 | bsz 2 | num_updates 11831 | lr 2.57242e-05 | gnorm 3.762 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 9954\n",
      "2023-04-17 13:37:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:37:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:37:53 | INFO | fairseq.trainer | begin training epoch 371\n",
      "2023-04-17 13:37:53 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:38:04 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:38:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:38:05 | INFO | valid | epoch 371 | valid on 'valid' subset | loss 5.657 | nll_loss 3.905 | ppl 14.98 | wps 7272.6 | wpb 290.8 | bsz 1 | num_updates 11863 | best_loss 3.979\n",
      "2023-04-17 13:38:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 371 @ 11863 updates\n",
      "2023-04-17 13:38:05 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:38:17 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:38:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 371 @ 11863 updates, score 5.657) (writing took 12.872393982019275 seconds)\n",
      "2023-04-17 13:38:17 | INFO | fairseq_cli.train | end of epoch 371 (average epoch stats below)\n",
      "2023-04-17 13:38:17 | INFO | train | epoch 371 | loss 2.486 | nll_loss 0.322 | ppl 1.25 | wps 899 | ups 1.32 | wpb 678.5 | bsz 2 | num_updates 11863 | lr 2.57121e-05 | gnorm 3.709 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 9978\n",
      "2023-04-17 13:38:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:38:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:38:17 | INFO | fairseq.trainer | begin training epoch 372\n",
      "2023-04-17 13:38:17 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:38:29 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:38:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 13:38:29 | INFO | valid | epoch 372 | valid on 'valid' subset | loss 5.717 | nll_loss 3.983 | ppl 15.82 | wps 7274.6 | wpb 290.8 | bsz 1 | num_updates 11895 | best_loss 3.979\n",
      "2023-04-17 13:38:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 372 @ 11895 updates\n",
      "2023-04-17 13:38:29 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:38:42 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:38:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 372 @ 11895 updates, score 5.717) (writing took 12.951887464965694 seconds)\n",
      "2023-04-17 13:38:42 | INFO | fairseq_cli.train | end of epoch 372 (average epoch stats below)\n",
      "2023-04-17 13:38:42 | INFO | train | epoch 372 | loss 2.481 | nll_loss 0.318 | ppl 1.25 | wps 894.6 | ups 1.32 | wpb 678.5 | bsz 2 | num_updates 11895 | lr 2.57e-05 | gnorm 3.623 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 10002\n",
      "2023-04-17 13:38:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:38:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:38:42 | INFO | fairseq.trainer | begin training epoch 373\n",
      "2023-04-17 13:38:42 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:38:44 | INFO | train_inner | epoch 373:      5 / 32 loss=2.48, nll_loss=0.316, ppl=1.25, wps=911.9, ups=1.35, wpb=676.4, bsz=2, num_updates=11900, lr=2.56981e-05, gnorm=3.682, clip=100, loss_scale=0.5, train_wall=34, gb_free=13.9, wall=10004\n",
      "2023-04-17 13:38:53 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:38:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:38:53 | INFO | valid | epoch 373 | valid on 'valid' subset | loss 5.671 | nll_loss 3.92 | ppl 15.14 | wps 7268 | wpb 290.8 | bsz 1 | num_updates 11927 | best_loss 3.979\n",
      "2023-04-17 13:38:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 373 @ 11927 updates\n",
      "2023-04-17 13:38:53 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:39:06 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:39:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 373 @ 11927 updates, score 5.671) (writing took 12.836706534959376 seconds)\n",
      "2023-04-17 13:39:06 | INFO | fairseq_cli.train | end of epoch 373 (average epoch stats below)\n",
      "2023-04-17 13:39:06 | INFO | train | epoch 373 | loss 2.48 | nll_loss 0.316 | ppl 1.24 | wps 900.2 | ups 1.33 | wpb 678.5 | bsz 2 | num_updates 11927 | lr 2.56879e-05 | gnorm 3.547 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 10026\n",
      "2023-04-17 13:39:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:39:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:39:06 | INFO | fairseq.trainer | begin training epoch 374\n",
      "2023-04-17 13:39:06 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:39:17 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:39:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:39:17 | INFO | valid | epoch 374 | valid on 'valid' subset | loss 5.702 | nll_loss 3.966 | ppl 15.62 | wps 7111 | wpb 290.8 | bsz 1 | num_updates 11959 | best_loss 3.979\n",
      "2023-04-17 13:39:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 374 @ 11959 updates\n",
      "2023-04-17 13:39:17 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:39:30 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:39:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 374 @ 11959 updates, score 5.702) (writing took 12.963582216994837 seconds)\n",
      "2023-04-17 13:39:30 | INFO | fairseq_cli.train | end of epoch 374 (average epoch stats below)\n",
      "2023-04-17 13:39:30 | INFO | train | epoch 374 | loss 2.476 | nll_loss 0.312 | ppl 1.24 | wps 891.9 | ups 1.31 | wpb 678.5 | bsz 2 | num_updates 11959 | lr 2.56758e-05 | gnorm 3.516 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 10050\n",
      "2023-04-17 13:39:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:39:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:39:30 | INFO | fairseq.trainer | begin training epoch 375\n",
      "2023-04-17 13:39:30 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:39:41 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:39:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:39:42 | INFO | valid | epoch 375 | valid on 'valid' subset | loss 5.687 | nll_loss 3.936 | ppl 15.31 | wps 7244.5 | wpb 290.8 | bsz 1 | num_updates 11991 | best_loss 3.979\n",
      "2023-04-17 13:39:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 375 @ 11991 updates\n",
      "2023-04-17 13:39:42 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:39:54 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:39:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 375 @ 11991 updates, score 5.687) (writing took 12.82914385991171 seconds)\n",
      "2023-04-17 13:39:54 | INFO | fairseq_cli.train | end of epoch 375 (average epoch stats below)\n",
      "2023-04-17 13:39:54 | INFO | train | epoch 375 | loss 2.483 | nll_loss 0.322 | ppl 1.25 | wps 897.4 | ups 1.32 | wpb 678.5 | bsz 2 | num_updates 11991 | lr 2.56638e-05 | gnorm 3.743 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 10075\n",
      "2023-04-17 13:39:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:39:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:39:54 | INFO | fairseq.trainer | begin training epoch 376\n",
      "2023-04-17 13:39:54 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:39:58 | INFO | train_inner | epoch 376:      9 / 32 loss=2.48, nll_loss=0.316, ppl=1.25, wps=910.9, ups=1.35, wpb=674.8, bsz=2, num_updates=12000, lr=2.56604e-05, gnorm=3.625, clip=100, loss_scale=0.5, train_wall=34, gb_free=13.9, wall=10078\n",
      "2023-04-17 13:40:06 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:40:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:40:06 | INFO | valid | epoch 376 | valid on 'valid' subset | loss 5.661 | nll_loss 3.901 | ppl 14.94 | wps 7292.2 | wpb 290.8 | bsz 1 | num_updates 12023 | best_loss 3.979\n",
      "2023-04-17 13:40:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 376 @ 12023 updates\n",
      "2023-04-17 13:40:06 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:40:18 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:40:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 376 @ 12023 updates, score 5.661) (writing took 12.804923115065321 seconds)\n",
      "2023-04-17 13:40:19 | INFO | fairseq_cli.train | end of epoch 376 (average epoch stats below)\n",
      "2023-04-17 13:40:19 | INFO | train | epoch 376 | loss 2.479 | nll_loss 0.313 | ppl 1.24 | wps 893.1 | ups 1.32 | wpb 678.5 | bsz 2 | num_updates 12023 | lr 2.56517e-05 | gnorm 3.683 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 10099\n",
      "2023-04-17 13:40:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:40:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:40:19 | INFO | fairseq.trainer | begin training epoch 377\n",
      "2023-04-17 13:40:19 | INFO | fairseq_cli.train | Start iterating over samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 13:40:30 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:40:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:40:30 | INFO | valid | epoch 377 | valid on 'valid' subset | loss 5.737 | nll_loss 3.985 | ppl 15.83 | wps 7346.6 | wpb 290.8 | bsz 1 | num_updates 12055 | best_loss 3.979\n",
      "2023-04-17 13:40:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 377 @ 12055 updates\n",
      "2023-04-17 13:40:30 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:40:43 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:40:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 377 @ 12055 updates, score 5.737) (writing took 12.844668759964406 seconds)\n",
      "2023-04-17 13:40:43 | INFO | fairseq_cli.train | end of epoch 377 (average epoch stats below)\n",
      "2023-04-17 13:40:43 | INFO | train | epoch 377 | loss 2.469 | nll_loss 0.304 | ppl 1.23 | wps 894.5 | ups 1.32 | wpb 678.5 | bsz 2 | num_updates 12055 | lr 2.56396e-05 | gnorm 3.374 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 10123\n",
      "2023-04-17 13:40:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:40:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:40:43 | INFO | fairseq.trainer | begin training epoch 378\n",
      "2023-04-17 13:40:43 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:40:54 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:40:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:40:54 | INFO | valid | epoch 378 | valid on 'valid' subset | loss 5.648 | nll_loss 3.884 | ppl 14.76 | wps 7066.6 | wpb 290.8 | bsz 1 | num_updates 12087 | best_loss 3.979\n",
      "2023-04-17 13:40:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 378 @ 12087 updates\n",
      "2023-04-17 13:40:54 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:41:07 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:41:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 378 @ 12087 updates, score 5.648) (writing took 13.121416346984915 seconds)\n",
      "2023-04-17 13:41:07 | INFO | fairseq_cli.train | end of epoch 378 (average epoch stats below)\n",
      "2023-04-17 13:41:07 | INFO | train | epoch 378 | loss 2.475 | nll_loss 0.311 | ppl 1.24 | wps 885.8 | ups 1.31 | wpb 678.5 | bsz 2 | num_updates 12087 | lr 2.56275e-05 | gnorm 3.476 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 10148\n",
      "2023-04-17 13:41:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:41:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:41:07 | INFO | fairseq.trainer | begin training epoch 379\n",
      "2023-04-17 13:41:07 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:41:12 | INFO | train_inner | epoch 379:     13 / 32 loss=2.472, nll_loss=0.308, ppl=1.24, wps=915.4, ups=1.34, wpb=682.2, bsz=2, num_updates=12100, lr=2.56226e-05, gnorm=3.487, clip=100, loss_scale=0.5, train_wall=35, gb_free=13.9, wall=10152\n",
      "2023-04-17 13:41:19 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:41:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:41:19 | INFO | valid | epoch 379 | valid on 'valid' subset | loss 5.7 | nll_loss 3.945 | ppl 15.4 | wps 7188.5 | wpb 290.8 | bsz 1 | num_updates 12119 | best_loss 3.979\n",
      "2023-04-17 13:41:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 379 @ 12119 updates\n",
      "2023-04-17 13:41:19 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:41:32 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:41:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 379 @ 12119 updates, score 5.7) (writing took 12.874605898978189 seconds)\n",
      "2023-04-17 13:41:32 | INFO | fairseq_cli.train | end of epoch 379 (average epoch stats below)\n",
      "2023-04-17 13:41:32 | INFO | train | epoch 379 | loss 2.474 | nll_loss 0.31 | ppl 1.24 | wps 895.3 | ups 1.32 | wpb 678.5 | bsz 2 | num_updates 12119 | lr 2.56155e-05 | gnorm 3.66 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 10172\n",
      "2023-04-17 13:41:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:41:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:41:32 | INFO | fairseq.trainer | begin training epoch 380\n",
      "2023-04-17 13:41:32 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:41:43 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:41:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:41:43 | INFO | valid | epoch 380 | valid on 'valid' subset | loss 5.669 | nll_loss 3.914 | ppl 15.07 | wps 7289.9 | wpb 290.8 | bsz 1 | num_updates 12151 | best_loss 3.979\n",
      "2023-04-17 13:41:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 380 @ 12151 updates\n",
      "2023-04-17 13:41:43 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:41:56 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:41:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 380 @ 12151 updates, score 5.669) (writing took 12.966556841973215 seconds)\n",
      "2023-04-17 13:41:56 | INFO | fairseq_cli.train | end of epoch 380 (average epoch stats below)\n",
      "2023-04-17 13:41:56 | INFO | train | epoch 380 | loss 2.476 | nll_loss 0.311 | ppl 1.24 | wps 893.6 | ups 1.32 | wpb 678.5 | bsz 2 | num_updates 12151 | lr 2.56034e-05 | gnorm 3.549 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 10196\n",
      "2023-04-17 13:41:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:41:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:41:56 | INFO | fairseq.trainer | begin training epoch 381\n",
      "2023-04-17 13:41:56 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:42:07 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:42:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:42:07 | INFO | valid | epoch 381 | valid on 'valid' subset | loss 5.648 | nll_loss 3.885 | ppl 14.77 | wps 7262.1 | wpb 290.8 | bsz 1 | num_updates 12183 | best_loss 3.979\n",
      "2023-04-17 13:42:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 381 @ 12183 updates\n",
      "2023-04-17 13:42:07 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:42:20 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:42:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 381 @ 12183 updates, score 5.648) (writing took 12.847044738009572 seconds)\n",
      "2023-04-17 13:42:20 | INFO | fairseq_cli.train | end of epoch 381 (average epoch stats below)\n",
      "2023-04-17 13:42:20 | INFO | train | epoch 381 | loss 2.475 | nll_loss 0.314 | ppl 1.24 | wps 898 | ups 1.32 | wpb 678.5 | bsz 2 | num_updates 12183 | lr 2.55913e-05 | gnorm 3.685 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 10220\n",
      "2023-04-17 13:42:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:42:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:42:20 | INFO | fairseq.trainer | begin training epoch 382\n",
      "2023-04-17 13:42:20 | INFO | fairseq_cli.train | Start iterating over samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 13:42:26 | INFO | train_inner | epoch 382:     17 / 32 loss=2.475, nll_loss=0.311, ppl=1.24, wps=913.4, ups=1.35, wpb=677.2, bsz=2, num_updates=12200, lr=2.55849e-05, gnorm=3.595, clip=100, loss_scale=0.5, train_wall=34, gb_free=13.9, wall=10227\n",
      "2023-04-17 13:42:31 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:42:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:42:32 | INFO | valid | epoch 382 | valid on 'valid' subset | loss 5.667 | nll_loss 3.916 | ppl 15.09 | wps 7245.4 | wpb 290.8 | bsz 1 | num_updates 12215 | best_loss 3.979\n",
      "2023-04-17 13:42:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 382 @ 12215 updates\n",
      "2023-04-17 13:42:32 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:42:44 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:42:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 382 @ 12215 updates, score 5.667) (writing took 12.925522361998446 seconds)\n",
      "2023-04-17 13:42:44 | INFO | fairseq_cli.train | end of epoch 382 (average epoch stats below)\n",
      "2023-04-17 13:42:44 | INFO | train | epoch 382 | loss 2.472 | nll_loss 0.308 | ppl 1.24 | wps 895.8 | ups 1.32 | wpb 678.5 | bsz 2 | num_updates 12215 | lr 2.55792e-05 | gnorm 3.402 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 10245\n",
      "2023-04-17 13:42:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:42:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:42:44 | INFO | fairseq.trainer | begin training epoch 383\n",
      "2023-04-17 13:42:44 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:42:56 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:42:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:42:56 | INFO | valid | epoch 383 | valid on 'valid' subset | loss 5.723 | nll_loss 3.965 | ppl 15.61 | wps 7315.1 | wpb 290.8 | bsz 1 | num_updates 12247 | best_loss 3.979\n",
      "2023-04-17 13:42:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 383 @ 12247 updates\n",
      "2023-04-17 13:42:56 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:43:05 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:43:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 383 @ 12247 updates, score 5.723) (writing took 8.928378232056275 seconds)\n",
      "2023-04-17 13:43:05 | INFO | fairseq_cli.train | end of epoch 383 (average epoch stats below)\n",
      "2023-04-17 13:43:05 | INFO | train | epoch 383 | loss 2.479 | nll_loss 0.314 | ppl 1.24 | wps 1071.2 | ups 1.58 | wpb 678.5 | bsz 2 | num_updates 12247 | lr 2.55672e-05 | gnorm 3.729 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 10265\n",
      "2023-04-17 13:43:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:43:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:43:05 | INFO | fairseq.trainer | begin training epoch 384\n",
      "2023-04-17 13:43:05 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:43:16 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:43:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:43:16 | INFO | valid | epoch 384 | valid on 'valid' subset | loss 5.719 | nll_loss 3.98 | ppl 15.78 | wps 7376.4 | wpb 290.8 | bsz 1 | num_updates 12279 | best_loss 3.979\n",
      "2023-04-17 13:43:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 384 @ 12279 updates\n",
      "2023-04-17 13:43:16 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:43:29 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:43:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 384 @ 12279 updates, score 5.719) (writing took 12.967892091954127 seconds)\n",
      "2023-04-17 13:43:29 | INFO | fairseq_cli.train | end of epoch 384 (average epoch stats below)\n",
      "2023-04-17 13:43:29 | INFO | train | epoch 384 | loss 2.475 | nll_loss 0.312 | ppl 1.24 | wps 890.6 | ups 1.31 | wpb 678.5 | bsz 2 | num_updates 12279 | lr 2.55551e-05 | gnorm 3.523 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 10289\n",
      "2023-04-17 13:43:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:43:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:43:29 | INFO | fairseq.trainer | begin training epoch 385\n",
      "2023-04-17 13:43:29 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:43:37 | INFO | train_inner | epoch 385:     21 / 32 loss=2.476, nll_loss=0.312, ppl=1.24, wps=978.1, ups=1.42, wpb=687.5, bsz=2, num_updates=12300, lr=2.55472e-05, gnorm=3.615, clip=100, loss_scale=0.5, train_wall=34, gb_free=13.9, wall=10297\n",
      "2023-04-17 13:43:40 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:43:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:43:40 | INFO | valid | epoch 385 | valid on 'valid' subset | loss 5.691 | nll_loss 3.951 | ppl 15.47 | wps 7138.4 | wpb 290.8 | bsz 1 | num_updates 12311 | best_loss 3.979\n",
      "2023-04-17 13:43:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 385 @ 12311 updates\n",
      "2023-04-17 13:43:40 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:43:49 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:43:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 385 @ 12311 updates, score 5.691) (writing took 8.986433513928205 seconds)\n",
      "2023-04-17 13:43:49 | INFO | fairseq_cli.train | end of epoch 385 (average epoch stats below)\n",
      "2023-04-17 13:43:49 | INFO | train | epoch 385 | loss 2.474 | nll_loss 0.309 | ppl 1.24 | wps 1068.5 | ups 1.57 | wpb 678.5 | bsz 2 | num_updates 12311 | lr 2.5543e-05 | gnorm 3.871 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 10310\n",
      "2023-04-17 13:43:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:43:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:43:49 | INFO | fairseq.trainer | begin training epoch 386\n",
      "2023-04-17 13:43:49 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:44:02 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:44:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:44:02 | INFO | valid | epoch 386 | valid on 'valid' subset | loss 5.716 | nll_loss 3.966 | ppl 15.63 | wps 6258.8 | wpb 290.8 | bsz 1 | num_updates 12343 | best_loss 3.979\n",
      "2023-04-17 13:44:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 386 @ 12343 updates\n",
      "2023-04-17 13:44:02 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:44:15 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:44:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 386 @ 12343 updates, score 5.716) (writing took 12.999534375965595 seconds)\n",
      "2023-04-17 13:44:15 | INFO | fairseq_cli.train | end of epoch 386 (average epoch stats below)\n",
      "2023-04-17 13:44:15 | INFO | train | epoch 386 | loss 2.472 | nll_loss 0.309 | ppl 1.24 | wps 856.8 | ups 1.26 | wpb 678.5 | bsz 2 | num_updates 12343 | lr 2.55309e-05 | gnorm 3.411 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 10335\n",
      "2023-04-17 13:44:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:44:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:44:15 | INFO | fairseq.trainer | begin training epoch 387\n",
      "2023-04-17 13:44:15 | INFO | fairseq_cli.train | Start iterating over samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 13:44:27 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:44:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:44:27 | INFO | valid | epoch 387 | valid on 'valid' subset | loss 5.691 | nll_loss 3.942 | ppl 15.37 | wps 6278.8 | wpb 290.8 | bsz 1 | num_updates 12375 | best_loss 3.979\n",
      "2023-04-17 13:44:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 387 @ 12375 updates\n",
      "2023-04-17 13:44:27 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:44:39 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:44:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 387 @ 12375 updates, score 5.691) (writing took 12.795261804014444 seconds)\n",
      "2023-04-17 13:44:40 | INFO | fairseq_cli.train | end of epoch 387 (average epoch stats below)\n",
      "2023-04-17 13:44:40 | INFO | train | epoch 387 | loss 2.467 | nll_loss 0.303 | ppl 1.23 | wps 872.6 | ups 1.29 | wpb 678.5 | bsz 2 | num_updates 12375 | lr 2.55189e-05 | gnorm 3.579 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 10360\n",
      "2023-04-17 13:44:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:44:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:44:40 | INFO | fairseq.trainer | begin training epoch 388\n",
      "2023-04-17 13:44:40 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:44:48 | INFO | train_inner | epoch 388:     25 / 32 loss=2.468, nll_loss=0.305, ppl=1.24, wps=935.2, ups=1.39, wpb=672.7, bsz=2, num_updates=12400, lr=2.55094e-05, gnorm=3.535, clip=100, loss_scale=0.5, train_wall=36, gb_free=13.9, wall=10369\n",
      "2023-04-17 13:44:51 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:44:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:44:51 | INFO | valid | epoch 388 | valid on 'valid' subset | loss 5.636 | nll_loss 3.88 | ppl 14.72 | wps 7146 | wpb 290.8 | bsz 1 | num_updates 12407 | best_loss 3.979\n",
      "2023-04-17 13:44:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 388 @ 12407 updates\n",
      "2023-04-17 13:44:51 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:45:04 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:45:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 388 @ 12407 updates, score 5.636) (writing took 12.953292399994098 seconds)\n",
      "2023-04-17 13:45:04 | INFO | fairseq_cli.train | end of epoch 388 (average epoch stats below)\n",
      "2023-04-17 13:45:04 | INFO | train | epoch 388 | loss 2.462 | nll_loss 0.297 | ppl 1.23 | wps 893.6 | ups 1.32 | wpb 678.5 | bsz 2 | num_updates 12407 | lr 2.55068e-05 | gnorm 3.318 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 10384\n",
      "2023-04-17 13:45:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:45:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:45:04 | INFO | fairseq.trainer | begin training epoch 389\n",
      "2023-04-17 13:45:04 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:45:15 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:45:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:45:15 | INFO | valid | epoch 389 | valid on 'valid' subset | loss 5.766 | nll_loss 4.028 | ppl 16.31 | wps 7307.3 | wpb 290.8 | bsz 1 | num_updates 12439 | best_loss 3.979\n",
      "2023-04-17 13:45:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 389 @ 12439 updates\n",
      "2023-04-17 13:45:15 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:45:28 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:45:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 389 @ 12439 updates, score 5.766) (writing took 12.809278466040269 seconds)\n",
      "2023-04-17 13:45:28 | INFO | fairseq_cli.train | end of epoch 389 (average epoch stats below)\n",
      "2023-04-17 13:45:28 | INFO | train | epoch 389 | loss 2.474 | nll_loss 0.311 | ppl 1.24 | wps 898.3 | ups 1.32 | wpb 678.5 | bsz 2 | num_updates 12439 | lr 2.54947e-05 | gnorm 3.342 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 10408\n",
      "2023-04-17 13:45:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:45:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:45:28 | INFO | fairseq.trainer | begin training epoch 390\n",
      "2023-04-17 13:45:28 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:45:39 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:45:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:45:39 | INFO | valid | epoch 390 | valid on 'valid' subset | loss 5.784 | nll_loss 4.049 | ppl 16.56 | wps 7331.8 | wpb 290.8 | bsz 1 | num_updates 12471 | best_loss 3.979\n",
      "2023-04-17 13:45:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 390 @ 12471 updates\n",
      "2023-04-17 13:45:39 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:45:52 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:45:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 390 @ 12471 updates, score 5.784) (writing took 12.867438546032645 seconds)\n",
      "2023-04-17 13:45:52 | INFO | fairseq_cli.train | end of epoch 390 (average epoch stats below)\n",
      "2023-04-17 13:45:52 | INFO | train | epoch 390 | loss 2.466 | nll_loss 0.302 | ppl 1.23 | wps 896.9 | ups 1.32 | wpb 678.5 | bsz 2 | num_updates 12471 | lr 2.54826e-05 | gnorm 5.185 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 10433\n",
      "2023-04-17 13:45:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:45:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:45:52 | INFO | fairseq.trainer | begin training epoch 391\n",
      "2023-04-17 13:45:52 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:46:03 | INFO | train_inner | epoch 391:     29 / 32 loss=2.469, nll_loss=0.305, ppl=1.24, wps=914.6, ups=1.35, wpb=677.6, bsz=2, num_updates=12500, lr=2.54717e-05, gnorm=3.994, clip=100, loss_scale=0.5, train_wall=34, gb_free=13.9, wall=10443\n",
      "2023-04-17 13:46:03 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:46:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:46:04 | INFO | valid | epoch 391 | valid on 'valid' subset | loss 5.709 | nll_loss 3.954 | ppl 15.49 | wps 7160.1 | wpb 290.8 | bsz 1 | num_updates 12503 | best_loss 3.979\n",
      "2023-04-17 13:46:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 391 @ 12503 updates\n",
      "2023-04-17 13:46:04 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:46:16 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:46:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 391 @ 12503 updates, score 5.709) (writing took 12.778686671052128 seconds)\n",
      "2023-04-17 13:46:16 | INFO | fairseq_cli.train | end of epoch 391 (average epoch stats below)\n",
      "2023-04-17 13:46:16 | INFO | train | epoch 391 | loss 2.467 | nll_loss 0.305 | ppl 1.24 | wps 899 | ups 1.32 | wpb 678.5 | bsz 2 | num_updates 12503 | lr 2.54706e-05 | gnorm 3.587 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 10457\n",
      "2023-04-17 13:46:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:46:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:46:16 | INFO | fairseq.trainer | begin training epoch 392\n",
      "2023-04-17 13:46:16 | INFO | fairseq_cli.train | Start iterating over samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 13:46:28 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:46:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:46:28 | INFO | valid | epoch 392 | valid on 'valid' subset | loss 5.757 | nll_loss 4.042 | ppl 16.47 | wps 7323.4 | wpb 290.8 | bsz 1 | num_updates 12535 | best_loss 3.979\n",
      "2023-04-17 13:46:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 392 @ 12535 updates\n",
      "2023-04-17 13:46:28 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:46:40 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:46:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 392 @ 12535 updates, score 5.757) (writing took 12.82122019608505 seconds)\n",
      "2023-04-17 13:46:41 | INFO | fairseq_cli.train | end of epoch 392 (average epoch stats below)\n",
      "2023-04-17 13:46:41 | INFO | train | epoch 392 | loss 2.464 | nll_loss 0.297 | ppl 1.23 | wps 898.9 | ups 1.32 | wpb 678.5 | bsz 2 | num_updates 12535 | lr 2.54585e-05 | gnorm 3.843 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 10481\n",
      "2023-04-17 13:46:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:46:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:46:41 | INFO | fairseq.trainer | begin training epoch 393\n",
      "2023-04-17 13:46:41 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:46:52 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:46:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:46:52 | INFO | valid | epoch 393 | valid on 'valid' subset | loss 5.778 | nll_loss 4.047 | ppl 16.53 | wps 7366.6 | wpb 290.8 | bsz 1 | num_updates 12567 | best_loss 3.979\n",
      "2023-04-17 13:46:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 393 @ 12567 updates\n",
      "2023-04-17 13:46:52 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:47:05 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:47:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 393 @ 12567 updates, score 5.778) (writing took 12.869124497985467 seconds)\n",
      "2023-04-17 13:47:05 | INFO | fairseq_cli.train | end of epoch 393 (average epoch stats below)\n",
      "2023-04-17 13:47:05 | INFO | train | epoch 393 | loss 2.461 | nll_loss 0.297 | ppl 1.23 | wps 898.1 | ups 1.32 | wpb 678.5 | bsz 2 | num_updates 12567 | lr 2.54464e-05 | gnorm 3.325 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 10505\n",
      "2023-04-17 13:47:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:47:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:47:05 | INFO | fairseq.trainer | begin training epoch 394\n",
      "2023-04-17 13:47:05 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:47:16 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:47:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:47:16 | INFO | valid | epoch 394 | valid on 'valid' subset | loss 5.72 | nll_loss 3.969 | ppl 15.66 | wps 7258.7 | wpb 290.8 | bsz 1 | num_updates 12599 | best_loss 3.979\n",
      "2023-04-17 13:47:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 394 @ 12599 updates\n",
      "2023-04-17 13:47:16 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:47:29 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:47:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 394 @ 12599 updates, score 5.72) (writing took 12.9299813460093 seconds)\n",
      "2023-04-17 13:47:29 | INFO | fairseq_cli.train | end of epoch 394 (average epoch stats below)\n",
      "2023-04-17 13:47:29 | INFO | train | epoch 394 | loss 2.464 | nll_loss 0.302 | ppl 1.23 | wps 893.5 | ups 1.32 | wpb 678.5 | bsz 2 | num_updates 12599 | lr 2.54343e-05 | gnorm 3.283 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 10529\n",
      "2023-04-17 13:47:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:47:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:47:29 | INFO | fairseq.trainer | begin training epoch 395\n",
      "2023-04-17 13:47:29 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:47:30 | INFO | train_inner | epoch 395:      1 / 32 loss=2.463, nll_loss=0.299, ppl=1.23, wps=779.4, ups=1.15, wpb=677.7, bsz=2, num_updates=12600, lr=2.5434e-05, gnorm=3.483, clip=100, loss_scale=0.5, train_wall=34, gb_free=13.9, wall=10530\n",
      "2023-04-17 13:47:40 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:47:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:47:41 | INFO | valid | epoch 395 | valid on 'valid' subset | loss 5.753 | nll_loss 4.003 | ppl 16.03 | wps 7289.2 | wpb 290.8 | bsz 1 | num_updates 12631 | best_loss 3.979\n",
      "2023-04-17 13:47:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 395 @ 12631 updates\n",
      "2023-04-17 13:47:41 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:47:53 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:47:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 395 @ 12631 updates, score 5.753) (writing took 12.833032433059998 seconds)\n",
      "2023-04-17 13:47:53 | INFO | fairseq_cli.train | end of epoch 395 (average epoch stats below)\n",
      "2023-04-17 13:47:53 | INFO | train | epoch 395 | loss 2.46 | nll_loss 0.298 | ppl 1.23 | wps 893.2 | ups 1.32 | wpb 678.5 | bsz 2 | num_updates 12631 | lr 2.54223e-05 | gnorm 3.437 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 10554\n",
      "2023-04-17 13:47:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:47:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:47:53 | INFO | fairseq.trainer | begin training epoch 396\n",
      "2023-04-17 13:47:53 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:48:04 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:48:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:48:05 | INFO | valid | epoch 396 | valid on 'valid' subset | loss 5.689 | nll_loss 3.95 | ppl 15.45 | wps 7238.1 | wpb 290.8 | bsz 1 | num_updates 12663 | best_loss 3.979\n",
      "2023-04-17 13:48:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 396 @ 12663 updates\n",
      "2023-04-17 13:48:05 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:48:17 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:48:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 396 @ 12663 updates, score 5.689) (writing took 12.93237209902145 seconds)\n",
      "2023-04-17 13:48:18 | INFO | fairseq_cli.train | end of epoch 396 (average epoch stats below)\n",
      "2023-04-17 13:48:18 | INFO | train | epoch 396 | loss 2.46 | nll_loss 0.295 | ppl 1.23 | wps 896.1 | ups 1.32 | wpb 678.5 | bsz 2 | num_updates 12663 | lr 2.54102e-05 | gnorm 3.546 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 10578\n",
      "2023-04-17 13:48:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:48:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:48:18 | INFO | fairseq.trainer | begin training epoch 397\n",
      "2023-04-17 13:48:18 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:48:29 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:48:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 13:48:29 | INFO | valid | epoch 397 | valid on 'valid' subset | loss 5.739 | nll_loss 3.977 | ppl 15.75 | wps 7261.6 | wpb 290.8 | bsz 1 | num_updates 12695 | best_loss 3.979\n",
      "2023-04-17 13:48:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 397 @ 12695 updates\n",
      "2023-04-17 13:48:29 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:48:42 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:48:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 397 @ 12695 updates, score 5.739) (writing took 12.78211336908862 seconds)\n",
      "2023-04-17 13:48:42 | INFO | fairseq_cli.train | end of epoch 397 (average epoch stats below)\n",
      "2023-04-17 13:48:42 | INFO | train | epoch 397 | loss 2.46 | nll_loss 0.298 | ppl 1.23 | wps 902 | ups 1.33 | wpb 678.5 | bsz 2 | num_updates 12695 | lr 2.53981e-05 | gnorm 3.502 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 10602\n",
      "2023-04-17 13:48:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:48:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:48:42 | INFO | fairseq.trainer | begin training epoch 398\n",
      "2023-04-17 13:48:42 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:48:44 | INFO | train_inner | epoch 398:      5 / 32 loss=2.46, nll_loss=0.296, ppl=1.23, wps=924.2, ups=1.35, wpb=684.2, bsz=2, num_updates=12700, lr=2.53962e-05, gnorm=3.497, clip=100, loss_scale=0.5, train_wall=34, gb_free=13.9, wall=10604\n",
      "2023-04-17 13:48:53 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:48:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:48:53 | INFO | valid | epoch 398 | valid on 'valid' subset | loss 5.806 | nll_loss 4.078 | ppl 16.89 | wps 7280.8 | wpb 290.8 | bsz 1 | num_updates 12727 | best_loss 3.979\n",
      "2023-04-17 13:48:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 398 @ 12727 updates\n",
      "2023-04-17 13:48:53 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:49:06 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:49:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 398 @ 12727 updates, score 5.806) (writing took 12.867749788914807 seconds)\n",
      "2023-04-17 13:49:06 | INFO | fairseq_cli.train | end of epoch 398 (average epoch stats below)\n",
      "2023-04-17 13:49:06 | INFO | train | epoch 398 | loss 2.461 | nll_loss 0.297 | ppl 1.23 | wps 900.8 | ups 1.33 | wpb 678.5 | bsz 2 | num_updates 12727 | lr 2.5386e-05 | gnorm 3.42 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 10626\n",
      "2023-04-17 13:49:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:49:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:49:06 | INFO | fairseq.trainer | begin training epoch 399\n",
      "2023-04-17 13:49:06 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:49:17 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:49:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:49:17 | INFO | valid | epoch 399 | valid on 'valid' subset | loss 5.723 | nll_loss 3.982 | ppl 15.8 | wps 7288.4 | wpb 290.8 | bsz 1 | num_updates 12759 | best_loss 3.979\n",
      "2023-04-17 13:49:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 399 @ 12759 updates\n",
      "2023-04-17 13:49:17 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:49:30 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:49:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 399 @ 12759 updates, score 5.723) (writing took 12.838120358996093 seconds)\n",
      "2023-04-17 13:49:30 | INFO | fairseq_cli.train | end of epoch 399 (average epoch stats below)\n",
      "2023-04-17 13:49:30 | INFO | train | epoch 399 | loss 2.456 | nll_loss 0.293 | ppl 1.23 | wps 899.5 | ups 1.33 | wpb 678.5 | bsz 2 | num_updates 12759 | lr 2.5374e-05 | gnorm 3.262 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 10650\n",
      "2023-04-17 13:49:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:49:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:49:30 | INFO | fairseq.trainer | begin training epoch 400\n",
      "2023-04-17 13:49:30 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:49:41 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:49:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:49:41 | INFO | valid | epoch 400 | valid on 'valid' subset | loss 5.735 | nll_loss 3.986 | ppl 15.85 | wps 7296.5 | wpb 290.8 | bsz 1 | num_updates 12791 | best_loss 3.979\n",
      "2023-04-17 13:49:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 400 @ 12791 updates\n",
      "2023-04-17 13:49:41 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:49:54 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:49:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 400 @ 12791 updates, score 5.735) (writing took 12.99894433002919 seconds)\n",
      "2023-04-17 13:49:54 | INFO | fairseq_cli.train | end of epoch 400 (average epoch stats below)\n",
      "2023-04-17 13:49:54 | INFO | train | epoch 400 | loss 2.457 | nll_loss 0.295 | ppl 1.23 | wps 891.2 | ups 1.31 | wpb 678.5 | bsz 2 | num_updates 12791 | lr 2.53619e-05 | gnorm 3.487 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 10675\n",
      "2023-04-17 13:49:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:49:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:49:54 | INFO | fairseq.trainer | begin training epoch 401\n",
      "2023-04-17 13:49:54 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:49:58 | INFO | train_inner | epoch 401:      9 / 32 loss=2.458, nll_loss=0.295, ppl=1.23, wps=904, ups=1.35, wpb=669, bsz=2, num_updates=12800, lr=2.53585e-05, gnorm=3.387, clip=100, loss_scale=0.5, train_wall=34, gb_free=13.9, wall=10678\n",
      "2023-04-17 13:50:05 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:50:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:50:06 | INFO | valid | epoch 401 | valid on 'valid' subset | loss 5.74 | nll_loss 4.001 | ppl 16.02 | wps 7250.1 | wpb 290.8 | bsz 1 | num_updates 12823 | best_loss 3.979\n",
      "2023-04-17 13:50:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 401 @ 12823 updates\n",
      "2023-04-17 13:50:06 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:50:18 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:50:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 401 @ 12823 updates, score 5.74) (writing took 12.799415384070016 seconds)\n",
      "2023-04-17 13:50:18 | INFO | fairseq_cli.train | end of epoch 401 (average epoch stats below)\n",
      "2023-04-17 13:50:18 | INFO | train | epoch 401 | loss 2.46 | nll_loss 0.297 | ppl 1.23 | wps 899 | ups 1.32 | wpb 678.5 | bsz 2 | num_updates 12823 | lr 2.53498e-05 | gnorm 3.522 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 10699\n",
      "2023-04-17 13:50:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:50:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:50:18 | INFO | fairseq.trainer | begin training epoch 402\n",
      "2023-04-17 13:50:18 | INFO | fairseq_cli.train | Start iterating over samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 13:50:30 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:50:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:50:30 | INFO | valid | epoch 402 | valid on 'valid' subset | loss 5.845 | nll_loss 4.106 | ppl 17.21 | wps 7253.7 | wpb 290.8 | bsz 1 | num_updates 12855 | best_loss 3.979\n",
      "2023-04-17 13:50:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 402 @ 12855 updates\n",
      "2023-04-17 13:50:30 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:50:43 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:50:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 402 @ 12855 updates, score 5.845) (writing took 13.18866226694081 seconds)\n",
      "2023-04-17 13:50:43 | INFO | fairseq_cli.train | end of epoch 402 (average epoch stats below)\n",
      "2023-04-17 13:50:43 | INFO | train | epoch 402 | loss 2.449 | nll_loss 0.284 | ppl 1.22 | wps 885.2 | ups 1.3 | wpb 678.5 | bsz 2 | num_updates 12855 | lr 2.53377e-05 | gnorm 3.315 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 10723\n",
      "2023-04-17 13:50:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:50:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:50:43 | INFO | fairseq.trainer | begin training epoch 403\n",
      "2023-04-17 13:50:43 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:50:54 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:50:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:50:54 | INFO | valid | epoch 403 | valid on 'valid' subset | loss 5.846 | nll_loss 4.093 | ppl 17.06 | wps 7218 | wpb 290.8 | bsz 1 | num_updates 12887 | best_loss 3.979\n",
      "2023-04-17 13:50:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 403 @ 12887 updates\n",
      "2023-04-17 13:50:54 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:51:07 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:51:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 403 @ 12887 updates, score 5.846) (writing took 13.005430746008642 seconds)\n",
      "2023-04-17 13:51:07 | INFO | fairseq_cli.train | end of epoch 403 (average epoch stats below)\n",
      "2023-04-17 13:51:07 | INFO | train | epoch 403 | loss 2.448 | nll_loss 0.285 | ppl 1.22 | wps 892.7 | ups 1.32 | wpb 678.5 | bsz 2 | num_updates 12887 | lr 2.53257e-05 | gnorm 3.289 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 10748\n",
      "2023-04-17 13:51:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:51:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:51:07 | INFO | fairseq.trainer | begin training epoch 404\n",
      "2023-04-17 13:51:07 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:51:12 | INFO | train_inner | epoch 404:     13 / 32 loss=2.452, nll_loss=0.288, ppl=1.22, wps=916, ups=1.34, wpb=681.9, bsz=2, num_updates=12900, lr=2.53208e-05, gnorm=3.4, clip=100, loss_scale=0.5, train_wall=34, gb_free=13.9, wall=10752\n",
      "2023-04-17 13:51:19 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:51:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:51:19 | INFO | valid | epoch 404 | valid on 'valid' subset | loss 5.712 | nll_loss 3.956 | ppl 15.52 | wps 6310.9 | wpb 290.8 | bsz 1 | num_updates 12919 | best_loss 3.979\n",
      "2023-04-17 13:51:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 404 @ 12919 updates\n",
      "2023-04-17 13:51:19 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:51:32 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:51:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 404 @ 12919 updates, score 5.712) (writing took 13.084187611937523 seconds)\n",
      "2023-04-17 13:51:32 | INFO | fairseq_cli.train | end of epoch 404 (average epoch stats below)\n",
      "2023-04-17 13:51:32 | INFO | train | epoch 404 | loss 2.456 | nll_loss 0.292 | ppl 1.22 | wps 872.2 | ups 1.29 | wpb 678.5 | bsz 2 | num_updates 12919 | lr 2.53136e-05 | gnorm 3.648 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 10772\n",
      "2023-04-17 13:51:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:51:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:51:32 | INFO | fairseq.trainer | begin training epoch 405\n",
      "2023-04-17 13:51:32 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:51:43 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:51:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:51:44 | INFO | valid | epoch 405 | valid on 'valid' subset | loss 5.769 | nll_loss 4.045 | ppl 16.51 | wps 7352.3 | wpb 290.8 | bsz 1 | num_updates 12951 | best_loss 3.979\n",
      "2023-04-17 13:51:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 405 @ 12951 updates\n",
      "2023-04-17 13:51:44 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:51:58 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:51:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 405 @ 12951 updates, score 5.769) (writing took 14.211310119950213 seconds)\n",
      "2023-04-17 13:51:58 | INFO | fairseq_cli.train | end of epoch 405 (average epoch stats below)\n",
      "2023-04-17 13:51:58 | INFO | train | epoch 405 | loss 2.451 | nll_loss 0.287 | ppl 1.22 | wps 848.3 | ups 1.25 | wpb 678.5 | bsz 2 | num_updates 12951 | lr 2.53015e-05 | gnorm 3.277 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 10798\n",
      "2023-04-17 13:51:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:51:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:51:58 | INFO | fairseq.trainer | begin training epoch 406\n",
      "2023-04-17 13:51:58 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:52:09 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:52:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:52:09 | INFO | valid | epoch 406 | valid on 'valid' subset | loss 5.748 | nll_loss 3.996 | ppl 15.96 | wps 7297.2 | wpb 290.8 | bsz 1 | num_updates 12983 | best_loss 3.979\n",
      "2023-04-17 13:52:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 406 @ 12983 updates\n",
      "2023-04-17 13:52:09 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:52:22 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:52:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 406 @ 12983 updates, score 5.748) (writing took 13.046811245963909 seconds)\n",
      "2023-04-17 13:52:22 | INFO | fairseq_cli.train | end of epoch 406 (average epoch stats below)\n",
      "2023-04-17 13:52:22 | INFO | train | epoch 406 | loss 2.46 | nll_loss 0.298 | ppl 1.23 | wps 892.5 | ups 1.32 | wpb 678.5 | bsz 2 | num_updates 12983 | lr 2.52894e-05 | gnorm 3.747 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 10822\n",
      "2023-04-17 13:52:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:52:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:52:22 | INFO | fairseq.trainer | begin training epoch 407\n",
      "2023-04-17 13:52:22 | INFO | fairseq_cli.train | Start iterating over samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 13:52:28 | INFO | train_inner | epoch 407:     17 / 32 loss=2.454, nll_loss=0.292, ppl=1.22, wps=898.4, ups=1.31, wpb=684.4, bsz=2, num_updates=13000, lr=2.5283e-05, gnorm=3.494, clip=100, loss_scale=0.5, train_wall=35, gb_free=13.9, wall=10828\n",
      "2023-04-17 13:52:33 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:52:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:52:33 | INFO | valid | epoch 407 | valid on 'valid' subset | loss 5.793 | nll_loss 4.052 | ppl 16.59 | wps 7217.4 | wpb 290.8 | bsz 1 | num_updates 13015 | best_loss 3.979\n",
      "2023-04-17 13:52:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 407 @ 13015 updates\n",
      "2023-04-17 13:52:33 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:52:46 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:52:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 407 @ 13015 updates, score 5.793) (writing took 12.795172600075603 seconds)\n",
      "2023-04-17 13:52:46 | INFO | fairseq_cli.train | end of epoch 407 (average epoch stats below)\n",
      "2023-04-17 13:52:46 | INFO | train | epoch 407 | loss 2.46 | nll_loss 0.299 | ppl 1.23 | wps 900.5 | ups 1.33 | wpb 678.5 | bsz 2 | num_updates 13015 | lr 2.52774e-05 | gnorm 3.639 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 10847\n",
      "2023-04-17 13:52:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:52:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:52:46 | INFO | fairseq.trainer | begin training epoch 408\n",
      "2023-04-17 13:52:46 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:52:57 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:52:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:52:58 | INFO | valid | epoch 408 | valid on 'valid' subset | loss 5.744 | nll_loss 3.994 | ppl 15.94 | wps 7207.5 | wpb 290.8 | bsz 1 | num_updates 13047 | best_loss 3.979\n",
      "2023-04-17 13:52:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 408 @ 13047 updates\n",
      "2023-04-17 13:52:58 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:53:10 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:53:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 408 @ 13047 updates, score 5.744) (writing took 12.876253861933947 seconds)\n",
      "2023-04-17 13:53:10 | INFO | fairseq_cli.train | end of epoch 408 (average epoch stats below)\n",
      "2023-04-17 13:53:10 | INFO | train | epoch 408 | loss 2.456 | nll_loss 0.295 | ppl 1.23 | wps 897.1 | ups 1.32 | wpb 678.5 | bsz 2 | num_updates 13047 | lr 2.52653e-05 | gnorm 3.676 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 10871\n",
      "2023-04-17 13:53:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:53:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:53:10 | INFO | fairseq.trainer | begin training epoch 409\n",
      "2023-04-17 13:53:10 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:53:22 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:53:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:53:22 | INFO | valid | epoch 409 | valid on 'valid' subset | loss 5.751 | nll_loss 4.017 | ppl 16.19 | wps 7341.1 | wpb 290.8 | bsz 1 | num_updates 13079 | best_loss 3.979\n",
      "2023-04-17 13:53:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 409 @ 13079 updates\n",
      "2023-04-17 13:53:22 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:53:34 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:53:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 409 @ 13079 updates, score 5.751) (writing took 12.920586800086312 seconds)\n",
      "2023-04-17 13:53:35 | INFO | fairseq_cli.train | end of epoch 409 (average epoch stats below)\n",
      "2023-04-17 13:53:35 | INFO | train | epoch 409 | loss 2.451 | nll_loss 0.289 | ppl 1.22 | wps 895.5 | ups 1.32 | wpb 678.5 | bsz 2 | num_updates 13079 | lr 2.52532e-05 | gnorm 3.365 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 10895\n",
      "2023-04-17 13:53:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:53:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:53:35 | INFO | fairseq.trainer | begin training epoch 410\n",
      "2023-04-17 13:53:35 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:53:42 | INFO | train_inner | epoch 410:     21 / 32 loss=2.457, nll_loss=0.296, ppl=1.23, wps=906.7, ups=1.35, wpb=670.7, bsz=2, num_updates=13100, lr=2.52453e-05, gnorm=3.607, clip=100, loss_scale=0.5, train_wall=34, gb_free=13.9, wall=10902\n",
      "2023-04-17 13:53:46 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:53:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:53:46 | INFO | valid | epoch 410 | valid on 'valid' subset | loss 5.779 | nll_loss 4.039 | ppl 16.44 | wps 7317.4 | wpb 290.8 | bsz 1 | num_updates 13111 | best_loss 3.979\n",
      "2023-04-17 13:53:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 410 @ 13111 updates\n",
      "2023-04-17 13:53:46 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:53:59 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:53:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 410 @ 13111 updates, score 5.779) (writing took 12.948566475068219 seconds)\n",
      "2023-04-17 13:53:59 | INFO | fairseq_cli.train | end of epoch 410 (average epoch stats below)\n",
      "2023-04-17 13:53:59 | INFO | train | epoch 410 | loss 2.453 | nll_loss 0.292 | ppl 1.22 | wps 895.3 | ups 1.32 | wpb 678.5 | bsz 2 | num_updates 13111 | lr 2.52411e-05 | gnorm 3.508 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 10919\n",
      "2023-04-17 13:53:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:53:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:53:59 | INFO | fairseq.trainer | begin training epoch 411\n",
      "2023-04-17 13:53:59 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:54:10 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:54:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:54:10 | INFO | valid | epoch 411 | valid on 'valid' subset | loss 5.77 | nll_loss 4.035 | ppl 16.39 | wps 6928.3 | wpb 290.8 | bsz 1 | num_updates 13143 | best_loss 3.979\n",
      "2023-04-17 13:54:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 411 @ 13143 updates\n",
      "2023-04-17 13:54:10 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:54:23 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:54:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 411 @ 13143 updates, score 5.77) (writing took 12.81724074401427 seconds)\n",
      "2023-04-17 13:54:23 | INFO | fairseq_cli.train | end of epoch 411 (average epoch stats below)\n",
      "2023-04-17 13:54:23 | INFO | train | epoch 411 | loss 2.455 | nll_loss 0.293 | ppl 1.23 | wps 899.3 | ups 1.33 | wpb 678.5 | bsz 2 | num_updates 13143 | lr 2.52291e-05 | gnorm 3.509 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 10943\n",
      "2023-04-17 13:54:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:54:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:54:23 | INFO | fairseq.trainer | begin training epoch 412\n",
      "2023-04-17 13:54:23 | INFO | fairseq_cli.train | Start iterating over samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 13:54:34 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:54:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:54:34 | INFO | valid | epoch 412 | valid on 'valid' subset | loss 5.74 | nll_loss 4.002 | ppl 16.02 | wps 7200 | wpb 290.8 | bsz 1 | num_updates 13175 | best_loss 3.979\n",
      "2023-04-17 13:54:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 412 @ 13175 updates\n",
      "2023-04-17 13:54:34 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:54:47 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:54:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 412 @ 13175 updates, score 5.74) (writing took 12.905919902957976 seconds)\n",
      "2023-04-17 13:54:47 | INFO | fairseq_cli.train | end of epoch 412 (average epoch stats below)\n",
      "2023-04-17 13:54:47 | INFO | train | epoch 412 | loss 2.451 | nll_loss 0.288 | ppl 1.22 | wps 894.5 | ups 1.32 | wpb 678.5 | bsz 2 | num_updates 13175 | lr 2.5217e-05 | gnorm 3.328 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 10968\n",
      "2023-04-17 13:54:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:54:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:54:47 | INFO | fairseq.trainer | begin training epoch 413\n",
      "2023-04-17 13:54:47 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:54:56 | INFO | train_inner | epoch 413:     25 / 32 loss=2.45, nll_loss=0.289, ppl=1.22, wps=923.9, ups=1.35, wpb=684.9, bsz=2, num_updates=13200, lr=2.52075e-05, gnorm=3.336, clip=100, loss_scale=0.5, train_wall=34, gb_free=13.9, wall=10977\n",
      "2023-04-17 13:54:59 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:54:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:54:59 | INFO | valid | epoch 413 | valid on 'valid' subset | loss 5.742 | nll_loss 3.994 | ppl 15.93 | wps 7304.7 | wpb 290.8 | bsz 1 | num_updates 13207 | best_loss 3.979\n",
      "2023-04-17 13:54:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 413 @ 13207 updates\n",
      "2023-04-17 13:54:59 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:55:11 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:55:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 413 @ 13207 updates, score 5.742) (writing took 12.84391106700059 seconds)\n",
      "2023-04-17 13:55:12 | INFO | fairseq_cli.train | end of epoch 413 (average epoch stats below)\n",
      "2023-04-17 13:55:12 | INFO | train | epoch 413 | loss 2.446 | nll_loss 0.284 | ppl 1.22 | wps 897.1 | ups 1.32 | wpb 678.5 | bsz 2 | num_updates 13207 | lr 2.52049e-05 | gnorm 3.118 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 10992\n",
      "2023-04-17 13:55:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:55:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:55:12 | INFO | fairseq.trainer | begin training epoch 414\n",
      "2023-04-17 13:55:12 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:55:24 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:55:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:55:24 | INFO | valid | epoch 414 | valid on 'valid' subset | loss 5.842 | nll_loss 4.109 | ppl 17.26 | wps 6213.5 | wpb 290.8 | bsz 1 | num_updates 13239 | best_loss 3.979\n",
      "2023-04-17 13:55:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 414 @ 13239 updates\n",
      "2023-04-17 13:55:24 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:55:37 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:55:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 414 @ 13239 updates, score 5.842) (writing took 12.996292031020857 seconds)\n",
      "2023-04-17 13:55:37 | INFO | fairseq_cli.train | end of epoch 414 (average epoch stats below)\n",
      "2023-04-17 13:55:37 | INFO | train | epoch 414 | loss 2.447 | nll_loss 0.285 | ppl 1.22 | wps 858.2 | ups 1.26 | wpb 678.5 | bsz 2 | num_updates 13239 | lr 2.51928e-05 | gnorm 3.453 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 11017\n",
      "2023-04-17 13:55:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:55:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:55:37 | INFO | fairseq.trainer | begin training epoch 415\n",
      "2023-04-17 13:55:37 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:55:48 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:55:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:55:48 | INFO | valid | epoch 415 | valid on 'valid' subset | loss 5.758 | nll_loss 4.024 | ppl 16.27 | wps 6991.8 | wpb 290.8 | bsz 1 | num_updates 13271 | best_loss 3.979\n",
      "2023-04-17 13:55:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 415 @ 13271 updates\n",
      "2023-04-17 13:55:48 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:56:01 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:56:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 415 @ 13271 updates, score 5.758) (writing took 12.874548348947428 seconds)\n",
      "2023-04-17 13:56:01 | INFO | fairseq_cli.train | end of epoch 415 (average epoch stats below)\n",
      "2023-04-17 13:56:01 | INFO | train | epoch 415 | loss 2.446 | nll_loss 0.285 | ppl 1.22 | wps 897.3 | ups 1.32 | wpb 678.5 | bsz 2 | num_updates 13271 | lr 2.51808e-05 | gnorm 3.328 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 11041\n",
      "2023-04-17 13:56:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:56:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:56:01 | INFO | fairseq.trainer | begin training epoch 416\n",
      "2023-04-17 13:56:01 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:56:11 | INFO | train_inner | epoch 416:     29 / 32 loss=2.446, nll_loss=0.285, ppl=1.22, wps=904.6, ups=1.33, wpb=678.5, bsz=2, num_updates=13300, lr=2.51698e-05, gnorm=3.35, clip=100, loss_scale=0.5, train_wall=35, gb_free=13.9, wall=11052\n",
      "2023-04-17 13:56:12 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:56:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:56:12 | INFO | valid | epoch 416 | valid on 'valid' subset | loss 5.82 | nll_loss 4.082 | ppl 16.93 | wps 7297.3 | wpb 290.8 | bsz 1 | num_updates 13303 | best_loss 3.979\n",
      "2023-04-17 13:56:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 416 @ 13303 updates\n",
      "2023-04-17 13:56:12 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:56:25 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:56:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 416 @ 13303 updates, score 5.82) (writing took 12.953400440048426 seconds)\n",
      "2023-04-17 13:56:25 | INFO | fairseq_cli.train | end of epoch 416 (average epoch stats below)\n",
      "2023-04-17 13:56:25 | INFO | train | epoch 416 | loss 2.445 | nll_loss 0.283 | ppl 1.22 | wps 895.7 | ups 1.32 | wpb 678.5 | bsz 2 | num_updates 13303 | lr 2.51687e-05 | gnorm 3.27 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 11066\n",
      "2023-04-17 13:56:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:56:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:56:25 | INFO | fairseq.trainer | begin training epoch 417\n",
      "2023-04-17 13:56:25 | INFO | fairseq_cli.train | Start iterating over samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 13:56:36 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:56:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:56:37 | INFO | valid | epoch 417 | valid on 'valid' subset | loss 5.76 | nll_loss 4.021 | ppl 16.23 | wps 7326.4 | wpb 290.8 | bsz 1 | num_updates 13335 | best_loss 3.979\n",
      "2023-04-17 13:56:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 417 @ 13335 updates\n",
      "2023-04-17 13:56:37 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:56:49 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:56:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 417 @ 13335 updates, score 5.76) (writing took 12.640816491912119 seconds)\n",
      "2023-04-17 13:56:49 | INFO | fairseq_cli.train | end of epoch 417 (average epoch stats below)\n",
      "2023-04-17 13:56:49 | INFO | train | epoch 417 | loss 2.447 | nll_loss 0.284 | ppl 1.22 | wps 906.5 | ups 1.34 | wpb 678.5 | bsz 2 | num_updates 13335 | lr 2.51566e-05 | gnorm 3.835 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 11090\n",
      "2023-04-17 13:56:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:56:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:56:49 | INFO | fairseq.trainer | begin training epoch 418\n",
      "2023-04-17 13:56:49 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:57:00 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:57:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:57:01 | INFO | valid | epoch 418 | valid on 'valid' subset | loss 5.849 | nll_loss 4.112 | ppl 17.29 | wps 7228 | wpb 290.8 | bsz 1 | num_updates 13367 | best_loss 3.979\n",
      "2023-04-17 13:57:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 418 @ 13367 updates\n",
      "2023-04-17 13:57:01 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:57:13 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:57:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 418 @ 13367 updates, score 5.849) (writing took 12.931988109950908 seconds)\n",
      "2023-04-17 13:57:14 | INFO | fairseq_cli.train | end of epoch 418 (average epoch stats below)\n",
      "2023-04-17 13:57:14 | INFO | train | epoch 418 | loss 2.447 | nll_loss 0.286 | ppl 1.22 | wps 892.7 | ups 1.32 | wpb 678.5 | bsz 2 | num_updates 13367 | lr 2.51445e-05 | gnorm 3.177 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 11114\n",
      "2023-04-17 13:57:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:57:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:57:14 | INFO | fairseq.trainer | begin training epoch 419\n",
      "2023-04-17 13:57:14 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:57:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:57:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:57:25 | INFO | valid | epoch 419 | valid on 'valid' subset | loss 5.764 | nll_loss 4.028 | ppl 16.32 | wps 7259.3 | wpb 290.8 | bsz 1 | num_updates 13399 | best_loss 3.979\n",
      "2023-04-17 13:57:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 419 @ 13399 updates\n",
      "2023-04-17 13:57:25 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:57:38 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:57:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 419 @ 13399 updates, score 5.764) (writing took 13.058985018054955 seconds)\n",
      "2023-04-17 13:57:38 | INFO | fairseq_cli.train | end of epoch 419 (average epoch stats below)\n",
      "2023-04-17 13:57:38 | INFO | train | epoch 419 | loss 2.457 | nll_loss 0.295 | ppl 1.23 | wps 889.2 | ups 1.31 | wpb 678.5 | bsz 2 | num_updates 13399 | lr 2.51325e-05 | gnorm 3.352 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 11138\n",
      "2023-04-17 13:57:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:57:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:57:38 | INFO | fairseq.trainer | begin training epoch 420\n",
      "2023-04-17 13:57:38 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:57:38 | INFO | train_inner | epoch 420:      1 / 32 loss=2.451, nll_loss=0.289, ppl=1.22, wps=775.4, ups=1.15, wpb=676.1, bsz=2, num_updates=13400, lr=2.51321e-05, gnorm=3.459, clip=100, loss_scale=0.5, train_wall=34, gb_free=13.9, wall=11139\n",
      "2023-04-17 13:57:49 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:57:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:57:49 | INFO | valid | epoch 420 | valid on 'valid' subset | loss 5.773 | nll_loss 4.044 | ppl 16.49 | wps 7041.5 | wpb 290.8 | bsz 1 | num_updates 13431 | best_loss 3.979\n",
      "2023-04-17 13:57:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 420 @ 13431 updates\n",
      "2023-04-17 13:57:49 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:58:02 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:58:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 420 @ 13431 updates, score 5.773) (writing took 13.073678108048625 seconds)\n",
      "2023-04-17 13:58:02 | INFO | fairseq_cli.train | end of epoch 420 (average epoch stats below)\n",
      "2023-04-17 13:58:02 | INFO | train | epoch 420 | loss 2.454 | nll_loss 0.292 | ppl 1.22 | wps 888.8 | ups 1.31 | wpb 678.5 | bsz 2 | num_updates 13431 | lr 2.51204e-05 | gnorm 3.36 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 11163\n",
      "2023-04-17 13:58:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:58:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:58:02 | INFO | fairseq.trainer | begin training epoch 421\n",
      "2023-04-17 13:58:02 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:58:14 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:58:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:58:14 | INFO | valid | epoch 421 | valid on 'valid' subset | loss 5.764 | nll_loss 4.021 | ppl 16.24 | wps 7319.7 | wpb 290.8 | bsz 1 | num_updates 13463 | best_loss 3.979\n",
      "2023-04-17 13:58:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 421 @ 13463 updates\n",
      "2023-04-17 13:58:14 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:58:27 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:58:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 421 @ 13463 updates, score 5.764) (writing took 12.874515758012421 seconds)\n",
      "2023-04-17 13:58:27 | INFO | fairseq_cli.train | end of epoch 421 (average epoch stats below)\n",
      "2023-04-17 13:58:27 | INFO | train | epoch 421 | loss 2.448 | nll_loss 0.285 | ppl 1.22 | wps 892.6 | ups 1.32 | wpb 678.5 | bsz 2 | num_updates 13463 | lr 2.51083e-05 | gnorm 3.275 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 11187\n",
      "2023-04-17 13:58:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:58:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:58:27 | INFO | fairseq.trainer | begin training epoch 422\n",
      "2023-04-17 13:58:27 | INFO | fairseq_cli.train | Start iterating over samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 13:58:38 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:58:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:58:38 | INFO | valid | epoch 422 | valid on 'valid' subset | loss 5.762 | nll_loss 4.036 | ppl 16.41 | wps 7332 | wpb 290.8 | bsz 1 | num_updates 13495 | best_loss 3.979\n",
      "2023-04-17 13:58:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 422 @ 13495 updates\n",
      "2023-04-17 13:58:38 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:58:48 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:58:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 422 @ 13495 updates, score 5.762) (writing took 9.790390550042503 seconds)\n",
      "2023-04-17 13:58:48 | INFO | fairseq_cli.train | end of epoch 422 (average epoch stats below)\n",
      "2023-04-17 13:58:48 | INFO | train | epoch 422 | loss 2.444 | nll_loss 0.283 | ppl 1.22 | wps 1027.4 | ups 1.51 | wpb 678.5 | bsz 2 | num_updates 13495 | lr 2.50962e-05 | gnorm 3.471 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 11208\n",
      "2023-04-17 13:58:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:58:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:58:48 | INFO | fairseq.trainer | begin training epoch 423\n",
      "2023-04-17 13:58:48 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:58:50 | INFO | train_inner | epoch 423:      5 / 32 loss=2.448, nll_loss=0.285, ppl=1.22, wps=946.9, ups=1.4, wpb=676.6, bsz=2, num_updates=13500, lr=2.50943e-05, gnorm=3.372, clip=100, loss_scale=0.5, train_wall=35, gb_free=13.9, wall=11210\n",
      "2023-04-17 13:59:00 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:59:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:59:00 | INFO | valid | epoch 423 | valid on 'valid' subset | loss 5.76 | nll_loss 4.026 | ppl 16.29 | wps 6332.2 | wpb 290.8 | bsz 1 | num_updates 13527 | best_loss 3.979\n",
      "2023-04-17 13:59:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 423 @ 13527 updates\n",
      "2023-04-17 13:59:00 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:59:13 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:59:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 423 @ 13527 updates, score 5.76) (writing took 12.867869503097609 seconds)\n",
      "2023-04-17 13:59:13 | INFO | fairseq_cli.train | end of epoch 423 (average epoch stats below)\n",
      "2023-04-17 13:59:13 | INFO | train | epoch 423 | loss 2.443 | nll_loss 0.279 | ppl 1.21 | wps 864.1 | ups 1.27 | wpb 678.5 | bsz 2 | num_updates 13527 | lr 2.50842e-05 | gnorm 3.339 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 11233\n",
      "2023-04-17 13:59:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:59:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:59:13 | INFO | fairseq.trainer | begin training epoch 424\n",
      "2023-04-17 13:59:13 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:59:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:59:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:59:25 | INFO | valid | epoch 424 | valid on 'valid' subset | loss 5.782 | nll_loss 4.033 | ppl 16.37 | wps 6299 | wpb 290.8 | bsz 1 | num_updates 13559 | best_loss 3.979\n",
      "2023-04-17 13:59:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 424 @ 13559 updates\n",
      "2023-04-17 13:59:25 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:59:38 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 13:59:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 424 @ 13559 updates, score 5.782) (writing took 13.0035926529672 seconds)\n",
      "2023-04-17 13:59:38 | INFO | fairseq_cli.train | end of epoch 424 (average epoch stats below)\n",
      "2023-04-17 13:59:38 | INFO | train | epoch 424 | loss 2.438 | nll_loss 0.277 | ppl 1.21 | wps 867.8 | ups 1.28 | wpb 678.5 | bsz 2 | num_updates 13559 | lr 2.50721e-05 | gnorm 3.34 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 11258\n",
      "2023-04-17 13:59:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:59:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 13:59:38 | INFO | fairseq.trainer | begin training epoch 425\n",
      "2023-04-17 13:59:38 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 13:59:50 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 13:59:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 13:59:50 | INFO | valid | epoch 425 | valid on 'valid' subset | loss 5.759 | nll_loss 4.036 | ppl 16.41 | wps 6243.7 | wpb 290.8 | bsz 1 | num_updates 13591 | best_loss 3.979\n",
      "2023-04-17 13:59:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 425 @ 13591 updates\n",
      "2023-04-17 13:59:50 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:00:03 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:00:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 425 @ 13591 updates, score 5.759) (writing took 12.867479458102025 seconds)\n",
      "2023-04-17 14:00:03 | INFO | fairseq_cli.train | end of epoch 425 (average epoch stats below)\n",
      "2023-04-17 14:00:03 | INFO | train | epoch 425 | loss 2.441 | nll_loss 0.278 | ppl 1.21 | wps 872.1 | ups 1.29 | wpb 678.5 | bsz 2 | num_updates 13591 | lr 2.506e-05 | gnorm 3.528 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 11283\n",
      "2023-04-17 14:00:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:00:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:00:03 | INFO | fairseq.trainer | begin training epoch 426\n",
      "2023-04-17 14:00:03 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:00:06 | INFO | train_inner | epoch 426:      9 / 32 loss=2.441, nll_loss=0.279, ppl=1.21, wps=910.7, ups=1.31, wpb=696.4, bsz=2, num_updates=13600, lr=2.50566e-05, gnorm=3.355, clip=100, loss_scale=0.5, train_wall=37, gb_free=13.9, wall=11287\n",
      "2023-04-17 14:00:15 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:00:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:00:15 | INFO | valid | epoch 426 | valid on 'valid' subset | loss 5.731 | nll_loss 3.986 | ppl 15.85 | wps 6271.1 | wpb 290.8 | bsz 1 | num_updates 13623 | best_loss 3.979\n",
      "2023-04-17 14:00:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 426 @ 13623 updates\n",
      "2023-04-17 14:00:15 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:00:28 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:00:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 426 @ 13623 updates, score 5.731) (writing took 12.899082094081677 seconds)\n",
      "2023-04-17 14:00:28 | INFO | fairseq_cli.train | end of epoch 426 (average epoch stats below)\n",
      "2023-04-17 14:00:28 | INFO | train | epoch 426 | loss 2.445 | nll_loss 0.286 | ppl 1.22 | wps 868.4 | ups 1.28 | wpb 678.5 | bsz 2 | num_updates 13623 | lr 2.50479e-05 | gnorm 3.367 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 11308\n",
      "2023-04-17 14:00:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:00:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 14:00:28 | INFO | fairseq.trainer | begin training epoch 427\n",
      "2023-04-17 14:00:28 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:00:40 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:00:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:00:40 | INFO | valid | epoch 427 | valid on 'valid' subset | loss 5.824 | nll_loss 4.081 | ppl 16.93 | wps 6275.7 | wpb 290.8 | bsz 1 | num_updates 13655 | best_loss 3.979\n",
      "2023-04-17 14:00:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 427 @ 13655 updates\n",
      "2023-04-17 14:00:40 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:00:53 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:00:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 427 @ 13655 updates, score 5.824) (writing took 12.880733093013987 seconds)\n",
      "2023-04-17 14:00:53 | INFO | fairseq_cli.train | end of epoch 427 (average epoch stats below)\n",
      "2023-04-17 14:00:53 | INFO | train | epoch 427 | loss 2.437 | nll_loss 0.275 | ppl 1.21 | wps 871.5 | ups 1.28 | wpb 678.5 | bsz 2 | num_updates 13655 | lr 2.50358e-05 | gnorm 3.249 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 11333\n",
      "2023-04-17 14:00:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:00:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:00:53 | INFO | fairseq.trainer | begin training epoch 428\n",
      "2023-04-17 14:00:53 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:01:04 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:01:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:01:04 | INFO | valid | epoch 428 | valid on 'valid' subset | loss 5.747 | nll_loss 4.013 | ppl 16.15 | wps 7348 | wpb 290.8 | bsz 1 | num_updates 13687 | best_loss 3.979\n",
      "2023-04-17 14:01:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 428 @ 13687 updates\n",
      "2023-04-17 14:01:04 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:01:17 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:01:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 428 @ 13687 updates, score 5.747) (writing took 12.839619030011818 seconds)\n",
      "2023-04-17 14:01:17 | INFO | fairseq_cli.train | end of epoch 428 (average epoch stats below)\n",
      "2023-04-17 14:01:17 | INFO | train | epoch 428 | loss 2.443 | nll_loss 0.279 | ppl 1.21 | wps 899.1 | ups 1.33 | wpb 678.5 | bsz 2 | num_updates 13687 | lr 2.50238e-05 | gnorm 3.418 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 11357\n",
      "2023-04-17 14:01:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:01:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:01:17 | INFO | fairseq.trainer | begin training epoch 429\n",
      "2023-04-17 14:01:17 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:01:22 | INFO | train_inner | epoch 429:     13 / 32 loss=2.442, nll_loss=0.28, ppl=1.21, wps=887.2, ups=1.33, wpb=668.1, bsz=2, num_updates=13700, lr=2.50189e-05, gnorm=3.366, clip=100, loss_scale=0.5, train_wall=36, gb_free=13.9, wall=11362\n",
      "2023-04-17 14:01:28 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:01:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:01:28 | INFO | valid | epoch 429 | valid on 'valid' subset | loss 5.732 | nll_loss 3.986 | ppl 15.84 | wps 7298.6 | wpb 290.8 | bsz 1 | num_updates 13719 | best_loss 3.979\n",
      "2023-04-17 14:01:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 429 @ 13719 updates\n",
      "2023-04-17 14:01:28 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:01:41 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:01:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 429 @ 13719 updates, score 5.732) (writing took 12.818583339918405 seconds)\n",
      "2023-04-17 14:01:41 | INFO | fairseq_cli.train | end of epoch 429 (average epoch stats below)\n",
      "2023-04-17 14:01:41 | INFO | train | epoch 429 | loss 2.441 | nll_loss 0.28 | ppl 1.21 | wps 898.3 | ups 1.32 | wpb 678.5 | bsz 2 | num_updates 13719 | lr 2.50117e-05 | gnorm 3.309 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 11381\n",
      "2023-04-17 14:01:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:01:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:01:41 | INFO | fairseq.trainer | begin training epoch 430\n",
      "2023-04-17 14:01:41 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:01:52 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:01:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:01:53 | INFO | valid | epoch 430 | valid on 'valid' subset | loss 5.788 | nll_loss 4.054 | ppl 16.61 | wps 6902.4 | wpb 290.8 | bsz 1 | num_updates 13751 | best_loss 3.979\n",
      "2023-04-17 14:01:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 430 @ 13751 updates\n",
      "2023-04-17 14:01:53 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:02:05 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:02:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 430 @ 13751 updates, score 5.788) (writing took 13.08149095904082 seconds)\n",
      "2023-04-17 14:02:06 | INFO | fairseq_cli.train | end of epoch 430 (average epoch stats below)\n",
      "2023-04-17 14:02:06 | INFO | train | epoch 430 | loss 2.438 | nll_loss 0.276 | ppl 1.21 | wps 887.1 | ups 1.31 | wpb 678.5 | bsz 2 | num_updates 13751 | lr 2.49996e-05 | gnorm 2.968 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 11406\n",
      "2023-04-17 14:02:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:02:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:02:06 | INFO | fairseq.trainer | begin training epoch 431\n",
      "2023-04-17 14:02:06 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:02:17 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:02:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:02:17 | INFO | valid | epoch 431 | valid on 'valid' subset | loss 5.796 | nll_loss 4.056 | ppl 16.63 | wps 7066.2 | wpb 290.8 | bsz 1 | num_updates 13783 | best_loss 3.979\n",
      "2023-04-17 14:02:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 431 @ 13783 updates\n",
      "2023-04-17 14:02:17 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:02:30 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:02:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 431 @ 13783 updates, score 5.796) (writing took 12.702855048002675 seconds)\n",
      "2023-04-17 14:02:30 | INFO | fairseq_cli.train | end of epoch 431 (average epoch stats below)\n",
      "2023-04-17 14:02:30 | INFO | train | epoch 431 | loss 2.436 | nll_loss 0.275 | ppl 1.21 | wps 900 | ups 1.33 | wpb 678.5 | bsz 2 | num_updates 13783 | lr 2.49875e-05 | gnorm 3.177 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 11430\n",
      "2023-04-17 14:02:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:02:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 14:02:30 | INFO | fairseq.trainer | begin training epoch 432\n",
      "2023-04-17 14:02:30 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:02:36 | INFO | train_inner | epoch 432:     17 / 32 loss=2.437, nll_loss=0.275, ppl=1.21, wps=914.2, ups=1.35, wpb=678.2, bsz=2, num_updates=13800, lr=2.49811e-05, gnorm=3.166, clip=100, loss_scale=0.5, train_wall=34, gb_free=13.9, wall=11436\n",
      "2023-04-17 14:02:41 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:02:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:02:41 | INFO | valid | epoch 432 | valid on 'valid' subset | loss 5.738 | nll_loss 3.999 | ppl 15.98 | wps 7286.1 | wpb 290.8 | bsz 1 | num_updates 13815 | best_loss 3.979\n",
      "2023-04-17 14:02:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 432 @ 13815 updates\n",
      "2023-04-17 14:02:41 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:02:54 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:02:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 432 @ 13815 updates, score 5.738) (writing took 12.918538066092879 seconds)\n",
      "2023-04-17 14:02:54 | INFO | fairseq_cli.train | end of epoch 432 (average epoch stats below)\n",
      "2023-04-17 14:02:54 | INFO | train | epoch 432 | loss 2.438 | nll_loss 0.276 | ppl 1.21 | wps 892.8 | ups 1.32 | wpb 678.5 | bsz 2 | num_updates 13815 | lr 2.49755e-05 | gnorm 3.507 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 11454\n",
      "2023-04-17 14:02:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:02:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:02:54 | INFO | fairseq.trainer | begin training epoch 433\n",
      "2023-04-17 14:02:54 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:03:05 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:03:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:03:06 | INFO | valid | epoch 433 | valid on 'valid' subset | loss 5.747 | nll_loss 4.004 | ppl 16.04 | wps 7228.7 | wpb 290.8 | bsz 1 | num_updates 13847 | best_loss 3.979\n",
      "2023-04-17 14:03:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 433 @ 13847 updates\n",
      "2023-04-17 14:03:06 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:03:18 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:03:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 433 @ 13847 updates, score 5.747) (writing took 12.893002219032496 seconds)\n",
      "2023-04-17 14:03:19 | INFO | fairseq_cli.train | end of epoch 433 (average epoch stats below)\n",
      "2023-04-17 14:03:19 | INFO | train | epoch 433 | loss 2.435 | nll_loss 0.273 | ppl 1.21 | wps 888.5 | ups 1.31 | wpb 678.5 | bsz 2 | num_updates 13847 | lr 2.49634e-05 | gnorm 3.243 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 11479\n",
      "2023-04-17 14:03:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:03:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:03:19 | INFO | fairseq.trainer | begin training epoch 434\n",
      "2023-04-17 14:03:19 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:03:30 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:03:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:03:30 | INFO | valid | epoch 434 | valid on 'valid' subset | loss 5.728 | nll_loss 3.988 | ppl 15.86 | wps 7174.5 | wpb 290.8 | bsz 1 | num_updates 13879 | best_loss 3.979\n",
      "2023-04-17 14:03:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 434 @ 13879 updates\n",
      "2023-04-17 14:03:30 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:03:43 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:03:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 434 @ 13879 updates, score 5.728) (writing took 12.939089097082615 seconds)\n",
      "2023-04-17 14:03:43 | INFO | fairseq_cli.train | end of epoch 434 (average epoch stats below)\n",
      "2023-04-17 14:03:43 | INFO | train | epoch 434 | loss 2.444 | nll_loss 0.279 | ppl 1.21 | wps 893.9 | ups 1.32 | wpb 678.5 | bsz 2 | num_updates 13879 | lr 2.49513e-05 | gnorm 3.328 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 11503\n",
      "2023-04-17 14:03:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:03:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:03:43 | INFO | fairseq.trainer | begin training epoch 435\n",
      "2023-04-17 14:03:43 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:03:50 | INFO | train_inner | epoch 435:     21 / 32 loss=2.44, nll_loss=0.278, ppl=1.21, wps=907.1, ups=1.34, wpb=675.9, bsz=2, num_updates=13900, lr=2.49434e-05, gnorm=3.32, clip=100, loss_scale=0.5, train_wall=35, gb_free=13.9, wall=11511\n",
      "2023-04-17 14:03:54 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:03:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:03:54 | INFO | valid | epoch 435 | valid on 'valid' subset | loss 5.787 | nll_loss 4.065 | ppl 16.73 | wps 7237 | wpb 290.8 | bsz 1 | num_updates 13911 | best_loss 3.979\n",
      "2023-04-17 14:03:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 435 @ 13911 updates\n",
      "2023-04-17 14:03:54 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:04:07 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:04:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 435 @ 13911 updates, score 5.787) (writing took 12.857979986001737 seconds)\n",
      "2023-04-17 14:04:07 | INFO | fairseq_cli.train | end of epoch 435 (average epoch stats below)\n",
      "2023-04-17 14:04:07 | INFO | train | epoch 435 | loss 2.439 | nll_loss 0.277 | ppl 1.21 | wps 893.9 | ups 1.32 | wpb 678.5 | bsz 2 | num_updates 13911 | lr 2.49392e-05 | gnorm 3.256 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 11527\n",
      "2023-04-17 14:04:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:04:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:04:07 | INFO | fairseq.trainer | begin training epoch 436\n",
      "2023-04-17 14:04:07 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:04:18 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:04:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:04:18 | INFO | valid | epoch 436 | valid on 'valid' subset | loss 5.773 | nll_loss 4.053 | ppl 16.6 | wps 6864.2 | wpb 290.8 | bsz 1 | num_updates 13943 | best_loss 3.979\n",
      "2023-04-17 14:04:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 436 @ 13943 updates\n",
      "2023-04-17 14:04:18 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:04:31 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:04:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 436 @ 13943 updates, score 5.773) (writing took 12.999756274977699 seconds)\n",
      "2023-04-17 14:04:31 | INFO | fairseq_cli.train | end of epoch 436 (average epoch stats below)\n",
      "2023-04-17 14:04:31 | INFO | train | epoch 436 | loss 2.435 | nll_loss 0.273 | ppl 1.21 | wps 906 | ups 1.34 | wpb 678.5 | bsz 2 | num_updates 13943 | lr 2.49272e-05 | gnorm 3.272 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 11551\n",
      "2023-04-17 14:04:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:04:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:04:31 | INFO | fairseq.trainer | begin training epoch 437\n",
      "2023-04-17 14:04:31 | INFO | fairseq_cli.train | Start iterating over samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 14:04:42 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:04:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:04:43 | INFO | valid | epoch 437 | valid on 'valid' subset | loss 5.822 | nll_loss 4.097 | ppl 17.11 | wps 7129.7 | wpb 290.8 | bsz 1 | num_updates 13975 | best_loss 3.979\n",
      "2023-04-17 14:04:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 437 @ 13975 updates\n",
      "2023-04-17 14:04:43 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:04:55 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:04:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 437 @ 13975 updates, score 5.822) (writing took 12.693586015957408 seconds)\n",
      "2023-04-17 14:04:55 | INFO | fairseq_cli.train | end of epoch 437 (average epoch stats below)\n",
      "2023-04-17 14:04:55 | INFO | train | epoch 437 | loss 2.431 | nll_loss 0.269 | ppl 1.21 | wps 901.1 | ups 1.33 | wpb 678.5 | bsz 2 | num_updates 13975 | lr 2.49151e-05 | gnorm 3.413 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 11575\n",
      "2023-04-17 14:04:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:04:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:04:55 | INFO | fairseq.trainer | begin training epoch 438\n",
      "2023-04-17 14:04:55 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:05:04 | INFO | train_inner | epoch 438:     25 / 32 loss=2.432, nll_loss=0.27, ppl=1.21, wps=913.3, ups=1.35, wpb=675.3, bsz=2, num_updates=14000, lr=2.49057e-05, gnorm=3.302, clip=100, loss_scale=0.5, train_wall=34, gb_free=13.9, wall=11585\n",
      "2023-04-17 14:05:07 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:05:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:05:07 | INFO | valid | epoch 438 | valid on 'valid' subset | loss 5.86 | nll_loss 4.138 | ppl 17.6 | wps 7050.9 | wpb 290.8 | bsz 1 | num_updates 14007 | best_loss 3.979\n",
      "2023-04-17 14:05:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 438 @ 14007 updates\n",
      "2023-04-17 14:05:07 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:05:20 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:05:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 438 @ 14007 updates, score 5.86) (writing took 13.089251247001812 seconds)\n",
      "2023-04-17 14:05:20 | INFO | fairseq_cli.train | end of epoch 438 (average epoch stats below)\n",
      "2023-04-17 14:05:20 | INFO | train | epoch 438 | loss 2.427 | nll_loss 0.265 | ppl 1.2 | wps 871 | ups 1.28 | wpb 678.5 | bsz 2 | num_updates 14007 | lr 2.4903e-05 | gnorm 3.08 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 11600\n",
      "2023-04-17 14:05:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:05:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:05:20 | INFO | fairseq.trainer | begin training epoch 439\n",
      "2023-04-17 14:05:20 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:05:32 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:05:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:05:32 | INFO | valid | epoch 439 | valid on 'valid' subset | loss 5.744 | nll_loss 4.017 | ppl 16.19 | wps 7278 | wpb 290.8 | bsz 1 | num_updates 14039 | best_loss 3.979\n",
      "2023-04-17 14:05:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 439 @ 14039 updates\n",
      "2023-04-17 14:05:32 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:05:45 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:05:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 439 @ 14039 updates, score 5.744) (writing took 12.745044921059161 seconds)\n",
      "2023-04-17 14:05:45 | INFO | fairseq_cli.train | end of epoch 439 (average epoch stats below)\n",
      "2023-04-17 14:05:45 | INFO | train | epoch 439 | loss 2.437 | nll_loss 0.276 | ppl 1.21 | wps 865.6 | ups 1.28 | wpb 678.5 | bsz 2 | num_updates 14039 | lr 2.48909e-05 | gnorm 3.169 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 11625\n",
      "2023-04-17 14:05:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:05:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:05:45 | INFO | fairseq.trainer | begin training epoch 440\n",
      "2023-04-17 14:05:45 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:05:57 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:05:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:05:58 | INFO | valid | epoch 440 | valid on 'valid' subset | loss 5.802 | nll_loss 4.067 | ppl 16.76 | wps 7284.2 | wpb 290.8 | bsz 1 | num_updates 14071 | best_loss 3.979\n",
      "2023-04-17 14:05:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 440 @ 14071 updates\n",
      "2023-04-17 14:05:58 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:06:10 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:06:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 440 @ 14071 updates, score 5.802) (writing took 12.935637742979452 seconds)\n",
      "2023-04-17 14:06:11 | INFO | fairseq_cli.train | end of epoch 440 (average epoch stats below)\n",
      "2023-04-17 14:06:11 | INFO | train | epoch 440 | loss 2.432 | nll_loss 0.273 | ppl 1.21 | wps 858.5 | ups 1.27 | wpb 678.5 | bsz 2 | num_updates 14071 | lr 2.48789e-05 | gnorm 3.203 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 11651\n",
      "2023-04-17 14:06:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:06:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:06:11 | INFO | fairseq.trainer | begin training epoch 441\n",
      "2023-04-17 14:06:11 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:06:22 | INFO | train_inner | epoch 441:     29 / 32 loss=2.435, nll_loss=0.274, ppl=1.21, wps=878.9, ups=1.29, wpb=681.3, bsz=2, num_updates=14100, lr=2.48679e-05, gnorm=3.201, clip=100, loss_scale=0.5, train_wall=38, gb_free=13.9, wall=11662\n",
      "2023-04-17 14:06:23 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:06:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:06:23 | INFO | valid | epoch 441 | valid on 'valid' subset | loss 5.782 | nll_loss 4.043 | ppl 16.48 | wps 7051 | wpb 290.8 | bsz 1 | num_updates 14103 | best_loss 3.979\n",
      "2023-04-17 14:06:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 441 @ 14103 updates\n",
      "2023-04-17 14:06:23 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:06:36 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:06:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 441 @ 14103 updates, score 5.782) (writing took 12.9565976629965 seconds)\n",
      "2023-04-17 14:06:36 | INFO | fairseq_cli.train | end of epoch 441 (average epoch stats below)\n",
      "2023-04-17 14:06:36 | INFO | train | epoch 441 | loss 2.434 | nll_loss 0.274 | ppl 1.21 | wps 851.1 | ups 1.25 | wpb 678.5 | bsz 2 | num_updates 14103 | lr 2.48668e-05 | gnorm 3.293 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 11676\n",
      "2023-04-17 14:06:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:06:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:06:36 | INFO | fairseq.trainer | begin training epoch 442\n",
      "2023-04-17 14:06:36 | INFO | fairseq_cli.train | Start iterating over samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 14:06:48 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:06:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:06:48 | INFO | valid | epoch 442 | valid on 'valid' subset | loss 5.878 | nll_loss 4.159 | ppl 17.86 | wps 7189.4 | wpb 290.8 | bsz 1 | num_updates 14135 | best_loss 3.979\n",
      "2023-04-17 14:06:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 442 @ 14135 updates\n",
      "2023-04-17 14:06:48 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:07:01 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:07:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 442 @ 14135 updates, score 5.878) (writing took 13.000843687914312 seconds)\n",
      "2023-04-17 14:07:01 | INFO | fairseq_cli.train | end of epoch 442 (average epoch stats below)\n",
      "2023-04-17 14:07:01 | INFO | train | epoch 442 | loss 2.427 | nll_loss 0.266 | ppl 1.2 | wps 871.6 | ups 1.28 | wpb 678.5 | bsz 2 | num_updates 14135 | lr 2.48547e-05 | gnorm 3.216 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 11701\n",
      "2023-04-17 14:07:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:07:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:07:01 | INFO | fairseq.trainer | begin training epoch 443\n",
      "2023-04-17 14:07:01 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:07:12 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:07:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:07:12 | INFO | valid | epoch 443 | valid on 'valid' subset | loss 5.889 | nll_loss 4.169 | ppl 17.98 | wps 7073.9 | wpb 290.8 | bsz 1 | num_updates 14167 | best_loss 3.979\n",
      "2023-04-17 14:07:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 443 @ 14167 updates\n",
      "2023-04-17 14:07:12 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:07:25 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:07:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 443 @ 14167 updates, score 5.889) (writing took 12.933219476020895 seconds)\n",
      "2023-04-17 14:07:25 | INFO | fairseq_cli.train | end of epoch 443 (average epoch stats below)\n",
      "2023-04-17 14:07:25 | INFO | train | epoch 443 | loss 2.431 | nll_loss 0.271 | ppl 1.21 | wps 891.3 | ups 1.31 | wpb 678.5 | bsz 2 | num_updates 14167 | lr 2.48426e-05 | gnorm 3.34 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 11726\n",
      "2023-04-17 14:07:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:07:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:07:25 | INFO | fairseq.trainer | begin training epoch 444\n",
      "2023-04-17 14:07:25 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:07:37 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:07:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:07:37 | INFO | valid | epoch 444 | valid on 'valid' subset | loss 5.799 | nll_loss 4.072 | ppl 16.81 | wps 7330.2 | wpb 290.8 | bsz 1 | num_updates 14199 | best_loss 3.979\n",
      "2023-04-17 14:07:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 444 @ 14199 updates\n",
      "2023-04-17 14:07:37 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:07:50 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:07:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 444 @ 14199 updates, score 5.799) (writing took 13.20653325796593 seconds)\n",
      "2023-04-17 14:07:50 | INFO | fairseq_cli.train | end of epoch 444 (average epoch stats below)\n",
      "2023-04-17 14:07:50 | INFO | train | epoch 444 | loss 2.43 | nll_loss 0.269 | ppl 1.21 | wps 880.5 | ups 1.3 | wpb 678.5 | bsz 2 | num_updates 14199 | lr 2.48306e-05 | gnorm 3.097 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 11750\n",
      "2023-04-17 14:07:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:07:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:07:50 | INFO | fairseq.trainer | begin training epoch 445\n",
      "2023-04-17 14:07:50 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:07:50 | INFO | train_inner | epoch 445:      1 / 32 loss=2.429, nll_loss=0.268, ppl=1.2, wps=763.5, ups=1.13, wpb=676.2, bsz=2, num_updates=14200, lr=2.48302e-05, gnorm=3.213, clip=100, loss_scale=0.5, train_wall=35, gb_free=13.9, wall=11751\n",
      "2023-04-17 14:08:01 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:08:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:08:01 | INFO | valid | epoch 445 | valid on 'valid' subset | loss 5.84 | nll_loss 4.11 | ppl 17.27 | wps 7347.5 | wpb 290.8 | bsz 1 | num_updates 14231 | best_loss 3.979\n",
      "2023-04-17 14:08:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 445 @ 14231 updates\n",
      "2023-04-17 14:08:01 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:08:14 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:08:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 445 @ 14231 updates, score 5.84) (writing took 12.78800100099761 seconds)\n",
      "2023-04-17 14:08:14 | INFO | fairseq_cli.train | end of epoch 445 (average epoch stats below)\n",
      "2023-04-17 14:08:14 | INFO | train | epoch 445 | loss 2.426 | nll_loss 0.265 | ppl 1.2 | wps 896.4 | ups 1.32 | wpb 678.5 | bsz 2 | num_updates 14231 | lr 2.48185e-05 | gnorm 3.052 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 11774\n",
      "2023-04-17 14:08:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:08:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:08:14 | INFO | fairseq.trainer | begin training epoch 446\n",
      "2023-04-17 14:08:14 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:08:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:08:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:08:26 | INFO | valid | epoch 446 | valid on 'valid' subset | loss 5.814 | nll_loss 4.08 | ppl 16.91 | wps 7199.4 | wpb 290.8 | bsz 1 | num_updates 14263 | best_loss 3.979\n",
      "2023-04-17 14:08:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 446 @ 14263 updates\n",
      "2023-04-17 14:08:26 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:08:38 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:08:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 446 @ 14263 updates, score 5.814) (writing took 13.02015106100589 seconds)\n",
      "2023-04-17 14:08:39 | INFO | fairseq_cli.train | end of epoch 446 (average epoch stats below)\n",
      "2023-04-17 14:08:39 | INFO | train | epoch 446 | loss 2.43 | nll_loss 0.269 | ppl 1.2 | wps 890.7 | ups 1.31 | wpb 678.5 | bsz 2 | num_updates 14263 | lr 2.48064e-05 | gnorm 3.158 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 11799\n",
      "2023-04-17 14:08:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:08:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:08:39 | INFO | fairseq.trainer | begin training epoch 447\n",
      "2023-04-17 14:08:39 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:08:50 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:08:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 14:08:50 | INFO | valid | epoch 447 | valid on 'valid' subset | loss 5.773 | nll_loss 4.046 | ppl 16.52 | wps 7180 | wpb 290.8 | bsz 1 | num_updates 14295 | best_loss 3.979\n",
      "2023-04-17 14:08:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 447 @ 14295 updates\n",
      "2023-04-17 14:08:50 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:09:03 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:09:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 447 @ 14295 updates, score 5.773) (writing took 12.770092953927815 seconds)\n",
      "2023-04-17 14:09:03 | INFO | fairseq_cli.train | end of epoch 447 (average epoch stats below)\n",
      "2023-04-17 14:09:03 | INFO | train | epoch 447 | loss 2.428 | nll_loss 0.267 | ppl 1.2 | wps 898.9 | ups 1.32 | wpb 678.5 | bsz 2 | num_updates 14295 | lr 2.47943e-05 | gnorm 3.06 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 11823\n",
      "2023-04-17 14:09:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:09:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:09:03 | INFO | fairseq.trainer | begin training epoch 448\n",
      "2023-04-17 14:09:03 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:09:05 | INFO | train_inner | epoch 448:      5 / 32 loss=2.428, nll_loss=0.267, ppl=1.2, wps=916, ups=1.35, wpb=679.1, bsz=2, num_updates=14300, lr=2.47925e-05, gnorm=3.107, clip=100, loss_scale=0.5, train_wall=34, gb_free=13.9, wall=11825\n",
      "2023-04-17 14:09:14 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:09:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:09:14 | INFO | valid | epoch 448 | valid on 'valid' subset | loss 5.83 | nll_loss 4.114 | ppl 17.31 | wps 6737.9 | wpb 290.8 | bsz 1 | num_updates 14327 | best_loss 3.979\n",
      "2023-04-17 14:09:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 448 @ 14327 updates\n",
      "2023-04-17 14:09:14 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:09:27 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:09:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 448 @ 14327 updates, score 5.83) (writing took 13.07985512597952 seconds)\n",
      "2023-04-17 14:09:27 | INFO | fairseq_cli.train | end of epoch 448 (average epoch stats below)\n",
      "2023-04-17 14:09:27 | INFO | train | epoch 448 | loss 2.43 | nll_loss 0.268 | ppl 1.2 | wps 888.6 | ups 1.31 | wpb 678.5 | bsz 2 | num_updates 14327 | lr 2.47823e-05 | gnorm 3.254 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 11847\n",
      "2023-04-17 14:09:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:09:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:09:27 | INFO | fairseq.trainer | begin training epoch 449\n",
      "2023-04-17 14:09:27 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:09:38 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:09:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:09:39 | INFO | valid | epoch 449 | valid on 'valid' subset | loss 5.792 | nll_loss 4.057 | ppl 16.65 | wps 7168.4 | wpb 290.8 | bsz 1 | num_updates 14359 | best_loss 3.979\n",
      "2023-04-17 14:09:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 449 @ 14359 updates\n",
      "2023-04-17 14:09:39 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:09:51 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:09:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 449 @ 14359 updates, score 5.792) (writing took 12.636248513008468 seconds)\n",
      "2023-04-17 14:09:51 | INFO | fairseq_cli.train | end of epoch 449 (average epoch stats below)\n",
      "2023-04-17 14:09:51 | INFO | train | epoch 449 | loss 2.435 | nll_loss 0.273 | ppl 1.21 | wps 905.2 | ups 1.33 | wpb 678.5 | bsz 2 | num_updates 14359 | lr 2.47702e-05 | gnorm 3.469 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 11871\n",
      "2023-04-17 14:09:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:09:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:09:51 | INFO | fairseq.trainer | begin training epoch 450\n",
      "2023-04-17 14:09:51 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:10:02 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:10:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:10:02 | INFO | valid | epoch 450 | valid on 'valid' subset | loss 5.82 | nll_loss 4.1 | ppl 17.14 | wps 7224.2 | wpb 290.8 | bsz 1 | num_updates 14391 | best_loss 3.979\n",
      "2023-04-17 14:10:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 450 @ 14391 updates\n",
      "2023-04-17 14:10:02 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:10:15 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:10:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 450 @ 14391 updates, score 5.82) (writing took 13.152389436028898 seconds)\n",
      "2023-04-17 14:10:15 | INFO | fairseq_cli.train | end of epoch 450 (average epoch stats below)\n",
      "2023-04-17 14:10:15 | INFO | train | epoch 450 | loss 2.422 | nll_loss 0.261 | ppl 1.2 | wps 892.9 | ups 1.32 | wpb 678.5 | bsz 2 | num_updates 14391 | lr 2.47581e-05 | gnorm 2.929 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 11896\n",
      "2023-04-17 14:10:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:10:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:10:15 | INFO | fairseq.trainer | begin training epoch 451\n",
      "2023-04-17 14:10:15 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:10:19 | INFO | train_inner | epoch 451:      9 / 32 loss=2.428, nll_loss=0.267, ppl=1.2, wps=920, ups=1.35, wpb=682.5, bsz=2, num_updates=14400, lr=2.47547e-05, gnorm=3.188, clip=100, loss_scale=0.5, train_wall=34, gb_free=13.9, wall=11899\n",
      "2023-04-17 14:10:27 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:10:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:10:27 | INFO | valid | epoch 451 | valid on 'valid' subset | loss 5.848 | nll_loss 4.133 | ppl 17.55 | wps 7193.5 | wpb 290.8 | bsz 1 | num_updates 14423 | best_loss 3.979\n",
      "2023-04-17 14:10:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 451 @ 14423 updates\n",
      "2023-04-17 14:10:27 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:10:40 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:10:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 451 @ 14423 updates, score 5.848) (writing took 12.921222212025896 seconds)\n",
      "2023-04-17 14:10:40 | INFO | fairseq_cli.train | end of epoch 451 (average epoch stats below)\n",
      "2023-04-17 14:10:40 | INFO | train | epoch 451 | loss 2.424 | nll_loss 0.263 | ppl 1.2 | wps 892 | ups 1.31 | wpb 678.5 | bsz 2 | num_updates 14423 | lr 2.4746e-05 | gnorm 2.986 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 11920\n",
      "2023-04-17 14:10:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:10:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:10:40 | INFO | fairseq.trainer | begin training epoch 452\n",
      "2023-04-17 14:10:40 | INFO | fairseq_cli.train | Start iterating over samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 14:10:51 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:10:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:10:51 | INFO | valid | epoch 452 | valid on 'valid' subset | loss 5.811 | nll_loss 4.079 | ppl 16.9 | wps 7091.1 | wpb 290.8 | bsz 1 | num_updates 14455 | best_loss 3.979\n",
      "2023-04-17 14:10:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 452 @ 14455 updates\n",
      "2023-04-17 14:10:51 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:11:04 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:11:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 452 @ 14455 updates, score 5.811) (writing took 12.987622951040976 seconds)\n",
      "2023-04-17 14:11:04 | INFO | fairseq_cli.train | end of epoch 452 (average epoch stats below)\n",
      "2023-04-17 14:11:04 | INFO | train | epoch 452 | loss 2.427 | nll_loss 0.265 | ppl 1.2 | wps 890.9 | ups 1.31 | wpb 678.5 | bsz 2 | num_updates 14455 | lr 2.4734e-05 | gnorm 3.222 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 11944\n",
      "2023-04-17 14:11:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:11:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:11:04 | INFO | fairseq.trainer | begin training epoch 453\n",
      "2023-04-17 14:11:04 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:11:15 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:11:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:11:16 | INFO | valid | epoch 453 | valid on 'valid' subset | loss 5.822 | nll_loss 4.098 | ppl 17.13 | wps 7177.9 | wpb 290.8 | bsz 1 | num_updates 14487 | best_loss 3.979\n",
      "2023-04-17 14:11:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 453 @ 14487 updates\n",
      "2023-04-17 14:11:16 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:11:28 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:11:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 453 @ 14487 updates, score 5.822) (writing took 12.753191672964022 seconds)\n",
      "2023-04-17 14:11:28 | INFO | fairseq_cli.train | end of epoch 453 (average epoch stats below)\n",
      "2023-04-17 14:11:28 | INFO | train | epoch 453 | loss 2.422 | nll_loss 0.262 | ppl 1.2 | wps 896.8 | ups 1.32 | wpb 678.5 | bsz 2 | num_updates 14487 | lr 2.47219e-05 | gnorm 3.123 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 11969\n",
      "2023-04-17 14:11:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:11:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:11:28 | INFO | fairseq.trainer | begin training epoch 454\n",
      "2023-04-17 14:11:28 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:11:33 | INFO | train_inner | epoch 454:     13 / 32 loss=2.425, nll_loss=0.264, ppl=1.2, wps=909, ups=1.35, wpb=675.5, bsz=2, num_updates=14500, lr=2.4717e-05, gnorm=3.146, clip=100, loss_scale=0.5, train_wall=34, gb_free=13.9, wall=11973\n",
      "2023-04-17 14:11:40 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:11:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:11:40 | INFO | valid | epoch 454 | valid on 'valid' subset | loss 5.824 | nll_loss 4.096 | ppl 17.11 | wps 7243 | wpb 290.8 | bsz 1 | num_updates 14519 | best_loss 3.979\n",
      "2023-04-17 14:11:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 454 @ 14519 updates\n",
      "2023-04-17 14:11:40 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:11:53 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:11:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 454 @ 14519 updates, score 5.824) (writing took 13.118010230944492 seconds)\n",
      "2023-04-17 14:11:53 | INFO | fairseq_cli.train | end of epoch 454 (average epoch stats below)\n",
      "2023-04-17 14:11:53 | INFO | train | epoch 454 | loss 2.427 | nll_loss 0.267 | ppl 1.2 | wps 887 | ups 1.31 | wpb 678.5 | bsz 2 | num_updates 14519 | lr 2.47098e-05 | gnorm 3.205 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 11993\n",
      "2023-04-17 14:11:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:11:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:11:53 | INFO | fairseq.trainer | begin training epoch 455\n",
      "2023-04-17 14:11:53 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:12:04 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:12:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:12:04 | INFO | valid | epoch 455 | valid on 'valid' subset | loss 5.83 | nll_loss 4.117 | ppl 17.35 | wps 7129.3 | wpb 290.8 | bsz 1 | num_updates 14551 | best_loss 3.979\n",
      "2023-04-17 14:12:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 455 @ 14551 updates\n",
      "2023-04-17 14:12:04 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:12:17 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:12:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 455 @ 14551 updates, score 5.83) (writing took 12.790192073094659 seconds)\n",
      "2023-04-17 14:12:17 | INFO | fairseq_cli.train | end of epoch 455 (average epoch stats below)\n",
      "2023-04-17 14:12:17 | INFO | train | epoch 455 | loss 2.425 | nll_loss 0.264 | ppl 1.2 | wps 897 | ups 1.32 | wpb 678.5 | bsz 2 | num_updates 14551 | lr 2.46977e-05 | gnorm 3.26 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 12017\n",
      "2023-04-17 14:12:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:12:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:12:17 | INFO | fairseq.trainer | begin training epoch 456\n",
      "2023-04-17 14:12:17 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:12:28 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:12:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:12:28 | INFO | valid | epoch 456 | valid on 'valid' subset | loss 5.891 | nll_loss 4.168 | ppl 17.97 | wps 7167.4 | wpb 290.8 | bsz 1 | num_updates 14583 | best_loss 3.979\n",
      "2023-04-17 14:12:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 456 @ 14583 updates\n",
      "2023-04-17 14:12:28 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:12:41 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:12:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 456 @ 14583 updates, score 5.891) (writing took 13.154892539023422 seconds)\n",
      "2023-04-17 14:12:42 | INFO | fairseq_cli.train | end of epoch 456 (average epoch stats below)\n",
      "2023-04-17 14:12:42 | INFO | train | epoch 456 | loss 2.423 | nll_loss 0.263 | ppl 1.2 | wps 886.4 | ups 1.31 | wpb 678.5 | bsz 2 | num_updates 14583 | lr 2.46857e-05 | gnorm 3.062 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 12042\n",
      "2023-04-17 14:12:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:12:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:12:42 | INFO | fairseq.trainer | begin training epoch 457\n",
      "2023-04-17 14:12:42 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:12:48 | INFO | train_inner | epoch 457:     17 / 32 loss=2.423, nll_loss=0.263, ppl=1.2, wps=917.5, ups=1.34, wpb=684.4, bsz=2, num_updates=14600, lr=2.46792e-05, gnorm=3.152, clip=100, loss_scale=0.5, train_wall=34, gb_free=13.9, wall=12048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 14:12:53 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:12:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:12:53 | INFO | valid | epoch 457 | valid on 'valid' subset | loss 5.928 | nll_loss 4.206 | ppl 18.45 | wps 7174.4 | wpb 290.8 | bsz 1 | num_updates 14615 | best_loss 3.979\n",
      "2023-04-17 14:12:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 457 @ 14615 updates\n",
      "2023-04-17 14:12:53 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:13:06 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:13:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 457 @ 14615 updates, score 5.928) (writing took 12.724051375989802 seconds)\n",
      "2023-04-17 14:13:06 | INFO | fairseq_cli.train | end of epoch 457 (average epoch stats below)\n",
      "2023-04-17 14:13:06 | INFO | train | epoch 457 | loss 2.418 | nll_loss 0.258 | ppl 1.2 | wps 898.8 | ups 1.32 | wpb 678.5 | bsz 2 | num_updates 14615 | lr 2.46736e-05 | gnorm 3.154 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 12066\n",
      "2023-04-17 14:13:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:13:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:13:06 | INFO | fairseq.trainer | begin training epoch 458\n",
      "2023-04-17 14:13:06 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:13:18 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:13:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:13:18 | INFO | valid | epoch 458 | valid on 'valid' subset | loss 5.857 | nll_loss 4.128 | ppl 17.48 | wps 6159.7 | wpb 290.8 | bsz 1 | num_updates 14647 | best_loss 3.979\n",
      "2023-04-17 14:13:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 458 @ 14647 updates\n",
      "2023-04-17 14:13:18 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:13:31 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:13:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 458 @ 14647 updates, score 5.857) (writing took 13.239283376024105 seconds)\n",
      "2023-04-17 14:13:31 | INFO | fairseq_cli.train | end of epoch 458 (average epoch stats below)\n",
      "2023-04-17 14:13:31 | INFO | train | epoch 458 | loss 2.42 | nll_loss 0.259 | ppl 1.2 | wps 854.9 | ups 1.26 | wpb 678.5 | bsz 2 | num_updates 14647 | lr 2.46615e-05 | gnorm 3.202 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 12091\n",
      "2023-04-17 14:13:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:13:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:13:31 | INFO | fairseq.trainer | begin training epoch 459\n",
      "2023-04-17 14:13:31 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:13:43 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:13:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:13:43 | INFO | valid | epoch 459 | valid on 'valid' subset | loss 5.909 | nll_loss 4.183 | ppl 18.17 | wps 6125.7 | wpb 290.8 | bsz 1 | num_updates 14679 | best_loss 3.979\n",
      "2023-04-17 14:13:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 459 @ 14679 updates\n",
      "2023-04-17 14:13:43 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:13:56 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:13:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 459 @ 14679 updates, score 5.909) (writing took 12.710992604959756 seconds)\n",
      "2023-04-17 14:13:56 | INFO | fairseq_cli.train | end of epoch 459 (average epoch stats below)\n",
      "2023-04-17 14:13:56 | INFO | train | epoch 459 | loss 2.421 | nll_loss 0.261 | ppl 1.2 | wps 870.5 | ups 1.28 | wpb 678.5 | bsz 2 | num_updates 14679 | lr 2.46494e-05 | gnorm 3.276 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 12116\n",
      "2023-04-17 14:13:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:13:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:13:56 | INFO | fairseq.trainer | begin training epoch 460\n",
      "2023-04-17 14:13:56 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:14:04 | INFO | train_inner | epoch 460:     21 / 32 loss=2.42, nll_loss=0.259, ppl=1.2, wps=890, ups=1.31, wpb=677.1, bsz=2, num_updates=14700, lr=2.46415e-05, gnorm=3.231, clip=100, loss_scale=0.5, train_wall=36, gb_free=13.9, wall=12124\n",
      "2023-04-17 14:14:08 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:14:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:14:08 | INFO | valid | epoch 460 | valid on 'valid' subset | loss 5.805 | nll_loss 4.079 | ppl 16.9 | wps 6184.3 | wpb 290.8 | bsz 1 | num_updates 14711 | best_loss 3.979\n",
      "2023-04-17 14:14:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 460 @ 14711 updates\n",
      "2023-04-17 14:14:08 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:14:17 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:14:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 460 @ 14711 updates, score 5.805) (writing took 9.211796084069647 seconds)\n",
      "2023-04-17 14:14:17 | INFO | fairseq_cli.train | end of epoch 460 (average epoch stats below)\n",
      "2023-04-17 14:14:17 | INFO | train | epoch 460 | loss 2.42 | nll_loss 0.26 | ppl 1.2 | wps 1031.8 | ups 1.52 | wpb 678.5 | bsz 2 | num_updates 14711 | lr 2.46374e-05 | gnorm 3.325 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 12137\n",
      "2023-04-17 14:14:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:14:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:14:17 | INFO | fairseq.trainer | begin training epoch 461\n",
      "2023-04-17 14:14:17 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:14:29 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:14:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:14:30 | INFO | valid | epoch 461 | valid on 'valid' subset | loss 5.937 | nll_loss 4.224 | ppl 18.69 | wps 6100.7 | wpb 290.8 | bsz 1 | num_updates 14743 | best_loss 3.979\n",
      "2023-04-17 14:14:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 461 @ 14743 updates\n",
      "2023-04-17 14:14:30 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:14:42 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:14:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 461 @ 14743 updates, score 5.937) (writing took 12.858115712064318 seconds)\n",
      "2023-04-17 14:14:42 | INFO | fairseq_cli.train | end of epoch 461 (average epoch stats below)\n",
      "2023-04-17 14:14:42 | INFO | train | epoch 461 | loss 2.419 | nll_loss 0.258 | ppl 1.2 | wps 857.5 | ups 1.26 | wpb 678.5 | bsz 2 | num_updates 14743 | lr 2.46253e-05 | gnorm 3.179 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 12163\n",
      "2023-04-17 14:14:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:14:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:14:42 | INFO | fairseq.trainer | begin training epoch 462\n",
      "2023-04-17 14:14:42 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:14:54 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:14:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 14:14:55 | INFO | valid | epoch 462 | valid on 'valid' subset | loss 5.813 | nll_loss 4.087 | ppl 16.99 | wps 6158.7 | wpb 290.8 | bsz 1 | num_updates 14775 | best_loss 3.979\n",
      "2023-04-17 14:14:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 462 @ 14775 updates\n",
      "2023-04-17 14:14:55 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:15:07 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:15:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 462 @ 14775 updates, score 5.813) (writing took 13.036878042970784 seconds)\n",
      "2023-04-17 14:15:08 | INFO | fairseq_cli.train | end of epoch 462 (average epoch stats below)\n",
      "2023-04-17 14:15:08 | INFO | train | epoch 462 | loss 2.425 | nll_loss 0.265 | ppl 1.2 | wps 861.8 | ups 1.27 | wpb 678.5 | bsz 2 | num_updates 14775 | lr 2.46132e-05 | gnorm 3.239 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 12188\n",
      "2023-04-17 14:15:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:15:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:15:08 | INFO | fairseq.trainer | begin training epoch 463\n",
      "2023-04-17 14:15:08 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:15:17 | INFO | train_inner | epoch 463:     25 / 32 loss=2.42, nll_loss=0.259, ppl=1.2, wps=920.7, ups=1.37, wpb=671.1, bsz=2, num_updates=14800, lr=2.46038e-05, gnorm=3.203, clip=100, loss_scale=0.5, train_wall=37, gb_free=13.9, wall=12197\n",
      "2023-04-17 14:15:19 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:15:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:15:19 | INFO | valid | epoch 463 | valid on 'valid' subset | loss 5.922 | nll_loss 4.221 | ppl 18.65 | wps 7259.6 | wpb 290.8 | bsz 1 | num_updates 14807 | best_loss 3.979\n",
      "2023-04-17 14:15:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 463 @ 14807 updates\n",
      "2023-04-17 14:15:19 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:15:32 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:15:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 463 @ 14807 updates, score 5.922) (writing took 12.806559582008049 seconds)\n",
      "2023-04-17 14:15:32 | INFO | fairseq_cli.train | end of epoch 463 (average epoch stats below)\n",
      "2023-04-17 14:15:32 | INFO | train | epoch 463 | loss 2.414 | nll_loss 0.254 | ppl 1.19 | wps 894 | ups 1.32 | wpb 678.5 | bsz 2 | num_updates 14807 | lr 2.46011e-05 | gnorm 3.098 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 12212\n",
      "2023-04-17 14:15:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:15:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:15:32 | INFO | fairseq.trainer | begin training epoch 464\n",
      "2023-04-17 14:15:32 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:15:43 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:15:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:15:43 | INFO | valid | epoch 464 | valid on 'valid' subset | loss 5.904 | nll_loss 4.19 | ppl 18.25 | wps 7313.6 | wpb 290.8 | bsz 1 | num_updates 14839 | best_loss 3.979\n",
      "2023-04-17 14:15:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 464 @ 14839 updates\n",
      "2023-04-17 14:15:43 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:15:56 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:15:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 464 @ 14839 updates, score 5.904) (writing took 13.110856108018197 seconds)\n",
      "2023-04-17 14:15:56 | INFO | fairseq_cli.train | end of epoch 464 (average epoch stats below)\n",
      "2023-04-17 14:15:56 | INFO | train | epoch 464 | loss 2.421 | nll_loss 0.261 | ppl 1.2 | wps 888.1 | ups 1.31 | wpb 678.5 | bsz 2 | num_updates 14839 | lr 2.45891e-05 | gnorm 3.295 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 12237\n",
      "2023-04-17 14:15:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:15:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:15:56 | INFO | fairseq.trainer | begin training epoch 465\n",
      "2023-04-17 14:15:56 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:16:07 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:16:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:16:07 | INFO | valid | epoch 465 | valid on 'valid' subset | loss 5.898 | nll_loss 4.176 | ppl 18.08 | wps 7251.2 | wpb 290.8 | bsz 1 | num_updates 14871 | best_loss 3.979\n",
      "2023-04-17 14:16:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 465 @ 14871 updates\n",
      "2023-04-17 14:16:07 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:16:20 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:16:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 465 @ 14871 updates, score 5.898) (writing took 12.778780302032828 seconds)\n",
      "2023-04-17 14:16:20 | INFO | fairseq_cli.train | end of epoch 465 (average epoch stats below)\n",
      "2023-04-17 14:16:20 | INFO | train | epoch 465 | loss 2.421 | nll_loss 0.263 | ppl 1.2 | wps 918.5 | ups 1.35 | wpb 678.5 | bsz 2 | num_updates 14871 | lr 2.4577e-05 | gnorm 3.021 | clip 100 | loss_scale 0.5 | train_wall 10 | gb_free 13.9 | wall 12260\n",
      "2023-04-17 14:16:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:16:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:16:20 | INFO | fairseq.trainer | begin training epoch 466\n",
      "2023-04-17 14:16:20 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:16:30 | INFO | train_inner | epoch 466:     29 / 32 loss=2.42, nll_loss=0.259, ppl=1.2, wps=930.7, ups=1.36, wpb=684.9, bsz=2, num_updates=14900, lr=2.4566e-05, gnorm=3.16, clip=100, loss_scale=0.5, train_wall=34, gb_free=13.9, wall=12270\n",
      "2023-04-17 14:16:31 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:16:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:16:31 | INFO | valid | epoch 466 | valid on 'valid' subset | loss 5.878 | nll_loss 4.157 | ppl 17.84 | wps 6824.7 | wpb 290.8 | bsz 1 | num_updates 14903 | best_loss 3.979\n",
      "2023-04-17 14:16:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 466 @ 14903 updates\n",
      "2023-04-17 14:16:31 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:16:44 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:16:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 466 @ 14903 updates, score 5.878) (writing took 13.030753990984522 seconds)\n",
      "2023-04-17 14:16:44 | INFO | fairseq_cli.train | end of epoch 466 (average epoch stats below)\n",
      "2023-04-17 14:16:44 | INFO | train | epoch 466 | loss 2.417 | nll_loss 0.256 | ppl 1.19 | wps 891.3 | ups 1.31 | wpb 678.5 | bsz 2 | num_updates 14903 | lr 2.45649e-05 | gnorm 3.134 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 12285\n",
      "2023-04-17 14:16:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:16:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:16:44 | INFO | fairseq.trainer | begin training epoch 467\n",
      "2023-04-17 14:16:44 | INFO | fairseq_cli.train | Start iterating over samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 14:16:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:16:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:16:56 | INFO | valid | epoch 467 | valid on 'valid' subset | loss 5.813 | nll_loss 4.084 | ppl 16.96 | wps 7162.3 | wpb 290.8 | bsz 1 | num_updates 14935 | best_loss 3.979\n",
      "2023-04-17 14:16:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 467 @ 14935 updates\n",
      "2023-04-17 14:16:56 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:17:08 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:17:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 467 @ 14935 updates, score 5.813) (writing took 12.707713095005602 seconds)\n",
      "2023-04-17 14:17:08 | INFO | fairseq_cli.train | end of epoch 467 (average epoch stats below)\n",
      "2023-04-17 14:17:08 | INFO | train | epoch 467 | loss 2.423 | nll_loss 0.265 | ppl 1.2 | wps 903 | ups 1.33 | wpb 678.5 | bsz 2 | num_updates 14935 | lr 2.45528e-05 | gnorm 3.184 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 12309\n",
      "2023-04-17 14:17:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:17:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:17:08 | INFO | fairseq.trainer | begin training epoch 468\n",
      "2023-04-17 14:17:08 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:17:20 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:17:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:17:20 | INFO | valid | epoch 468 | valid on 'valid' subset | loss 5.854 | nll_loss 4.125 | ppl 17.45 | wps 7072.8 | wpb 290.8 | bsz 1 | num_updates 14967 | best_loss 3.979\n",
      "2023-04-17 14:17:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 468 @ 14967 updates\n",
      "2023-04-17 14:17:20 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:17:33 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:17:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 468 @ 14967 updates, score 5.854) (writing took 12.967558082076721 seconds)\n",
      "2023-04-17 14:17:33 | INFO | fairseq_cli.train | end of epoch 468 (average epoch stats below)\n",
      "2023-04-17 14:17:33 | INFO | train | epoch 468 | loss 2.416 | nll_loss 0.255 | ppl 1.19 | wps 892.6 | ups 1.32 | wpb 678.5 | bsz 2 | num_updates 14967 | lr 2.45408e-05 | gnorm 3.099 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 12333\n",
      "2023-04-17 14:17:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:17:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:17:33 | INFO | fairseq.trainer | begin training epoch 469\n",
      "2023-04-17 14:17:33 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:17:44 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:17:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:17:44 | INFO | valid | epoch 469 | valid on 'valid' subset | loss 5.789 | nll_loss 4.047 | ppl 16.53 | wps 7057.7 | wpb 290.8 | bsz 1 | num_updates 14999 | best_loss 3.979\n",
      "2023-04-17 14:17:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 469 @ 14999 updates\n",
      "2023-04-17 14:17:44 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:17:57 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:17:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 469 @ 14999 updates, score 5.789) (writing took 12.877014716039412 seconds)\n",
      "2023-04-17 14:17:57 | INFO | fairseq_cli.train | end of epoch 469 (average epoch stats below)\n",
      "2023-04-17 14:17:57 | INFO | train | epoch 469 | loss 2.421 | nll_loss 0.262 | ppl 1.2 | wps 892.5 | ups 1.32 | wpb 678.5 | bsz 2 | num_updates 14999 | lr 2.45287e-05 | gnorm 3.128 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 12357\n",
      "2023-04-17 14:17:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:17:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:17:57 | INFO | fairseq.trainer | begin training epoch 470\n",
      "2023-04-17 14:17:57 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:17:57 | INFO | train_inner | epoch 470:      1 / 32 loss=2.42, nll_loss=0.26, ppl=1.2, wps=771.3, ups=1.15, wpb=673.3, bsz=2, num_updates=15000, lr=2.45283e-05, gnorm=3.14, clip=100, loss_scale=0.5, train_wall=34, gb_free=13.9, wall=12358\n",
      "2023-04-17 14:17:57 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:17:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:17:58 | INFO | valid | epoch 470 | valid on 'valid' subset | loss 5.787 | nll_loss 4.049 | ppl 16.55 | wps 7063.9 | wpb 290.8 | bsz 1 | num_updates 15000 | best_loss 3.979\n",
      "2023-04-17 14:17:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 470 @ 15000 updates\n",
      "2023-04-17 14:17:58 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_470_15000.pt\n",
      "2023-04-17 14:18:06 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_470_15000.pt\n",
      "2023-04-17 14:18:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_470_15000.pt (epoch 470 @ 15000 updates, score 5.787) (writing took 15.752219054033048 seconds)\n",
      "2023-04-17 14:18:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:18:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:18:26 | INFO | valid | epoch 470 | valid on 'valid' subset | loss 5.885 | nll_loss 4.163 | ppl 17.91 | wps 7081 | wpb 290.8 | bsz 1 | num_updates 15031 | best_loss 3.979\n",
      "2023-04-17 14:18:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 470 @ 15031 updates\n",
      "2023-04-17 14:18:26 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:18:39 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:18:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 470 @ 15031 updates, score 5.885) (writing took 14.138502136920579 seconds)\n",
      "2023-04-17 14:18:40 | INFO | fairseq_cli.train | end of epoch 470 (average epoch stats below)\n",
      "2023-04-17 14:18:40 | INFO | train | epoch 470 | loss 2.411 | nll_loss 0.251 | ppl 1.19 | wps 509.2 | ups 0.75 | wpb 678.5 | bsz 2 | num_updates 15031 | lr 2.45166e-05 | gnorm 3.097 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 12400\n",
      "2023-04-17 14:18:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:18:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:18:40 | INFO | fairseq.trainer | begin training epoch 471\n",
      "2023-04-17 14:18:40 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:18:51 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:18:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:18:51 | INFO | valid | epoch 471 | valid on 'valid' subset | loss 5.833 | nll_loss 4.113 | ppl 17.31 | wps 5269.7 | wpb 290.8 | bsz 1 | num_updates 15063 | best_loss 3.979\n",
      "2023-04-17 14:18:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 471 @ 15063 updates\n",
      "2023-04-17 14:18:51 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:19:05 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 14:19:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 471 @ 15063 updates, score 5.833) (writing took 14.530070435954258 seconds)\n",
      "2023-04-17 14:19:06 | INFO | fairseq_cli.train | end of epoch 471 (average epoch stats below)\n",
      "2023-04-17 14:19:06 | INFO | train | epoch 471 | loss 2.413 | nll_loss 0.252 | ppl 1.19 | wps 837 | ups 1.23 | wpb 678.5 | bsz 2 | num_updates 15063 | lr 2.45045e-05 | gnorm 3.024 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 12426\n",
      "2023-04-17 14:19:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:19:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:19:06 | INFO | fairseq.trainer | begin training epoch 472\n",
      "2023-04-17 14:19:06 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:19:17 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:19:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:19:18 | INFO | valid | epoch 472 | valid on 'valid' subset | loss 5.905 | nll_loss 4.188 | ppl 18.22 | wps 5729.1 | wpb 290.8 | bsz 1 | num_updates 15095 | best_loss 3.979\n",
      "2023-04-17 14:19:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 472 @ 15095 updates\n",
      "2023-04-17 14:19:18 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:19:30 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:19:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 472 @ 15095 updates, score 5.905) (writing took 12.90697359002661 seconds)\n",
      "2023-04-17 14:19:31 | INFO | fairseq_cli.train | end of epoch 472 (average epoch stats below)\n",
      "2023-04-17 14:19:31 | INFO | train | epoch 472 | loss 2.414 | nll_loss 0.254 | ppl 1.19 | wps 870.8 | ups 1.28 | wpb 678.5 | bsz 2 | num_updates 15095 | lr 2.44925e-05 | gnorm 2.844 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 12451\n",
      "2023-04-17 14:19:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:19:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:19:31 | INFO | fairseq.trainer | begin training epoch 473\n",
      "2023-04-17 14:19:31 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:19:32 | INFO | train_inner | epoch 473:      5 / 32 loss=2.413, nll_loss=0.253, ppl=1.19, wps=717.4, ups=1.05, wpb=681.6, bsz=2, num_updates=15100, lr=2.44906e-05, gnorm=3.018, clip=100, loss_scale=0.5, train_wall=36, gb_free=13.9, wall=12453\n",
      "2023-04-17 14:19:42 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:19:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:19:42 | INFO | valid | epoch 473 | valid on 'valid' subset | loss 5.874 | nll_loss 4.16 | ppl 17.88 | wps 7256.7 | wpb 290.8 | bsz 1 | num_updates 15127 | best_loss 3.979\n",
      "2023-04-17 14:19:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 473 @ 15127 updates\n",
      "2023-04-17 14:19:42 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:19:55 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:19:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 473 @ 15127 updates, score 5.874) (writing took 12.91657438594848 seconds)\n",
      "2023-04-17 14:19:55 | INFO | fairseq_cli.train | end of epoch 473 (average epoch stats below)\n",
      "2023-04-17 14:19:55 | INFO | train | epoch 473 | loss 2.416 | nll_loss 0.256 | ppl 1.19 | wps 879.7 | ups 1.3 | wpb 678.5 | bsz 2 | num_updates 15127 | lr 2.44804e-05 | gnorm 3.133 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 12476\n",
      "2023-04-17 14:19:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:19:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:19:55 | INFO | fairseq.trainer | begin training epoch 474\n",
      "2023-04-17 14:19:55 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:20:07 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:20:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:20:07 | INFO | valid | epoch 474 | valid on 'valid' subset | loss 5.851 | nll_loss 4.122 | ppl 17.41 | wps 6888.3 | wpb 290.8 | bsz 1 | num_updates 15159 | best_loss 3.979\n",
      "2023-04-17 14:20:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 474 @ 15159 updates\n",
      "2023-04-17 14:20:07 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:20:21 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:20:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 474 @ 15159 updates, score 5.851) (writing took 13.726630645920523 seconds)\n",
      "2023-04-17 14:20:21 | INFO | fairseq_cli.train | end of epoch 474 (average epoch stats below)\n",
      "2023-04-17 14:20:21 | INFO | train | epoch 474 | loss 2.411 | nll_loss 0.249 | ppl 1.19 | wps 847 | ups 1.25 | wpb 678.5 | bsz 2 | num_updates 15159 | lr 2.44683e-05 | gnorm 3.029 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 12501\n",
      "2023-04-17 14:20:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:20:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:20:21 | INFO | fairseq.trainer | begin training epoch 475\n",
      "2023-04-17 14:20:21 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:20:33 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:20:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:20:33 | INFO | valid | epoch 475 | valid on 'valid' subset | loss 5.861 | nll_loss 4.142 | ppl 17.66 | wps 7336.5 | wpb 290.8 | bsz 1 | num_updates 15191 | best_loss 3.979\n",
      "2023-04-17 14:20:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 475 @ 15191 updates\n",
      "2023-04-17 14:20:33 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:20:45 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:20:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 475 @ 15191 updates, score 5.861) (writing took 12.429964472074062 seconds)\n",
      "2023-04-17 14:20:46 | INFO | fairseq_cli.train | end of epoch 475 (average epoch stats below)\n",
      "2023-04-17 14:20:46 | INFO | train | epoch 475 | loss 2.41 | nll_loss 0.251 | ppl 1.19 | wps 881.3 | ups 1.3 | wpb 678.5 | bsz 2 | num_updates 15191 | lr 2.44562e-05 | gnorm 3.042 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 12526\n",
      "2023-04-17 14:20:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:20:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:20:46 | INFO | fairseq.trainer | begin training epoch 476\n",
      "2023-04-17 14:20:46 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:20:49 | INFO | train_inner | epoch 476:      9 / 32 loss=2.412, nll_loss=0.252, ppl=1.19, wps=892.2, ups=1.31, wpb=681.2, bsz=2, num_updates=15200, lr=2.44528e-05, gnorm=3.023, clip=100, loss_scale=0.5, train_wall=36, gb_free=13.9, wall=12529\n",
      "2023-04-17 14:20:57 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:20:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:20:57 | INFO | valid | epoch 476 | valid on 'valid' subset | loss 5.888 | nll_loss 4.177 | ppl 18.08 | wps 7134.1 | wpb 290.8 | bsz 1 | num_updates 15223 | best_loss 3.979\n",
      "2023-04-17 14:20:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 476 @ 15223 updates\n",
      "2023-04-17 14:20:57 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 14:21:09 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:21:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 476 @ 15223 updates, score 5.888) (writing took 12.417720625991933 seconds)\n",
      "2023-04-17 14:21:10 | INFO | fairseq_cli.train | end of epoch 476 (average epoch stats below)\n",
      "2023-04-17 14:21:10 | INFO | train | epoch 476 | loss 2.415 | nll_loss 0.254 | ppl 1.19 | wps 904.2 | ups 1.33 | wpb 678.5 | bsz 2 | num_updates 15223 | lr 2.44442e-05 | gnorm 3.052 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 12550\n",
      "2023-04-17 14:21:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:21:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:21:10 | INFO | fairseq.trainer | begin training epoch 477\n",
      "2023-04-17 14:21:10 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:21:21 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:21:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:21:22 | INFO | valid | epoch 477 | valid on 'valid' subset | loss 5.875 | nll_loss 4.163 | ppl 17.92 | wps 7010.5 | wpb 290.8 | bsz 1 | num_updates 15255 | best_loss 3.979\n",
      "2023-04-17 14:21:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 477 @ 15255 updates\n",
      "2023-04-17 14:21:22 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:21:34 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:21:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 477 @ 15255 updates, score 5.875) (writing took 12.181553435977548 seconds)\n",
      "2023-04-17 14:21:34 | INFO | fairseq_cli.train | end of epoch 477 (average epoch stats below)\n",
      "2023-04-17 14:21:34 | INFO | train | epoch 477 | loss 2.411 | nll_loss 0.251 | ppl 1.19 | wps 894.6 | ups 1.32 | wpb 678.5 | bsz 2 | num_updates 15255 | lr 2.44321e-05 | gnorm 3.043 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 12574\n",
      "2023-04-17 14:21:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:21:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:21:34 | INFO | fairseq.trainer | begin training epoch 478\n",
      "2023-04-17 14:21:34 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:21:45 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:21:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:21:46 | INFO | valid | epoch 478 | valid on 'valid' subset | loss 5.797 | nll_loss 4.074 | ppl 16.84 | wps 7082.3 | wpb 290.8 | bsz 1 | num_updates 15287 | best_loss 3.979\n",
      "2023-04-17 14:21:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 478 @ 15287 updates\n",
      "2023-04-17 14:21:46 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:21:54 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:21:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 478 @ 15287 updates, score 5.797) (writing took 8.746734476066194 seconds)\n",
      "2023-04-17 14:21:54 | INFO | fairseq_cli.train | end of epoch 478 (average epoch stats below)\n",
      "2023-04-17 14:21:54 | INFO | train | epoch 478 | loss 2.409 | nll_loss 0.25 | ppl 1.19 | wps 1054.7 | ups 1.55 | wpb 678.5 | bsz 2 | num_updates 15287 | lr 2.442e-05 | gnorm 2.899 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 12595\n",
      "2023-04-17 14:21:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:21:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:21:54 | INFO | fairseq.trainer | begin training epoch 479\n",
      "2023-04-17 14:21:54 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:21:59 | INFO | train_inner | epoch 479:     13 / 32 loss=2.41, nll_loss=0.251, ppl=1.19, wps=976.2, ups=1.42, wpb=688.1, bsz=2, num_updates=15300, lr=2.44151e-05, gnorm=2.978, clip=100, loss_scale=0.5, train_wall=36, gb_free=13.9, wall=12600\n",
      "2023-04-17 14:22:06 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:22:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:22:07 | INFO | valid | epoch 479 | valid on 'valid' subset | loss 5.864 | nll_loss 4.151 | ppl 17.77 | wps 6668.1 | wpb 290.8 | bsz 1 | num_updates 15319 | best_loss 3.979\n",
      "2023-04-17 14:22:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 479 @ 15319 updates\n",
      "2023-04-17 14:22:07 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:22:19 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:22:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 479 @ 15319 updates, score 5.864) (writing took 12.41524041199591 seconds)\n",
      "2023-04-17 14:22:19 | INFO | fairseq_cli.train | end of epoch 479 (average epoch stats below)\n",
      "2023-04-17 14:22:19 | INFO | train | epoch 479 | loss 2.409 | nll_loss 0.251 | ppl 1.19 | wps 884.8 | ups 1.3 | wpb 678.5 | bsz 2 | num_updates 15319 | lr 2.44079e-05 | gnorm 3.055 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 12619\n",
      "2023-04-17 14:22:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:22:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:22:19 | INFO | fairseq.trainer | begin training epoch 480\n",
      "2023-04-17 14:22:19 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:22:31 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:22:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:22:32 | INFO | valid | epoch 480 | valid on 'valid' subset | loss 5.826 | nll_loss 4.099 | ppl 17.13 | wps 7141.1 | wpb 290.8 | bsz 1 | num_updates 15351 | best_loss 3.979\n",
      "2023-04-17 14:22:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 480 @ 15351 updates\n",
      "2023-04-17 14:22:32 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:22:44 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:22:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 480 @ 15351 updates, score 5.826) (writing took 12.636323762009852 seconds)\n",
      "2023-04-17 14:22:44 | INFO | fairseq_cli.train | end of epoch 480 (average epoch stats below)\n",
      "2023-04-17 14:22:44 | INFO | train | epoch 480 | loss 2.412 | nll_loss 0.252 | ppl 1.19 | wps 861.7 | ups 1.27 | wpb 678.5 | bsz 2 | num_updates 15351 | lr 2.43958e-05 | gnorm 3.13 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 12644\n",
      "2023-04-17 14:22:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:22:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:22:44 | INFO | fairseq.trainer | begin training epoch 481\n",
      "2023-04-17 14:22:44 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:22:56 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:22:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:22:57 | INFO | valid | epoch 481 | valid on 'valid' subset | loss 5.873 | nll_loss 4.156 | ppl 17.83 | wps 6168.7 | wpb 290.8 | bsz 1 | num_updates 15383 | best_loss 3.979\n",
      "2023-04-17 14:22:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 481 @ 15383 updates\n",
      "2023-04-17 14:22:57 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 14:23:10 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:23:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 481 @ 15383 updates, score 5.873) (writing took 13.12052166101057 seconds)\n",
      "2023-04-17 14:23:10 | INFO | fairseq_cli.train | end of epoch 481 (average epoch stats below)\n",
      "2023-04-17 14:23:10 | INFO | train | epoch 481 | loss 2.41 | nll_loss 0.25 | ppl 1.19 | wps 845 | ups 1.25 | wpb 678.5 | bsz 2 | num_updates 15383 | lr 2.43838e-05 | gnorm 3.007 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 12670\n",
      "2023-04-17 14:23:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:23:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:23:10 | INFO | fairseq.trainer | begin training epoch 482\n",
      "2023-04-17 14:23:10 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:23:16 | INFO | train_inner | epoch 482:     17 / 32 loss=2.412, nll_loss=0.253, ppl=1.19, wps=865.2, ups=1.3, wpb=666.8, bsz=2, num_updates=15400, lr=2.43774e-05, gnorm=3.134, clip=100, loss_scale=0.5, train_wall=38, gb_free=13.9, wall=12677\n",
      "2023-04-17 14:23:22 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:23:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:23:22 | INFO | valid | epoch 482 | valid on 'valid' subset | loss 5.834 | nll_loss 4.119 | ppl 17.38 | wps 7174.2 | wpb 290.8 | bsz 1 | num_updates 15415 | best_loss 3.979\n",
      "2023-04-17 14:23:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 482 @ 15415 updates\n",
      "2023-04-17 14:23:22 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:23:37 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:23:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 482 @ 15415 updates, score 5.834) (writing took 14.415659835096449 seconds)\n",
      "2023-04-17 14:23:37 | INFO | fairseq_cli.train | end of epoch 482 (average epoch stats below)\n",
      "2023-04-17 14:23:37 | INFO | train | epoch 482 | loss 2.412 | nll_loss 0.254 | ppl 1.19 | wps 803.7 | ups 1.18 | wpb 678.5 | bsz 2 | num_updates 15415 | lr 2.43717e-05 | gnorm 3.086 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 12697\n",
      "2023-04-17 14:23:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:23:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:23:37 | INFO | fairseq.trainer | begin training epoch 483\n",
      "2023-04-17 14:23:37 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:23:49 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:23:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:23:50 | INFO | valid | epoch 483 | valid on 'valid' subset | loss 5.876 | nll_loss 4.154 | ppl 17.8 | wps 5811.3 | wpb 290.8 | bsz 1 | num_updates 15447 | best_loss 3.979\n",
      "2023-04-17 14:23:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 483 @ 15447 updates\n",
      "2023-04-17 14:23:50 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:24:02 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:24:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 483 @ 15447 updates, score 5.876) (writing took 12.985579542000778 seconds)\n",
      "2023-04-17 14:24:03 | INFO | fairseq_cli.train | end of epoch 483 (average epoch stats below)\n",
      "2023-04-17 14:24:03 | INFO | train | epoch 483 | loss 2.405 | nll_loss 0.246 | ppl 1.19 | wps 844.5 | ups 1.24 | wpb 678.5 | bsz 2 | num_updates 15447 | lr 2.43596e-05 | gnorm 2.882 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 12723\n",
      "2023-04-17 14:24:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:24:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:24:03 | INFO | fairseq.trainer | begin training epoch 484\n",
      "2023-04-17 14:24:03 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:24:15 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:24:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:24:15 | INFO | valid | epoch 484 | valid on 'valid' subset | loss 5.855 | nll_loss 4.141 | ppl 17.64 | wps 6982.7 | wpb 290.8 | bsz 1 | num_updates 15479 | best_loss 3.979\n",
      "2023-04-17 14:24:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 484 @ 15479 updates\n",
      "2023-04-17 14:24:15 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:24:29 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:24:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 484 @ 15479 updates, score 5.855) (writing took 14.522823915001936 seconds)\n",
      "2023-04-17 14:24:30 | INFO | fairseq_cli.train | end of epoch 484 (average epoch stats below)\n",
      "2023-04-17 14:24:30 | INFO | train | epoch 484 | loss 2.411 | nll_loss 0.251 | ppl 1.19 | wps 806.5 | ups 1.19 | wpb 678.5 | bsz 2 | num_updates 15479 | lr 2.43475e-05 | gnorm 3.022 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 12750\n",
      "2023-04-17 14:24:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:24:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:24:30 | INFO | fairseq.trainer | begin training epoch 485\n",
      "2023-04-17 14:24:30 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:24:37 | INFO | train_inner | epoch 485:     21 / 32 loss=2.408, nll_loss=0.249, ppl=1.19, wps=842.4, ups=1.23, wpb=682.3, bsz=2, num_updates=15500, lr=2.43396e-05, gnorm=2.902, clip=100, loss_scale=0.5, train_wall=38, gb_free=13.9, wall=12758\n",
      "2023-04-17 14:24:41 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:24:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:24:41 | INFO | valid | epoch 485 | valid on 'valid' subset | loss 5.944 | nll_loss 4.236 | ppl 18.84 | wps 7331.2 | wpb 290.8 | bsz 1 | num_updates 15511 | best_loss 3.979\n",
      "2023-04-17 14:24:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 485 @ 15511 updates\n",
      "2023-04-17 14:24:41 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:24:54 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:24:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 485 @ 15511 updates, score 5.944) (writing took 12.744721569935791 seconds)\n",
      "2023-04-17 14:24:54 | INFO | fairseq_cli.train | end of epoch 485 (average epoch stats below)\n",
      "2023-04-17 14:24:54 | INFO | train | epoch 485 | loss 2.406 | nll_loss 0.247 | ppl 1.19 | wps 882.3 | ups 1.3 | wpb 678.5 | bsz 2 | num_updates 15511 | lr 2.43355e-05 | gnorm 2.755 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 12774\n",
      "2023-04-17 14:24:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:24:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:24:54 | INFO | fairseq.trainer | begin training epoch 486\n",
      "2023-04-17 14:24:54 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:25:06 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:25:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:25:06 | INFO | valid | epoch 486 | valid on 'valid' subset | loss 5.93 | nll_loss 4.222 | ppl 18.67 | wps 6385.8 | wpb 290.8 | bsz 1 | num_updates 15543 | best_loss 3.979\n",
      "2023-04-17 14:25:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 486 @ 15543 updates\n",
      "2023-04-17 14:25:06 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 14:25:18 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:25:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 486 @ 15543 updates, score 5.93) (writing took 12.703949862974696 seconds)\n",
      "2023-04-17 14:25:19 | INFO | fairseq_cli.train | end of epoch 486 (average epoch stats below)\n",
      "2023-04-17 14:25:19 | INFO | train | epoch 486 | loss 2.407 | nll_loss 0.248 | ppl 1.19 | wps 885.1 | ups 1.3 | wpb 678.5 | bsz 2 | num_updates 15543 | lr 2.43234e-05 | gnorm 3.025 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 12799\n",
      "2023-04-17 14:25:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:25:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:25:19 | INFO | fairseq.trainer | begin training epoch 487\n",
      "2023-04-17 14:25:19 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:25:30 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:25:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:25:31 | INFO | valid | epoch 487 | valid on 'valid' subset | loss 5.923 | nll_loss 4.216 | ppl 18.58 | wps 6131.2 | wpb 290.8 | bsz 1 | num_updates 15575 | best_loss 3.979\n",
      "2023-04-17 14:25:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 487 @ 15575 updates\n",
      "2023-04-17 14:25:31 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:25:43 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:25:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 487 @ 15575 updates, score 5.923) (writing took 12.445749926031567 seconds)\n",
      "2023-04-17 14:25:43 | INFO | fairseq_cli.train | end of epoch 487 (average epoch stats below)\n",
      "2023-04-17 14:25:43 | INFO | train | epoch 487 | loss 2.404 | nll_loss 0.244 | ppl 1.18 | wps 886.3 | ups 1.31 | wpb 678.5 | bsz 2 | num_updates 15575 | lr 2.43113e-05 | gnorm 2.834 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 12823\n",
      "2023-04-17 14:25:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:25:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:25:43 | INFO | fairseq.trainer | begin training epoch 488\n",
      "2023-04-17 14:25:43 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:25:52 | INFO | train_inner | epoch 488:     25 / 32 loss=2.406, nll_loss=0.247, ppl=1.19, wps=899.6, ups=1.34, wpb=673.5, bsz=2, num_updates=15600, lr=2.43019e-05, gnorm=3.014, clip=100, loss_scale=0.5, train_wall=36, gb_free=13.9, wall=12832\n",
      "2023-04-17 14:25:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:25:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:25:55 | INFO | valid | epoch 488 | valid on 'valid' subset | loss 5.875 | nll_loss 4.161 | ppl 17.89 | wps 7241.7 | wpb 290.8 | bsz 1 | num_updates 15607 | best_loss 3.979\n",
      "2023-04-17 14:25:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 488 @ 15607 updates\n",
      "2023-04-17 14:25:55 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:26:07 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:26:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 488 @ 15607 updates, score 5.875) (writing took 12.254221496055834 seconds)\n",
      "2023-04-17 14:26:07 | INFO | fairseq_cli.train | end of epoch 488 (average epoch stats below)\n",
      "2023-04-17 14:26:07 | INFO | train | epoch 488 | loss 2.409 | nll_loss 0.25 | ppl 1.19 | wps 909.3 | ups 1.34 | wpb 678.5 | bsz 2 | num_updates 15607 | lr 2.42992e-05 | gnorm 3.228 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 12847\n",
      "2023-04-17 14:26:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:26:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:26:07 | INFO | fairseq.trainer | begin training epoch 489\n",
      "2023-04-17 14:26:07 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:26:18 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:26:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:26:19 | INFO | valid | epoch 489 | valid on 'valid' subset | loss 5.953 | nll_loss 4.246 | ppl 18.97 | wps 7302.4 | wpb 290.8 | bsz 1 | num_updates 15639 | best_loss 3.979\n",
      "2023-04-17 14:26:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 489 @ 15639 updates\n",
      "2023-04-17 14:26:19 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:26:30 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:26:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 489 @ 15639 updates, score 5.953) (writing took 12.013128080987372 seconds)\n",
      "2023-04-17 14:26:31 | INFO | fairseq_cli.train | end of epoch 489 (average epoch stats below)\n",
      "2023-04-17 14:26:31 | INFO | train | epoch 489 | loss 2.409 | nll_loss 0.25 | ppl 1.19 | wps 923.9 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 15639 | lr 2.42872e-05 | gnorm 3.052 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 12871\n",
      "2023-04-17 14:26:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:26:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:26:31 | INFO | fairseq.trainer | begin training epoch 490\n",
      "2023-04-17 14:26:31 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:26:42 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:26:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:26:42 | INFO | valid | epoch 490 | valid on 'valid' subset | loss 5.924 | nll_loss 4.213 | ppl 18.54 | wps 7190.9 | wpb 290.8 | bsz 1 | num_updates 15671 | best_loss 3.979\n",
      "2023-04-17 14:26:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 490 @ 15671 updates\n",
      "2023-04-17 14:26:42 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:26:54 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:26:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 490 @ 15671 updates, score 5.924) (writing took 12.192186399013735 seconds)\n",
      "2023-04-17 14:26:54 | INFO | fairseq_cli.train | end of epoch 490 (average epoch stats below)\n",
      "2023-04-17 14:26:54 | INFO | train | epoch 490 | loss 2.406 | nll_loss 0.247 | ppl 1.19 | wps 918.9 | ups 1.35 | wpb 678.5 | bsz 2 | num_updates 15671 | lr 2.42751e-05 | gnorm 3.001 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 12894\n",
      "2023-04-17 14:26:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:26:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:26:54 | INFO | fairseq.trainer | begin training epoch 491\n",
      "2023-04-17 14:26:54 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:27:04 | INFO | train_inner | epoch 491:     29 / 32 loss=2.407, nll_loss=0.248, ppl=1.19, wps=939.6, ups=1.38, wpb=678.5, bsz=2, num_updates=15700, lr=2.42642e-05, gnorm=2.951, clip=100, loss_scale=0.5, train_wall=35, gb_free=13.9, wall=12905\n",
      "2023-04-17 14:27:05 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:27:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:27:06 | INFO | valid | epoch 491 | valid on 'valid' subset | loss 5.936 | nll_loss 4.222 | ppl 18.66 | wps 7280.4 | wpb 290.8 | bsz 1 | num_updates 15703 | best_loss 3.979\n",
      "2023-04-17 14:27:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 491 @ 15703 updates\n",
      "2023-04-17 14:27:06 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 14:27:18 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:27:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 491 @ 15703 updates, score 5.936) (writing took 12.151281126076356 seconds)\n",
      "2023-04-17 14:27:18 | INFO | fairseq_cli.train | end of epoch 491 (average epoch stats below)\n",
      "2023-04-17 14:27:18 | INFO | train | epoch 491 | loss 2.406 | nll_loss 0.247 | ppl 1.19 | wps 920.7 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 15703 | lr 2.4263e-05 | gnorm 2.866 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 12918\n",
      "2023-04-17 14:27:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:27:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:27:18 | INFO | fairseq.trainer | begin training epoch 492\n",
      "2023-04-17 14:27:18 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:27:29 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:27:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:27:29 | INFO | valid | epoch 492 | valid on 'valid' subset | loss 5.907 | nll_loss 4.186 | ppl 18.2 | wps 7194 | wpb 290.8 | bsz 1 | num_updates 15735 | best_loss 3.979\n",
      "2023-04-17 14:27:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 492 @ 15735 updates\n",
      "2023-04-17 14:27:29 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:27:42 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:27:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 492 @ 15735 updates, score 5.907) (writing took 12.837556028971449 seconds)\n",
      "2023-04-17 14:27:42 | INFO | fairseq_cli.train | end of epoch 492 (average epoch stats below)\n",
      "2023-04-17 14:27:42 | INFO | train | epoch 492 | loss 2.404 | nll_loss 0.245 | ppl 1.19 | wps 883.9 | ups 1.3 | wpb 678.5 | bsz 2 | num_updates 15735 | lr 2.42509e-05 | gnorm 3.057 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 12943\n",
      "2023-04-17 14:27:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:27:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:27:42 | INFO | fairseq.trainer | begin training epoch 493\n",
      "2023-04-17 14:27:42 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:27:54 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:27:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:27:54 | INFO | valid | epoch 493 | valid on 'valid' subset | loss 5.922 | nll_loss 4.221 | ppl 18.64 | wps 6986.1 | wpb 290.8 | bsz 1 | num_updates 15767 | best_loss 3.979\n",
      "2023-04-17 14:27:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 493 @ 15767 updates\n",
      "2023-04-17 14:27:54 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:28:05 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:28:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 493 @ 15767 updates, score 5.922) (writing took 11.84851108607836 seconds)\n",
      "2023-04-17 14:28:06 | INFO | fairseq_cli.train | end of epoch 493 (average epoch stats below)\n",
      "2023-04-17 14:28:06 | INFO | train | epoch 493 | loss 2.407 | nll_loss 0.247 | ppl 1.19 | wps 931.1 | ups 1.37 | wpb 678.5 | bsz 2 | num_updates 15767 | lr 2.42389e-05 | gnorm 3.026 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 12966\n",
      "2023-04-17 14:28:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:28:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:28:06 | INFO | fairseq.trainer | begin training epoch 494\n",
      "2023-04-17 14:28:06 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:28:18 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:28:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:28:19 | INFO | valid | epoch 494 | valid on 'valid' subset | loss 5.92 | nll_loss 4.209 | ppl 18.5 | wps 6040.5 | wpb 290.8 | bsz 1 | num_updates 15799 | best_loss 3.979\n",
      "2023-04-17 14:28:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 494 @ 15799 updates\n",
      "2023-04-17 14:28:19 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:28:31 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:28:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 494 @ 15799 updates, score 5.92) (writing took 12.29997633898165 seconds)\n",
      "2023-04-17 14:28:31 | INFO | fairseq_cli.train | end of epoch 494 (average epoch stats below)\n",
      "2023-04-17 14:28:31 | INFO | train | epoch 494 | loss 2.405 | nll_loss 0.246 | ppl 1.19 | wps 859.2 | ups 1.27 | wpb 678.5 | bsz 2 | num_updates 15799 | lr 2.42268e-05 | gnorm 3.091 | clip 100 | loss_scale 0.5 | train_wall 13 | gb_free 13.9 | wall 12991\n",
      "2023-04-17 14:28:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:28:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:28:31 | INFO | fairseq.trainer | begin training epoch 495\n",
      "2023-04-17 14:28:31 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:28:31 | INFO | train_inner | epoch 495:      1 / 32 loss=2.405, nll_loss=0.246, ppl=1.19, wps=780.6, ups=1.15, wpb=678.3, bsz=2, num_updates=15800, lr=2.42264e-05, gnorm=3.072, clip=100, loss_scale=0.5, train_wall=36, gb_free=13.9, wall=12992\n",
      "2023-04-17 14:28:43 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:28:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:28:43 | INFO | valid | epoch 495 | valid on 'valid' subset | loss 5.864 | nll_loss 4.134 | ppl 17.56 | wps 7287.6 | wpb 290.8 | bsz 1 | num_updates 15831 | best_loss 3.979\n",
      "2023-04-17 14:28:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 495 @ 15831 updates\n",
      "2023-04-17 14:28:43 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:28:56 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:28:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 495 @ 15831 updates, score 5.864) (writing took 12.452711964957416 seconds)\n",
      "2023-04-17 14:28:56 | INFO | fairseq_cli.train | end of epoch 495 (average epoch stats below)\n",
      "2023-04-17 14:28:56 | INFO | train | epoch 495 | loss 2.407 | nll_loss 0.25 | ppl 1.19 | wps 870.7 | ups 1.28 | wpb 678.5 | bsz 2 | num_updates 15831 | lr 2.42147e-05 | gnorm 3.021 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 13016\n",
      "2023-04-17 14:28:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:28:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:28:56 | INFO | fairseq.trainer | begin training epoch 496\n",
      "2023-04-17 14:28:56 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:29:08 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:29:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:29:08 | INFO | valid | epoch 496 | valid on 'valid' subset | loss 5.954 | nll_loss 4.25 | ppl 19.03 | wps 7122.6 | wpb 290.8 | bsz 1 | num_updates 15863 | best_loss 3.979\n",
      "2023-04-17 14:29:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 496 @ 15863 updates\n",
      "2023-04-17 14:29:08 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:29:20 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 14:29:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 496 @ 15863 updates, score 5.954) (writing took 11.437967587960884 seconds)\n",
      "2023-04-17 14:29:20 | INFO | fairseq_cli.train | end of epoch 496 (average epoch stats below)\n",
      "2023-04-17 14:29:20 | INFO | train | epoch 496 | loss 2.407 | nll_loss 0.248 | ppl 1.19 | wps 908.5 | ups 1.34 | wpb 678.5 | bsz 2 | num_updates 15863 | lr 2.42026e-05 | gnorm 3.081 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 13040\n",
      "2023-04-17 14:29:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:29:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:29:20 | INFO | fairseq.trainer | begin training epoch 497\n",
      "2023-04-17 14:29:20 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:29:33 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:29:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:29:33 | INFO | valid | epoch 497 | valid on 'valid' subset | loss 5.946 | nll_loss 4.231 | ppl 18.78 | wps 6953.9 | wpb 290.8 | bsz 1 | num_updates 15895 | best_loss 3.979\n",
      "2023-04-17 14:29:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 497 @ 15895 updates\n",
      "2023-04-17 14:29:33 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:29:46 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:29:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 497 @ 15895 updates, score 5.946) (writing took 13.646878003957681 seconds)\n",
      "2023-04-17 14:29:46 | INFO | fairseq_cli.train | end of epoch 497 (average epoch stats below)\n",
      "2023-04-17 14:29:46 | INFO | train | epoch 497 | loss 2.401 | nll_loss 0.242 | ppl 1.18 | wps 813.6 | ups 1.2 | wpb 678.5 | bsz 2 | num_updates 15895 | lr 2.41906e-05 | gnorm 3.012 | clip 100 | loss_scale 0.5 | train_wall 13 | gb_free 13.9 | wall 13067\n",
      "2023-04-17 14:29:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:29:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:29:46 | INFO | fairseq.trainer | begin training epoch 498\n",
      "2023-04-17 14:29:46 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:29:49 | INFO | train_inner | epoch 498:      5 / 32 loss=2.405, nll_loss=0.246, ppl=1.19, wps=881.4, ups=1.29, wpb=681.5, bsz=2, num_updates=15900, lr=2.41887e-05, gnorm=3.051, clip=100, loss_scale=0.5, train_wall=39, gb_free=13.9, wall=13069\n",
      "2023-04-17 14:29:59 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:29:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:30:00 | INFO | valid | epoch 498 | valid on 'valid' subset | loss 5.903 | nll_loss 4.184 | ppl 18.17 | wps 7028.2 | wpb 290.8 | bsz 1 | num_updates 15927 | best_loss 3.979\n",
      "2023-04-17 14:30:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 498 @ 15927 updates\n",
      "2023-04-17 14:30:00 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:30:12 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:30:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 498 @ 15927 updates, score 5.903) (writing took 12.871034237090498 seconds)\n",
      "2023-04-17 14:30:12 | INFO | fairseq_cli.train | end of epoch 498 (average epoch stats below)\n",
      "2023-04-17 14:30:12 | INFO | train | epoch 498 | loss 2.403 | nll_loss 0.244 | ppl 1.18 | wps 835.3 | ups 1.23 | wpb 678.5 | bsz 2 | num_updates 15927 | lr 2.41785e-05 | gnorm 2.947 | clip 100 | loss_scale 0.5 | train_wall 13 | gb_free 13.9 | wall 13093\n",
      "2023-04-17 14:30:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:30:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:30:12 | INFO | fairseq.trainer | begin training epoch 499\n",
      "2023-04-17 14:30:12 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:30:24 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:30:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:30:24 | INFO | valid | epoch 499 | valid on 'valid' subset | loss 5.985 | nll_loss 4.277 | ppl 19.38 | wps 7060.5 | wpb 290.8 | bsz 1 | num_updates 15959 | best_loss 3.979\n",
      "2023-04-17 14:30:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 499 @ 15959 updates\n",
      "2023-04-17 14:30:24 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:30:37 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:30:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 499 @ 15959 updates, score 5.985) (writing took 12.856102306977846 seconds)\n",
      "2023-04-17 14:30:37 | INFO | fairseq_cli.train | end of epoch 499 (average epoch stats below)\n",
      "2023-04-17 14:30:37 | INFO | train | epoch 499 | loss 2.399 | nll_loss 0.239 | ppl 1.18 | wps 874.4 | ups 1.29 | wpb 678.5 | bsz 2 | num_updates 15959 | lr 2.41664e-05 | gnorm 2.785 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 13118\n",
      "2023-04-17 14:30:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:30:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:30:37 | INFO | fairseq.trainer | begin training epoch 500\n",
      "2023-04-17 14:30:37 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:30:49 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:30:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:30:49 | INFO | valid | epoch 500 | valid on 'valid' subset | loss 5.914 | nll_loss 4.211 | ppl 18.51 | wps 7114.4 | wpb 290.8 | bsz 1 | num_updates 15991 | best_loss 3.979\n",
      "2023-04-17 14:30:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 500 @ 15991 updates\n",
      "2023-04-17 14:30:49 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:30:59 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:30:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 500 @ 15991 updates, score 5.914) (writing took 9.624470922979526 seconds)\n",
      "2023-04-17 14:30:59 | INFO | fairseq_cli.train | end of epoch 500 (average epoch stats below)\n",
      "2023-04-17 14:30:59 | INFO | train | epoch 500 | loss 2.403 | nll_loss 0.244 | ppl 1.18 | wps 1001.2 | ups 1.48 | wpb 678.5 | bsz 2 | num_updates 15991 | lr 2.41543e-05 | gnorm 3.089 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 13139\n",
      "2023-04-17 14:30:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:30:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:30:59 | INFO | fairseq.trainer | begin training epoch 501\n",
      "2023-04-17 14:30:59 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:31:03 | INFO | train_inner | epoch 501:      9 / 32 loss=2.401, nll_loss=0.242, ppl=1.18, wps=905.1, ups=1.35, wpb=668.8, bsz=2, num_updates=16000, lr=2.41509e-05, gnorm=2.989, clip=100, loss_scale=0.5, train_wall=37, gb_free=13.9, wall=13143\n",
      "2023-04-17 14:31:11 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:31:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:31:11 | INFO | valid | epoch 501 | valid on 'valid' subset | loss 5.901 | nll_loss 4.198 | ppl 18.36 | wps 7224 | wpb 290.8 | bsz 1 | num_updates 16023 | best_loss 3.979\n",
      "2023-04-17 14:31:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 501 @ 16023 updates\n",
      "2023-04-17 14:31:11 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 14:31:23 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:31:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 501 @ 16023 updates, score 5.901) (writing took 12.453548102057539 seconds)\n",
      "2023-04-17 14:31:24 | INFO | fairseq_cli.train | end of epoch 501 (average epoch stats below)\n",
      "2023-04-17 14:31:24 | INFO | train | epoch 501 | loss 2.401 | nll_loss 0.239 | ppl 1.18 | wps 879.2 | ups 1.3 | wpb 678.5 | bsz 2 | num_updates 16023 | lr 2.41423e-05 | gnorm 2.998 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 13164\n",
      "2023-04-17 14:31:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:31:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:31:24 | INFO | fairseq.trainer | begin training epoch 502\n",
      "2023-04-17 14:31:24 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:31:35 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:31:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:31:36 | INFO | valid | epoch 502 | valid on 'valid' subset | loss 5.864 | nll_loss 4.153 | ppl 17.79 | wps 5352.3 | wpb 290.8 | bsz 1 | num_updates 16055 | best_loss 3.979\n",
      "2023-04-17 14:31:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 502 @ 16055 updates\n",
      "2023-04-17 14:31:36 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:31:48 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:31:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 502 @ 16055 updates, score 5.864) (writing took 12.391346001066267 seconds)\n",
      "2023-04-17 14:31:48 | INFO | fairseq_cli.train | end of epoch 502 (average epoch stats below)\n",
      "2023-04-17 14:31:48 | INFO | train | epoch 502 | loss 2.402 | nll_loss 0.245 | ppl 1.18 | wps 885.1 | ups 1.3 | wpb 678.5 | bsz 2 | num_updates 16055 | lr 2.41302e-05 | gnorm 2.852 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 13188\n",
      "2023-04-17 14:31:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:31:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:31:48 | INFO | fairseq.trainer | begin training epoch 503\n",
      "2023-04-17 14:31:48 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:32:00 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:32:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:32:00 | INFO | valid | epoch 503 | valid on 'valid' subset | loss 5.894 | nll_loss 4.18 | ppl 18.13 | wps 7301.5 | wpb 290.8 | bsz 1 | num_updates 16087 | best_loss 3.979\n",
      "2023-04-17 14:32:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 503 @ 16087 updates\n",
      "2023-04-17 14:32:00 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:32:12 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:32:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 503 @ 16087 updates, score 5.894) (writing took 12.246330881956965 seconds)\n",
      "2023-04-17 14:32:12 | INFO | fairseq_cli.train | end of epoch 503 (average epoch stats below)\n",
      "2023-04-17 14:32:12 | INFO | train | epoch 503 | loss 2.403 | nll_loss 0.246 | ppl 1.19 | wps 894.8 | ups 1.32 | wpb 678.5 | bsz 2 | num_updates 16087 | lr 2.41181e-05 | gnorm 2.922 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 13213\n",
      "2023-04-17 14:32:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:32:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:32:12 | INFO | fairseq.trainer | begin training epoch 504\n",
      "2023-04-17 14:32:12 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:32:17 | INFO | train_inner | epoch 504:     13 / 32 loss=2.402, nll_loss=0.244, ppl=1.18, wps=915, ups=1.34, wpb=681.9, bsz=2, num_updates=16100, lr=2.41132e-05, gnorm=2.875, clip=100, loss_scale=0.5, train_wall=36, gb_free=13.9, wall=13217\n",
      "2023-04-17 14:32:24 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:32:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:32:24 | INFO | valid | epoch 504 | valid on 'valid' subset | loss 5.896 | nll_loss 4.181 | ppl 18.14 | wps 7143.8 | wpb 290.8 | bsz 1 | num_updates 16119 | best_loss 3.979\n",
      "2023-04-17 14:32:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 504 @ 16119 updates\n",
      "2023-04-17 14:32:24 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:32:37 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:32:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 504 @ 16119 updates, score 5.896) (writing took 12.62514789099805 seconds)\n",
      "2023-04-17 14:32:37 | INFO | fairseq_cli.train | end of epoch 504 (average epoch stats below)\n",
      "2023-04-17 14:32:37 | INFO | train | epoch 504 | loss 2.404 | nll_loss 0.246 | ppl 1.19 | wps 889.2 | ups 1.31 | wpb 678.5 | bsz 2 | num_updates 16119 | lr 2.4106e-05 | gnorm 2.921 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 13237\n",
      "2023-04-17 14:32:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:32:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:32:37 | INFO | fairseq.trainer | begin training epoch 505\n",
      "2023-04-17 14:32:37 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:32:49 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:32:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:32:50 | INFO | valid | epoch 505 | valid on 'valid' subset | loss 5.917 | nll_loss 4.205 | ppl 18.45 | wps 6939.3 | wpb 290.8 | bsz 1 | num_updates 16151 | best_loss 3.979\n",
      "2023-04-17 14:32:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 505 @ 16151 updates\n",
      "2023-04-17 14:32:50 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:33:02 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:33:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 505 @ 16151 updates, score 5.917) (writing took 13.055917016929016 seconds)\n",
      "2023-04-17 14:33:03 | INFO | fairseq_cli.train | end of epoch 505 (average epoch stats below)\n",
      "2023-04-17 14:33:03 | INFO | train | epoch 505 | loss 2.4 | nll_loss 0.241 | ppl 1.18 | wps 843.5 | ups 1.24 | wpb 678.5 | bsz 2 | num_updates 16151 | lr 2.4094e-05 | gnorm 3.026 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 13263\n",
      "2023-04-17 14:33:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:33:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:33:03 | INFO | fairseq.trainer | begin training epoch 506\n",
      "2023-04-17 14:33:03 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:33:14 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:33:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:33:14 | INFO | valid | epoch 506 | valid on 'valid' subset | loss 5.918 | nll_loss 4.213 | ppl 18.54 | wps 7118.5 | wpb 290.8 | bsz 1 | num_updates 16183 | best_loss 3.979\n",
      "2023-04-17 14:33:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 506 @ 16183 updates\n",
      "2023-04-17 14:33:14 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:33:27 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 14:33:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 506 @ 16183 updates, score 5.918) (writing took 12.841160787036642 seconds)\n",
      "2023-04-17 14:33:27 | INFO | fairseq_cli.train | end of epoch 506 (average epoch stats below)\n",
      "2023-04-17 14:33:27 | INFO | train | epoch 506 | loss 2.4 | nll_loss 0.242 | ppl 1.18 | wps 882.9 | ups 1.3 | wpb 678.5 | bsz 2 | num_updates 16183 | lr 2.40819e-05 | gnorm 2.901 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 13287\n",
      "2023-04-17 14:33:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:33:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:33:27 | INFO | fairseq.trainer | begin training epoch 507\n",
      "2023-04-17 14:33:27 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:33:33 | INFO | train_inner | epoch 507:     17 / 32 loss=2.4, nll_loss=0.242, ppl=1.18, wps=894.9, ups=1.31, wpb=683.3, bsz=2, num_updates=16200, lr=2.40755e-05, gnorm=2.936, clip=100, loss_scale=0.5, train_wall=37, gb_free=13.9, wall=13294\n",
      "2023-04-17 14:33:39 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:33:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:33:40 | INFO | valid | epoch 507 | valid on 'valid' subset | loss 5.911 | nll_loss 4.202 | ppl 18.4 | wps 6079.5 | wpb 290.8 | bsz 1 | num_updates 16215 | best_loss 3.979\n",
      "2023-04-17 14:33:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 507 @ 16215 updates\n",
      "2023-04-17 14:33:40 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:33:52 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:33:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 507 @ 16215 updates, score 5.911) (writing took 12.328944619046524 seconds)\n",
      "2023-04-17 14:33:52 | INFO | fairseq_cli.train | end of epoch 507 (average epoch stats below)\n",
      "2023-04-17 14:33:52 | INFO | train | epoch 507 | loss 2.398 | nll_loss 0.239 | ppl 1.18 | wps 875.3 | ups 1.29 | wpb 678.5 | bsz 2 | num_updates 16215 | lr 2.40698e-05 | gnorm 2.914 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 13312\n",
      "2023-04-17 14:33:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:33:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:33:52 | INFO | fairseq.trainer | begin training epoch 508\n",
      "2023-04-17 14:33:52 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:34:04 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:34:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:34:04 | INFO | valid | epoch 508 | valid on 'valid' subset | loss 5.954 | nll_loss 4.249 | ppl 19.01 | wps 6730.1 | wpb 290.8 | bsz 1 | num_updates 16247 | best_loss 3.979\n",
      "2023-04-17 14:34:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 508 @ 16247 updates\n",
      "2023-04-17 14:34:04 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:34:17 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:34:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 508 @ 16247 updates, score 5.954) (writing took 12.685384137905203 seconds)\n",
      "2023-04-17 14:34:17 | INFO | fairseq_cli.train | end of epoch 508 (average epoch stats below)\n",
      "2023-04-17 14:34:17 | INFO | train | epoch 508 | loss 2.4 | nll_loss 0.243 | ppl 1.18 | wps 877.5 | ups 1.29 | wpb 678.5 | bsz 2 | num_updates 16247 | lr 2.40577e-05 | gnorm 3.08 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 13337\n",
      "2023-04-17 14:34:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:34:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:34:17 | INFO | fairseq.trainer | begin training epoch 509\n",
      "2023-04-17 14:34:17 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:34:29 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:34:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:34:29 | INFO | valid | epoch 509 | valid on 'valid' subset | loss 5.882 | nll_loss 4.17 | ppl 18 | wps 3882.4 | wpb 290.8 | bsz 1 | num_updates 16279 | best_loss 3.979\n",
      "2023-04-17 14:34:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 509 @ 16279 updates\n",
      "2023-04-17 14:34:29 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:34:41 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:34:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 509 @ 16279 updates, score 5.882) (writing took 11.978279520990327 seconds)\n",
      "2023-04-17 14:34:41 | INFO | fairseq_cli.train | end of epoch 509 (average epoch stats below)\n",
      "2023-04-17 14:34:41 | INFO | train | epoch 509 | loss 2.397 | nll_loss 0.237 | ppl 1.18 | wps 895.8 | ups 1.32 | wpb 678.5 | bsz 2 | num_updates 16279 | lr 2.40457e-05 | gnorm 2.866 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 13361\n",
      "2023-04-17 14:34:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:34:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:34:41 | INFO | fairseq.trainer | begin training epoch 510\n",
      "2023-04-17 14:34:41 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:34:49 | INFO | train_inner | epoch 510:     21 / 32 loss=2.4, nll_loss=0.242, ppl=1.18, wps=901.2, ups=1.33, wpb=679.4, bsz=2, num_updates=16300, lr=2.40377e-05, gnorm=3.005, clip=100, loss_scale=0.5, train_wall=37, gb_free=13.9, wall=13369\n",
      "2023-04-17 14:34:53 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:34:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:34:53 | INFO | valid | epoch 510 | valid on 'valid' subset | loss 5.857 | nll_loss 4.138 | ppl 17.6 | wps 7064.5 | wpb 290.8 | bsz 1 | num_updates 16311 | best_loss 3.979\n",
      "2023-04-17 14:34:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 510 @ 16311 updates\n",
      "2023-04-17 14:34:53 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:35:05 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:35:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 510 @ 16311 updates, score 5.857) (writing took 12.452498930040747 seconds)\n",
      "2023-04-17 14:35:05 | INFO | fairseq_cli.train | end of epoch 510 (average epoch stats below)\n",
      "2023-04-17 14:35:05 | INFO | train | epoch 510 | loss 2.402 | nll_loss 0.245 | ppl 1.18 | wps 891.9 | ups 1.31 | wpb 678.5 | bsz 2 | num_updates 16311 | lr 2.40336e-05 | gnorm 3.15 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 13386\n",
      "2023-04-17 14:35:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:35:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:35:05 | INFO | fairseq.trainer | begin training epoch 511\n",
      "2023-04-17 14:35:05 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:35:17 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:35:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:35:17 | INFO | valid | epoch 511 | valid on 'valid' subset | loss 5.938 | nll_loss 4.233 | ppl 18.8 | wps 7269 | wpb 290.8 | bsz 1 | num_updates 16343 | best_loss 3.979\n",
      "2023-04-17 14:35:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 511 @ 16343 updates\n",
      "2023-04-17 14:35:17 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 14:35:29 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:35:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 511 @ 16343 updates, score 5.938) (writing took 12.414680543006398 seconds)\n",
      "2023-04-17 14:35:30 | INFO | fairseq_cli.train | end of epoch 511 (average epoch stats below)\n",
      "2023-04-17 14:35:30 | INFO | train | epoch 511 | loss 2.402 | nll_loss 0.245 | ppl 1.18 | wps 895.9 | ups 1.32 | wpb 678.5 | bsz 2 | num_updates 16343 | lr 2.40215e-05 | gnorm 3.225 | clip 100 | loss_scale 0.5 | train_wall 11 | gb_free 13.9 | wall 13410\n",
      "2023-04-17 14:35:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:35:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:35:30 | INFO | fairseq.trainer | begin training epoch 512\n",
      "2023-04-17 14:35:30 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:35:42 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:35:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:35:42 | INFO | valid | epoch 512 | valid on 'valid' subset | loss 5.938 | nll_loss 4.225 | ppl 18.7 | wps 6524.5 | wpb 290.8 | bsz 1 | num_updates 16375 | best_loss 3.979\n",
      "2023-04-17 14:35:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 512 @ 16375 updates\n",
      "2023-04-17 14:35:42 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:35:54 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:35:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 512 @ 16375 updates, score 5.938) (writing took 12.100751374964602 seconds)\n",
      "2023-04-17 14:35:54 | INFO | fairseq_cli.train | end of epoch 512 (average epoch stats below)\n",
      "2023-04-17 14:35:54 | INFO | train | epoch 512 | loss 2.395 | nll_loss 0.237 | ppl 1.18 | wps 889 | ups 1.31 | wpb 678.5 | bsz 2 | num_updates 16375 | lr 2.40094e-05 | gnorm 2.879 | clip 100 | loss_scale 0.5 | train_wall 12 | gb_free 13.9 | wall 13434\n",
      "2023-04-17 14:35:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:35:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:35:54 | INFO | fairseq.trainer | begin training epoch 513\n",
      "2023-04-17 14:35:54 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:36:03 | INFO | train_inner | epoch 513:     25 / 32 loss=2.398, nll_loss=0.241, ppl=1.18, wps=905.9, ups=1.35, wpb=670.9, bsz=2, num_updates=16400, lr=2.4e-05, gnorm=3.061, clip=100, loss_scale=0.5, train_wall=36, gb_free=13.9, wall=13443\n",
      "2023-04-17 14:36:05 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:36:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:36:05 | INFO | valid | epoch 513 | valid on 'valid' subset | loss 5.935 | nll_loss 4.217 | ppl 18.6 | wps 7039.8 | wpb 290.8 | bsz 1 | num_updates 16407 | best_loss 3.979\n",
      "2023-04-17 14:36:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 513 @ 16407 updates\n",
      "2023-04-17 14:36:05 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:36:17 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:36:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 513 @ 16407 updates, score 5.935) (writing took 12.207900427049026 seconds)\n",
      "2023-04-17 14:36:18 | INFO | fairseq_cli.train | end of epoch 513 (average epoch stats below)\n",
      "2023-04-17 14:36:18 | INFO | train | epoch 513 | loss 2.4 | nll_loss 0.241 | ppl 1.18 | wps 919.2 | ups 1.35 | wpb 678.5 | bsz 2 | num_updates 16407 | lr 2.39974e-05 | gnorm 3.079 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 13458\n",
      "2023-04-17 14:36:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:36:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:36:18 | INFO | fairseq.trainer | begin training epoch 514\n",
      "2023-04-17 14:36:18 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:36:29 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:36:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:36:29 | INFO | valid | epoch 514 | valid on 'valid' subset | loss 5.903 | nll_loss 4.194 | ppl 18.31 | wps 7004.8 | wpb 290.8 | bsz 1 | num_updates 16439 | best_loss 3.979\n",
      "2023-04-17 14:36:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 514 @ 16439 updates\n",
      "2023-04-17 14:36:29 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:36:42 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:36:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 514 @ 16439 updates, score 5.903) (writing took 12.944157675956376 seconds)\n",
      "2023-04-17 14:36:42 | INFO | fairseq_cli.train | end of epoch 514 (average epoch stats below)\n",
      "2023-04-17 14:36:42 | INFO | train | epoch 514 | loss 2.399 | nll_loss 0.242 | ppl 1.18 | wps 878.9 | ups 1.3 | wpb 678.5 | bsz 2 | num_updates 16439 | lr 2.39853e-05 | gnorm 3.073 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 13483\n",
      "2023-04-17 14:36:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:36:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:36:42 | INFO | fairseq.trainer | begin training epoch 515\n",
      "2023-04-17 14:36:42 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:36:54 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:36:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:36:55 | INFO | valid | epoch 515 | valid on 'valid' subset | loss 5.918 | nll_loss 4.189 | ppl 18.24 | wps 7237.5 | wpb 290.8 | bsz 1 | num_updates 16471 | best_loss 3.979\n",
      "2023-04-17 14:36:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 515 @ 16471 updates\n",
      "2023-04-17 14:36:55 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:37:06 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:37:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 515 @ 16471 updates, score 5.918) (writing took 12.025468308012933 seconds)\n",
      "2023-04-17 14:37:07 | INFO | fairseq_cli.train | end of epoch 515 (average epoch stats below)\n",
      "2023-04-17 14:37:07 | INFO | train | epoch 515 | loss 2.397 | nll_loss 0.239 | ppl 1.18 | wps 896.6 | ups 1.32 | wpb 678.5 | bsz 2 | num_updates 16471 | lr 2.39732e-05 | gnorm 2.829 | clip 100 | loss_scale 1 | train_wall 12 | gb_free 13.9 | wall 13507\n",
      "2023-04-17 14:37:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:37:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:37:07 | INFO | fairseq.trainer | begin training epoch 516\n",
      "2023-04-17 14:37:07 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:37:17 | INFO | train_inner | epoch 516:     29 / 32 loss=2.397, nll_loss=0.239, ppl=1.18, wps=931.8, ups=1.35, wpb=689.1, bsz=2, num_updates=16500, lr=2.39623e-05, gnorm=2.92, clip=100, loss_scale=1, train_wall=36, gb_free=13.9, wall=13517\n",
      "2023-04-17 14:37:18 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:37:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:37:18 | INFO | valid | epoch 516 | valid on 'valid' subset | loss 5.97 | nll_loss 4.276 | ppl 19.37 | wps 7143.4 | wpb 290.8 | bsz 1 | num_updates 16503 | best_loss 3.979\n",
      "2023-04-17 14:37:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 516 @ 16503 updates\n",
      "2023-04-17 14:37:18 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 14:37:30 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:37:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 516 @ 16503 updates, score 5.97) (writing took 12.23274564393796 seconds)\n",
      "2023-04-17 14:37:30 | INFO | fairseq_cli.train | end of epoch 516 (average epoch stats below)\n",
      "2023-04-17 14:37:30 | INFO | train | epoch 516 | loss 2.393 | nll_loss 0.233 | ppl 1.18 | wps 917.2 | ups 1.35 | wpb 678.5 | bsz 2 | num_updates 16503 | lr 2.39611e-05 | gnorm 2.83 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 13530\n",
      "2023-04-17 14:37:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:37:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:37:30 | INFO | fairseq.trainer | begin training epoch 517\n",
      "2023-04-17 14:37:30 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:37:42 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:37:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:37:42 | INFO | valid | epoch 517 | valid on 'valid' subset | loss 5.983 | nll_loss 4.271 | ppl 19.3 | wps 7176 | wpb 290.8 | bsz 1 | num_updates 16535 | best_loss 3.979\n",
      "2023-04-17 14:37:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 517 @ 16535 updates\n",
      "2023-04-17 14:37:42 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:37:54 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:37:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 517 @ 16535 updates, score 5.983) (writing took 12.047693107975647 seconds)\n",
      "2023-04-17 14:37:54 | INFO | fairseq_cli.train | end of epoch 517 (average epoch stats below)\n",
      "2023-04-17 14:37:54 | INFO | train | epoch 517 | loss 2.395 | nll_loss 0.238 | ppl 1.18 | wps 919.7 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 16535 | lr 2.39491e-05 | gnorm 2.882 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 13554\n",
      "2023-04-17 14:37:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:37:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:37:54 | INFO | fairseq.trainer | begin training epoch 518\n",
      "2023-04-17 14:37:54 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:38:05 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:38:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:38:05 | INFO | valid | epoch 518 | valid on 'valid' subset | loss 5.976 | nll_loss 4.271 | ppl 19.3 | wps 7185.3 | wpb 290.8 | bsz 1 | num_updates 16567 | best_loss 3.979\n",
      "2023-04-17 14:38:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 518 @ 16567 updates\n",
      "2023-04-17 14:38:05 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:38:17 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:38:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 518 @ 16567 updates, score 5.976) (writing took 12.184984102961607 seconds)\n",
      "2023-04-17 14:38:17 | INFO | fairseq_cli.train | end of epoch 518 (average epoch stats below)\n",
      "2023-04-17 14:38:17 | INFO | train | epoch 518 | loss 2.395 | nll_loss 0.237 | ppl 1.18 | wps 920.7 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 16567 | lr 2.3937e-05 | gnorm 2.86 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 13578\n",
      "2023-04-17 14:38:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:38:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:38:17 | INFO | fairseq.trainer | begin training epoch 519\n",
      "2023-04-17 14:38:17 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:38:29 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:38:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:38:29 | INFO | valid | epoch 519 | valid on 'valid' subset | loss 5.917 | nll_loss 4.204 | ppl 18.43 | wps 7156.2 | wpb 290.8 | bsz 1 | num_updates 16599 | best_loss 3.979\n",
      "2023-04-17 14:38:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 519 @ 16599 updates\n",
      "2023-04-17 14:38:29 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:38:41 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:38:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 519 @ 16599 updates, score 5.917) (writing took 11.936326444963925 seconds)\n",
      "2023-04-17 14:38:41 | INFO | fairseq_cli.train | end of epoch 519 (average epoch stats below)\n",
      "2023-04-17 14:38:41 | INFO | train | epoch 519 | loss 2.395 | nll_loss 0.237 | ppl 1.18 | wps 925.8 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 16599 | lr 2.39249e-05 | gnorm 2.818 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 13601\n",
      "2023-04-17 14:38:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:38:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:38:41 | INFO | fairseq.trainer | begin training epoch 520\n",
      "2023-04-17 14:38:41 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:38:41 | INFO | train_inner | epoch 520:      1 / 32 loss=2.395, nll_loss=0.237, ppl=1.18, wps=794.7, ups=1.18, wpb=670.9, bsz=2, num_updates=16600, lr=2.39245e-05, gnorm=2.849, clip=100, loss_scale=1, train_wall=35, gb_free=13.9, wall=13602\n",
      "2023-04-17 14:38:52 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:38:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:38:52 | INFO | valid | epoch 520 | valid on 'valid' subset | loss 5.944 | nll_loss 4.247 | ppl 18.99 | wps 7203.6 | wpb 290.8 | bsz 1 | num_updates 16631 | best_loss 3.979\n",
      "2023-04-17 14:38:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 520 @ 16631 updates\n",
      "2023-04-17 14:38:52 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:39:04 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:39:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 520 @ 16631 updates, score 5.944) (writing took 12.166368365986273 seconds)\n",
      "2023-04-17 14:39:04 | INFO | fairseq_cli.train | end of epoch 520 (average epoch stats below)\n",
      "2023-04-17 14:39:04 | INFO | train | epoch 520 | loss 2.391 | nll_loss 0.232 | ppl 1.17 | wps 921.3 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 16631 | lr 2.39128e-05 | gnorm 2.863 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 13625\n",
      "2023-04-17 14:39:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:39:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:39:04 | INFO | fairseq.trainer | begin training epoch 521\n",
      "2023-04-17 14:39:04 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:39:16 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:39:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:39:16 | INFO | valid | epoch 521 | valid on 'valid' subset | loss 5.937 | nll_loss 4.224 | ppl 18.68 | wps 6999 | wpb 290.8 | bsz 1 | num_updates 16663 | best_loss 3.979\n",
      "2023-04-17 14:39:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 521 @ 16663 updates\n",
      "2023-04-17 14:39:16 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:39:28 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 14:39:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 521 @ 16663 updates, score 5.937) (writing took 11.951062979991548 seconds)\n",
      "2023-04-17 14:39:28 | INFO | fairseq_cli.train | end of epoch 521 (average epoch stats below)\n",
      "2023-04-17 14:39:28 | INFO | train | epoch 521 | loss 2.399 | nll_loss 0.241 | ppl 1.18 | wps 926.9 | ups 1.37 | wpb 678.5 | bsz 2 | num_updates 16663 | lr 2.39008e-05 | gnorm 2.848 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 13648\n",
      "2023-04-17 14:39:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:39:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:39:28 | INFO | fairseq.trainer | begin training epoch 522\n",
      "2023-04-17 14:39:28 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:39:39 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:39:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:39:39 | INFO | valid | epoch 522 | valid on 'valid' subset | loss 5.96 | nll_loss 4.247 | ppl 18.99 | wps 7121 | wpb 290.8 | bsz 1 | num_updates 16695 | best_loss 3.979\n",
      "2023-04-17 14:39:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 522 @ 16695 updates\n",
      "2023-04-17 14:39:39 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:39:51 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:39:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 522 @ 16695 updates, score 5.96) (writing took 12.150035914033651 seconds)\n",
      "2023-04-17 14:39:51 | INFO | fairseq_cli.train | end of epoch 522 (average epoch stats below)\n",
      "2023-04-17 14:39:51 | INFO | train | epoch 522 | loss 2.393 | nll_loss 0.237 | ppl 1.18 | wps 920.4 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 16695 | lr 2.38887e-05 | gnorm 2.813 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 13672\n",
      "2023-04-17 14:39:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:39:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:39:51 | INFO | fairseq.trainer | begin training epoch 523\n",
      "2023-04-17 14:39:51 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:39:53 | INFO | train_inner | epoch 523:      5 / 32 loss=2.394, nll_loss=0.237, ppl=1.18, wps=943.9, ups=1.39, wpb=679.6, bsz=2, num_updates=16700, lr=2.38868e-05, gnorm=2.829, clip=100, loss_scale=1, train_wall=35, gb_free=13.9, wall=13674\n",
      "2023-04-17 14:40:03 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:40:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:40:03 | INFO | valid | epoch 523 | valid on 'valid' subset | loss 5.952 | nll_loss 4.248 | ppl 19.01 | wps 7118.2 | wpb 290.8 | bsz 1 | num_updates 16727 | best_loss 3.979\n",
      "2023-04-17 14:40:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 523 @ 16727 updates\n",
      "2023-04-17 14:40:03 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:40:15 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:40:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 523 @ 16727 updates, score 5.952) (writing took 12.130633941036649 seconds)\n",
      "2023-04-17 14:40:15 | INFO | fairseq_cli.train | end of epoch 523 (average epoch stats below)\n",
      "2023-04-17 14:40:15 | INFO | train | epoch 523 | loss 2.393 | nll_loss 0.235 | ppl 1.18 | wps 923.6 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 16727 | lr 2.38766e-05 | gnorm 2.728 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 13695\n",
      "2023-04-17 14:40:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:40:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:40:15 | INFO | fairseq.trainer | begin training epoch 524\n",
      "2023-04-17 14:40:15 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:40:26 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:40:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:40:26 | INFO | valid | epoch 524 | valid on 'valid' subset | loss 5.957 | nll_loss 4.247 | ppl 18.98 | wps 7045.1 | wpb 290.8 | bsz 1 | num_updates 16759 | best_loss 3.979\n",
      "2023-04-17 14:40:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 524 @ 16759 updates\n",
      "2023-04-17 14:40:26 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:40:38 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:40:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 524 @ 16759 updates, score 5.957) (writing took 11.894132585031912 seconds)\n",
      "2023-04-17 14:40:38 | INFO | fairseq_cli.train | end of epoch 524 (average epoch stats below)\n",
      "2023-04-17 14:40:38 | INFO | train | epoch 524 | loss 2.395 | nll_loss 0.238 | ppl 1.18 | wps 949.5 | ups 1.4 | wpb 678.5 | bsz 2 | num_updates 16759 | lr 2.38645e-05 | gnorm 2.865 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 13718\n",
      "2023-04-17 14:40:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:40:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:40:38 | INFO | fairseq.trainer | begin training epoch 525\n",
      "2023-04-17 14:40:38 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:40:49 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:40:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:40:50 | INFO | valid | epoch 525 | valid on 'valid' subset | loss 5.961 | nll_loss 4.25 | ppl 19.03 | wps 7226.8 | wpb 290.8 | bsz 1 | num_updates 16791 | best_loss 3.979\n",
      "2023-04-17 14:40:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 525 @ 16791 updates\n",
      "2023-04-17 14:40:50 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:41:02 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:41:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 525 @ 16791 updates, score 5.961) (writing took 12.064869605004787 seconds)\n",
      "2023-04-17 14:41:02 | INFO | fairseq_cli.train | end of epoch 525 (average epoch stats below)\n",
      "2023-04-17 14:41:02 | INFO | train | epoch 525 | loss 2.395 | nll_loss 0.238 | ppl 1.18 | wps 907.4 | ups 1.34 | wpb 678.5 | bsz 2 | num_updates 16791 | lr 2.38525e-05 | gnorm 2.859 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 13742\n",
      "2023-04-17 14:41:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:41:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:41:02 | INFO | fairseq.trainer | begin training epoch 526\n",
      "2023-04-17 14:41:02 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:41:05 | INFO | train_inner | epoch 526:      9 / 32 loss=2.395, nll_loss=0.238, ppl=1.18, wps=950.7, ups=1.39, wpb=681.9, bsz=2, num_updates=16800, lr=2.38491e-05, gnorm=2.835, clip=100, loss_scale=1, train_wall=35, gb_free=13.9, wall=13745\n",
      "2023-04-17 14:41:13 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:41:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:41:13 | INFO | valid | epoch 526 | valid on 'valid' subset | loss 5.989 | nll_loss 4.278 | ppl 19.4 | wps 7139.3 | wpb 290.8 | bsz 1 | num_updates 16823 | best_loss 3.979\n",
      "2023-04-17 14:41:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 526 @ 16823 updates\n",
      "2023-04-17 14:41:13 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 14:41:25 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:41:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 526 @ 16823 updates, score 5.989) (writing took 12.191460034926422 seconds)\n",
      "2023-04-17 14:41:26 | INFO | fairseq_cli.train | end of epoch 526 (average epoch stats below)\n",
      "2023-04-17 14:41:26 | INFO | train | epoch 526 | loss 2.398 | nll_loss 0.241 | ppl 1.18 | wps 913.7 | ups 1.35 | wpb 678.5 | bsz 2 | num_updates 16823 | lr 2.38404e-05 | gnorm 2.985 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 13766\n",
      "2023-04-17 14:41:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:41:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:41:26 | INFO | fairseq.trainer | begin training epoch 527\n",
      "2023-04-17 14:41:26 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:41:37 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:41:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:41:37 | INFO | valid | epoch 527 | valid on 'valid' subset | loss 5.95 | nll_loss 4.246 | ppl 18.97 | wps 7202.7 | wpb 290.8 | bsz 1 | num_updates 16855 | best_loss 3.979\n",
      "2023-04-17 14:41:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 527 @ 16855 updates\n",
      "2023-04-17 14:41:37 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:41:49 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:41:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 527 @ 16855 updates, score 5.95) (writing took 12.361106529948302 seconds)\n",
      "2023-04-17 14:41:49 | INFO | fairseq_cli.train | end of epoch 527 (average epoch stats below)\n",
      "2023-04-17 14:41:49 | INFO | train | epoch 527 | loss 2.395 | nll_loss 0.237 | ppl 1.18 | wps 916 | ups 1.35 | wpb 678.5 | bsz 2 | num_updates 16855 | lr 2.38283e-05 | gnorm 2.773 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 13789\n",
      "2023-04-17 14:41:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:41:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:41:49 | INFO | fairseq.trainer | begin training epoch 528\n",
      "2023-04-17 14:41:49 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:42:00 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:42:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:42:01 | INFO | valid | epoch 528 | valid on 'valid' subset | loss 5.969 | nll_loss 4.26 | ppl 19.16 | wps 7130.9 | wpb 290.8 | bsz 1 | num_updates 16887 | best_loss 3.979\n",
      "2023-04-17 14:42:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 528 @ 16887 updates\n",
      "2023-04-17 14:42:01 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:42:13 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:42:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 528 @ 16887 updates, score 5.969) (writing took 12.096973865060136 seconds)\n",
      "2023-04-17 14:42:13 | INFO | fairseq_cli.train | end of epoch 528 (average epoch stats below)\n",
      "2023-04-17 14:42:13 | INFO | train | epoch 528 | loss 2.391 | nll_loss 0.235 | ppl 1.18 | wps 923.9 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 16887 | lr 2.38162e-05 | gnorm 2.758 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 13813\n",
      "2023-04-17 14:42:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:42:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:42:13 | INFO | fairseq.trainer | begin training epoch 529\n",
      "2023-04-17 14:42:13 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:42:17 | INFO | train_inner | epoch 529:     13 / 32 loss=2.394, nll_loss=0.237, ppl=1.18, wps=940.1, ups=1.38, wpb=680.4, bsz=2, num_updates=16900, lr=2.38113e-05, gnorm=2.796, clip=100, loss_scale=1, train_wall=35, gb_free=13.9, wall=13818\n",
      "2023-04-17 14:42:24 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:42:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:42:24 | INFO | valid | epoch 529 | valid on 'valid' subset | loss 5.976 | nll_loss 4.264 | ppl 19.21 | wps 7199.3 | wpb 290.8 | bsz 1 | num_updates 16919 | best_loss 3.979\n",
      "2023-04-17 14:42:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 529 @ 16919 updates\n",
      "2023-04-17 14:42:24 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:42:36 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:42:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 529 @ 16919 updates, score 5.976) (writing took 12.062942849006504 seconds)\n",
      "2023-04-17 14:42:36 | INFO | fairseq_cli.train | end of epoch 529 (average epoch stats below)\n",
      "2023-04-17 14:42:36 | INFO | train | epoch 529 | loss 2.389 | nll_loss 0.231 | ppl 1.17 | wps 926.4 | ups 1.37 | wpb 678.5 | bsz 2 | num_updates 16919 | lr 2.38042e-05 | gnorm 2.595 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 13836\n",
      "2023-04-17 14:42:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:42:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:42:36 | INFO | fairseq.trainer | begin training epoch 530\n",
      "2023-04-17 14:42:36 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:42:47 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:42:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:42:48 | INFO | valid | epoch 530 | valid on 'valid' subset | loss 5.928 | nll_loss 4.212 | ppl 18.53 | wps 6982.6 | wpb 290.8 | bsz 1 | num_updates 16951 | best_loss 3.979\n",
      "2023-04-17 14:42:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 530 @ 16951 updates\n",
      "2023-04-17 14:42:48 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:42:59 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:43:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 530 @ 16951 updates, score 5.928) (writing took 12.079339768970385 seconds)\n",
      "2023-04-17 14:43:00 | INFO | fairseq_cli.train | end of epoch 530 (average epoch stats below)\n",
      "2023-04-17 14:43:00 | INFO | train | epoch 530 | loss 2.396 | nll_loss 0.24 | ppl 1.18 | wps 926.6 | ups 1.37 | wpb 678.5 | bsz 2 | num_updates 16951 | lr 2.37921e-05 | gnorm 2.912 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 13860\n",
      "2023-04-17 14:43:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:43:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:43:00 | INFO | fairseq.trainer | begin training epoch 531\n",
      "2023-04-17 14:43:00 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:43:11 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:43:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:43:11 | INFO | valid | epoch 531 | valid on 'valid' subset | loss 5.894 | nll_loss 4.176 | ppl 18.08 | wps 7267.1 | wpb 290.8 | bsz 1 | num_updates 16983 | best_loss 3.979\n",
      "2023-04-17 14:43:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 531 @ 16983 updates\n",
      "2023-04-17 14:43:11 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:43:23 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 14:43:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 531 @ 16983 updates, score 5.894) (writing took 11.899713389924727 seconds)\n",
      "2023-04-17 14:43:23 | INFO | fairseq_cli.train | end of epoch 531 (average epoch stats below)\n",
      "2023-04-17 14:43:23 | INFO | train | epoch 531 | loss 2.391 | nll_loss 0.233 | ppl 1.18 | wps 935.4 | ups 1.38 | wpb 678.5 | bsz 2 | num_updates 16983 | lr 2.378e-05 | gnorm 2.904 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 13883\n",
      "2023-04-17 14:43:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:43:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:43:23 | INFO | fairseq.trainer | begin training epoch 532\n",
      "2023-04-17 14:43:23 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:43:29 | INFO | train_inner | epoch 532:     17 / 32 loss=2.392, nll_loss=0.235, ppl=1.18, wps=941.8, ups=1.4, wpb=673.2, bsz=2, num_updates=17000, lr=2.37736e-05, gnorm=2.827, clip=100, loss_scale=1, train_wall=34, gb_free=13.9, wall=13889\n",
      "2023-04-17 14:43:34 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:43:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:43:34 | INFO | valid | epoch 532 | valid on 'valid' subset | loss 5.915 | nll_loss 4.215 | ppl 18.57 | wps 7244.9 | wpb 290.8 | bsz 1 | num_updates 17015 | best_loss 3.979\n",
      "2023-04-17 14:43:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 532 @ 17015 updates\n",
      "2023-04-17 14:43:34 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:43:46 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:43:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 532 @ 17015 updates, score 5.915) (writing took 12.326701267040335 seconds)\n",
      "2023-04-17 14:43:46 | INFO | fairseq_cli.train | end of epoch 532 (average epoch stats below)\n",
      "2023-04-17 14:43:46 | INFO | train | epoch 532 | loss 2.388 | nll_loss 0.23 | ppl 1.17 | wps 917.8 | ups 1.35 | wpb 678.5 | bsz 2 | num_updates 17015 | lr 2.37679e-05 | gnorm 2.753 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 13907\n",
      "2023-04-17 14:43:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:43:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:43:46 | INFO | fairseq.trainer | begin training epoch 533\n",
      "2023-04-17 14:43:46 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:43:58 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:43:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:43:58 | INFO | valid | epoch 533 | valid on 'valid' subset | loss 5.873 | nll_loss 4.156 | ppl 17.82 | wps 7265.1 | wpb 290.8 | bsz 1 | num_updates 17047 | best_loss 3.979\n",
      "2023-04-17 14:43:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 533 @ 17047 updates\n",
      "2023-04-17 14:43:58 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:44:10 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:44:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 533 @ 17047 updates, score 5.873) (writing took 12.14850963500794 seconds)\n",
      "2023-04-17 14:44:10 | INFO | fairseq_cli.train | end of epoch 533 (average epoch stats below)\n",
      "2023-04-17 14:44:10 | INFO | train | epoch 533 | loss 2.39 | nll_loss 0.233 | ppl 1.18 | wps 923.6 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 17047 | lr 2.37558e-05 | gnorm 2.817 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 13930\n",
      "2023-04-17 14:44:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:44:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:44:10 | INFO | fairseq.trainer | begin training epoch 534\n",
      "2023-04-17 14:44:10 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:44:21 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:44:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:44:21 | INFO | valid | epoch 534 | valid on 'valid' subset | loss 5.896 | nll_loss 4.176 | ppl 18.08 | wps 7275.9 | wpb 290.8 | bsz 1 | num_updates 17079 | best_loss 3.979\n",
      "2023-04-17 14:44:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 534 @ 17079 updates\n",
      "2023-04-17 14:44:21 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:44:33 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:44:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 534 @ 17079 updates, score 5.896) (writing took 12.053930942085572 seconds)\n",
      "2023-04-17 14:44:33 | INFO | fairseq_cli.train | end of epoch 534 (average epoch stats below)\n",
      "2023-04-17 14:44:33 | INFO | train | epoch 534 | loss 2.391 | nll_loss 0.234 | ppl 1.18 | wps 926.9 | ups 1.37 | wpb 678.5 | bsz 2 | num_updates 17079 | lr 2.37438e-05 | gnorm 2.72 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 13954\n",
      "2023-04-17 14:44:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:44:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:44:33 | INFO | fairseq.trainer | begin training epoch 535\n",
      "2023-04-17 14:44:33 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:44:41 | INFO | train_inner | epoch 535:     21 / 32 loss=2.389, nll_loss=0.232, ppl=1.17, wps=943.8, ups=1.39, wpb=679.6, bsz=2, num_updates=17100, lr=2.37358e-05, gnorm=2.796, clip=100, loss_scale=1, train_wall=34, gb_free=13.8, wall=13961\n",
      "2023-04-17 14:44:45 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:44:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:44:45 | INFO | valid | epoch 535 | valid on 'valid' subset | loss 5.938 | nll_loss 4.221 | ppl 18.64 | wps 7276.4 | wpb 290.8 | bsz 1 | num_updates 17111 | best_loss 3.979\n",
      "2023-04-17 14:44:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 535 @ 17111 updates\n",
      "2023-04-17 14:44:45 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:44:57 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:44:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 535 @ 17111 updates, score 5.938) (writing took 12.049579289974645 seconds)\n",
      "2023-04-17 14:44:57 | INFO | fairseq_cli.train | end of epoch 535 (average epoch stats below)\n",
      "2023-04-17 14:44:57 | INFO | train | epoch 535 | loss 2.391 | nll_loss 0.236 | ppl 1.18 | wps 927.5 | ups 1.37 | wpb 678.5 | bsz 2 | num_updates 17111 | lr 2.37317e-05 | gnorm 2.843 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 13977\n",
      "2023-04-17 14:44:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:44:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:44:57 | INFO | fairseq.trainer | begin training epoch 536\n",
      "2023-04-17 14:44:57 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:45:08 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:45:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:45:08 | INFO | valid | epoch 536 | valid on 'valid' subset | loss 5.93 | nll_loss 4.213 | ppl 18.55 | wps 7221.4 | wpb 290.8 | bsz 1 | num_updates 17143 | best_loss 3.979\n",
      "2023-04-17 14:45:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 536 @ 17143 updates\n",
      "2023-04-17 14:45:08 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 14:45:20 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:45:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 536 @ 17143 updates, score 5.93) (writing took 12.183405771967955 seconds)\n",
      "2023-04-17 14:45:21 | INFO | fairseq_cli.train | end of epoch 536 (average epoch stats below)\n",
      "2023-04-17 14:45:21 | INFO | train | epoch 536 | loss 2.388 | nll_loss 0.231 | ppl 1.17 | wps 916.3 | ups 1.35 | wpb 678.5 | bsz 2 | num_updates 17143 | lr 2.37196e-05 | gnorm 2.887 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 14001\n",
      "2023-04-17 14:45:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:45:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:45:21 | INFO | fairseq.trainer | begin training epoch 537\n",
      "2023-04-17 14:45:21 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:45:32 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:45:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:45:32 | INFO | valid | epoch 537 | valid on 'valid' subset | loss 5.987 | nll_loss 4.273 | ppl 19.34 | wps 6908.6 | wpb 290.8 | bsz 1 | num_updates 17175 | best_loss 3.979\n",
      "2023-04-17 14:45:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 537 @ 17175 updates\n",
      "2023-04-17 14:45:32 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:45:44 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:45:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 537 @ 17175 updates, score 5.987) (writing took 12.125573363970034 seconds)\n",
      "2023-04-17 14:45:44 | INFO | fairseq_cli.train | end of epoch 537 (average epoch stats below)\n",
      "2023-04-17 14:45:44 | INFO | train | epoch 537 | loss 2.386 | nll_loss 0.229 | ppl 1.17 | wps 922.2 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 17175 | lr 2.37075e-05 | gnorm 2.77 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 14024\n",
      "2023-04-17 14:45:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:45:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:45:44 | INFO | fairseq.trainer | begin training epoch 538\n",
      "2023-04-17 14:45:44 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:45:53 | INFO | train_inner | epoch 538:     25 / 32 loss=2.389, nll_loss=0.232, ppl=1.17, wps=941.8, ups=1.39, wpb=679, bsz=2, num_updates=17200, lr=2.36981e-05, gnorm=2.884, clip=100, loss_scale=1, train_wall=35, gb_free=13.9, wall=14033\n",
      "2023-04-17 14:45:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:45:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:45:55 | INFO | valid | epoch 538 | valid on 'valid' subset | loss 5.978 | nll_loss 4.272 | ppl 19.32 | wps 7171.6 | wpb 290.8 | bsz 1 | num_updates 17207 | best_loss 3.979\n",
      "2023-04-17 14:45:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 538 @ 17207 updates\n",
      "2023-04-17 14:45:55 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:46:07 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:46:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 538 @ 17207 updates, score 5.978) (writing took 12.061606723000295 seconds)\n",
      "2023-04-17 14:46:08 | INFO | fairseq_cli.train | end of epoch 538 (average epoch stats below)\n",
      "2023-04-17 14:46:08 | INFO | train | epoch 538 | loss 2.389 | nll_loss 0.234 | ppl 1.18 | wps 924 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 17207 | lr 2.36955e-05 | gnorm 2.933 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 14048\n",
      "2023-04-17 14:46:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:46:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:46:08 | INFO | fairseq.trainer | begin training epoch 539\n",
      "2023-04-17 14:46:08 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:46:19 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:46:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:46:19 | INFO | valid | epoch 539 | valid on 'valid' subset | loss 5.976 | nll_loss 4.283 | ppl 19.47 | wps 7231.8 | wpb 290.8 | bsz 1 | num_updates 17239 | best_loss 3.979\n",
      "2023-04-17 14:46:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 539 @ 17239 updates\n",
      "2023-04-17 14:46:19 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:46:31 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:46:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 539 @ 17239 updates, score 5.976) (writing took 12.054371695034206 seconds)\n",
      "2023-04-17 14:46:31 | INFO | fairseq_cli.train | end of epoch 539 (average epoch stats below)\n",
      "2023-04-17 14:46:31 | INFO | train | epoch 539 | loss 2.389 | nll_loss 0.23 | ppl 1.17 | wps 927.5 | ups 1.37 | wpb 678.5 | bsz 2 | num_updates 17239 | lr 2.36834e-05 | gnorm 2.929 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 14071\n",
      "2023-04-17 14:46:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:46:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:46:31 | INFO | fairseq.trainer | begin training epoch 540\n",
      "2023-04-17 14:46:31 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:46:42 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:46:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:46:42 | INFO | valid | epoch 540 | valid on 'valid' subset | loss 5.98 | nll_loss 4.292 | ppl 19.59 | wps 7128.3 | wpb 290.8 | bsz 1 | num_updates 17271 | best_loss 3.979\n",
      "2023-04-17 14:46:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 540 @ 17271 updates\n",
      "2023-04-17 14:46:42 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:46:54 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:46:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 540 @ 17271 updates, score 5.98) (writing took 11.99722030304838 seconds)\n",
      "2023-04-17 14:46:54 | INFO | fairseq_cli.train | end of epoch 540 (average epoch stats below)\n",
      "2023-04-17 14:46:54 | INFO | train | epoch 540 | loss 2.383 | nll_loss 0.225 | ppl 1.17 | wps 927.5 | ups 1.37 | wpb 678.5 | bsz 2 | num_updates 17271 | lr 2.36713e-05 | gnorm 2.474 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 14095\n",
      "2023-04-17 14:46:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:46:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:46:54 | INFO | fairseq.trainer | begin training epoch 541\n",
      "2023-04-17 14:46:54 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:47:05 | INFO | train_inner | epoch 541:     29 / 32 loss=2.385, nll_loss=0.227, ppl=1.17, wps=947.7, ups=1.4, wpb=679.3, bsz=2, num_updates=17300, lr=2.36604e-05, gnorm=2.654, clip=100, loss_scale=1, train_wall=34, gb_free=13.9, wall=14105\n",
      "2023-04-17 14:47:06 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:47:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:47:06 | INFO | valid | epoch 541 | valid on 'valid' subset | loss 6.001 | nll_loss 4.305 | ppl 19.77 | wps 7285 | wpb 290.8 | bsz 1 | num_updates 17303 | best_loss 3.979\n",
      "2023-04-17 14:47:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 541 @ 17303 updates\n",
      "2023-04-17 14:47:06 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 14:47:18 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:47:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 541 @ 17303 updates, score 6.001) (writing took 12.186918037943542 seconds)\n",
      "2023-04-17 14:47:18 | INFO | fairseq_cli.train | end of epoch 541 (average epoch stats below)\n",
      "2023-04-17 14:47:18 | INFO | train | epoch 541 | loss 2.384 | nll_loss 0.228 | ppl 1.17 | wps 920.9 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 17303 | lr 2.36592e-05 | gnorm 2.669 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 14118\n",
      "2023-04-17 14:47:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:47:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:47:18 | INFO | fairseq.trainer | begin training epoch 542\n",
      "2023-04-17 14:47:18 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:47:29 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:47:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:47:29 | INFO | valid | epoch 542 | valid on 'valid' subset | loss 5.958 | nll_loss 4.247 | ppl 18.99 | wps 7264.3 | wpb 290.8 | bsz 1 | num_updates 17335 | best_loss 3.979\n",
      "2023-04-17 14:47:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 542 @ 17335 updates\n",
      "2023-04-17 14:47:29 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:47:41 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:47:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 542 @ 17335 updates, score 5.958) (writing took 11.920168710057624 seconds)\n",
      "2023-04-17 14:47:41 | INFO | fairseq_cli.train | end of epoch 542 (average epoch stats below)\n",
      "2023-04-17 14:47:41 | INFO | train | epoch 542 | loss 2.388 | nll_loss 0.233 | ppl 1.18 | wps 933.4 | ups 1.38 | wpb 678.5 | bsz 2 | num_updates 17335 | lr 2.36472e-05 | gnorm 2.76 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 14141\n",
      "2023-04-17 14:47:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:47:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:47:41 | INFO | fairseq.trainer | begin training epoch 543\n",
      "2023-04-17 14:47:41 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:47:52 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:47:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:47:53 | INFO | valid | epoch 543 | valid on 'valid' subset | loss 5.981 | nll_loss 4.285 | ppl 19.5 | wps 7025.4 | wpb 290.8 | bsz 1 | num_updates 17367 | best_loss 3.979\n",
      "2023-04-17 14:47:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 543 @ 17367 updates\n",
      "2023-04-17 14:47:53 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:48:04 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:48:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 543 @ 17367 updates, score 5.981) (writing took 11.983426498016343 seconds)\n",
      "2023-04-17 14:48:05 | INFO | fairseq_cli.train | end of epoch 543 (average epoch stats below)\n",
      "2023-04-17 14:48:05 | INFO | train | epoch 543 | loss 2.39 | nll_loss 0.234 | ppl 1.18 | wps 928.1 | ups 1.37 | wpb 678.5 | bsz 2 | num_updates 17367 | lr 2.36351e-05 | gnorm 2.798 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 14165\n",
      "2023-04-17 14:48:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:48:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:48:05 | INFO | fairseq.trainer | begin training epoch 544\n",
      "2023-04-17 14:48:05 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:48:16 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:48:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:48:16 | INFO | valid | epoch 544 | valid on 'valid' subset | loss 5.964 | nll_loss 4.252 | ppl 19.05 | wps 7228.9 | wpb 290.8 | bsz 1 | num_updates 17399 | best_loss 3.979\n",
      "2023-04-17 14:48:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 544 @ 17399 updates\n",
      "2023-04-17 14:48:16 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:48:28 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:48:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 544 @ 17399 updates, score 5.964) (writing took 12.125428410014138 seconds)\n",
      "2023-04-17 14:48:28 | INFO | fairseq_cli.train | end of epoch 544 (average epoch stats below)\n",
      "2023-04-17 14:48:28 | INFO | train | epoch 544 | loss 2.385 | nll_loss 0.228 | ppl 1.17 | wps 924.7 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 17399 | lr 2.3623e-05 | gnorm 2.762 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 14188\n",
      "2023-04-17 14:48:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:48:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:48:28 | INFO | fairseq.trainer | begin training epoch 545\n",
      "2023-04-17 14:48:28 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:48:28 | INFO | train_inner | epoch 545:      1 / 32 loss=2.388, nll_loss=0.231, ppl=1.17, wps=807.4, ups=1.19, wpb=677.1, bsz=2, num_updates=17400, lr=2.36226e-05, gnorm=2.771, clip=100, loss_scale=1, train_wall=34, gb_free=13.9, wall=14189\n",
      "2023-04-17 14:48:39 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:48:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:48:40 | INFO | valid | epoch 545 | valid on 'valid' subset | loss 6.026 | nll_loss 4.316 | ppl 19.92 | wps 7251.1 | wpb 290.8 | bsz 1 | num_updates 17431 | best_loss 3.979\n",
      "2023-04-17 14:48:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 545 @ 17431 updates\n",
      "2023-04-17 14:48:40 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:48:51 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:48:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 545 @ 17431 updates, score 6.026) (writing took 11.991137762088329 seconds)\n",
      "2023-04-17 14:48:52 | INFO | fairseq_cli.train | end of epoch 545 (average epoch stats below)\n",
      "2023-04-17 14:48:52 | INFO | train | epoch 545 | loss 2.386 | nll_loss 0.229 | ppl 1.17 | wps 925.4 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 17431 | lr 2.36109e-05 | gnorm 2.858 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 14212\n",
      "2023-04-17 14:48:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:48:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:48:52 | INFO | fairseq.trainer | begin training epoch 546\n",
      "2023-04-17 14:48:52 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:49:02 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:49:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:49:02 | INFO | valid | epoch 546 | valid on 'valid' subset | loss 5.935 | nll_loss 4.217 | ppl 18.6 | wps 6999.1 | wpb 290.8 | bsz 1 | num_updates 17463 | best_loss 3.979\n",
      "2023-04-17 14:49:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 546 @ 17463 updates\n",
      "2023-04-17 14:49:02 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:49:14 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 14:49:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 546 @ 17463 updates, score 5.935) (writing took 11.993259301991202 seconds)\n",
      "2023-04-17 14:49:14 | INFO | fairseq_cli.train | end of epoch 546 (average epoch stats below)\n",
      "2023-04-17 14:49:14 | INFO | train | epoch 546 | loss 2.385 | nll_loss 0.228 | ppl 1.17 | wps 948.2 | ups 1.4 | wpb 678.5 | bsz 2 | num_updates 17463 | lr 2.35989e-05 | gnorm 2.628 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 14235\n",
      "2023-04-17 14:49:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:49:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:49:14 | INFO | fairseq.trainer | begin training epoch 547\n",
      "2023-04-17 14:49:14 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:49:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:49:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:49:26 | INFO | valid | epoch 547 | valid on 'valid' subset | loss 5.974 | nll_loss 4.267 | ppl 19.25 | wps 7126.4 | wpb 290.8 | bsz 1 | num_updates 17495 | best_loss 3.979\n",
      "2023-04-17 14:49:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 547 @ 17495 updates\n",
      "2023-04-17 14:49:26 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:49:38 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:49:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 547 @ 17495 updates, score 5.974) (writing took 12.060766249080189 seconds)\n",
      "2023-04-17 14:49:38 | INFO | fairseq_cli.train | end of epoch 547 (average epoch stats below)\n",
      "2023-04-17 14:49:38 | INFO | train | epoch 547 | loss 2.388 | nll_loss 0.234 | ppl 1.18 | wps 930.8 | ups 1.37 | wpb 678.5 | bsz 2 | num_updates 17495 | lr 2.35868e-05 | gnorm 2.818 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 14258\n",
      "2023-04-17 14:49:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:49:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:49:38 | INFO | fairseq.trainer | begin training epoch 548\n",
      "2023-04-17 14:49:38 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:49:40 | INFO | train_inner | epoch 548:      5 / 32 loss=2.386, nll_loss=0.23, ppl=1.17, wps=954.4, ups=1.41, wpb=678.5, bsz=2, num_updates=17500, lr=2.35849e-05, gnorm=2.754, clip=100, loss_scale=1, train_wall=34, gb_free=13.9, wall=14260\n",
      "2023-04-17 14:49:49 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:49:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:49:49 | INFO | valid | epoch 548 | valid on 'valid' subset | loss 5.934 | nll_loss 4.237 | ppl 18.86 | wps 7306.4 | wpb 290.8 | bsz 1 | num_updates 17527 | best_loss 3.979\n",
      "2023-04-17 14:49:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 548 @ 17527 updates\n",
      "2023-04-17 14:49:49 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:50:01 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:50:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 548 @ 17527 updates, score 5.934) (writing took 12.185089703998528 seconds)\n",
      "2023-04-17 14:50:01 | INFO | fairseq_cli.train | end of epoch 548 (average epoch stats below)\n",
      "2023-04-17 14:50:01 | INFO | train | epoch 548 | loss 2.387 | nll_loss 0.23 | ppl 1.17 | wps 923.6 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 17527 | lr 2.35747e-05 | gnorm 2.602 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 14282\n",
      "2023-04-17 14:50:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:50:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:50:01 | INFO | fairseq.trainer | begin training epoch 549\n",
      "2023-04-17 14:50:01 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:50:12 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:50:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:50:13 | INFO | valid | epoch 549 | valid on 'valid' subset | loss 6.035 | nll_loss 4.345 | ppl 20.33 | wps 7253.5 | wpb 290.8 | bsz 1 | num_updates 17559 | best_loss 3.979\n",
      "2023-04-17 14:50:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 549 @ 17559 updates\n",
      "2023-04-17 14:50:13 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:50:24 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:50:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 549 @ 17559 updates, score 6.035) (writing took 12.110877600032836 seconds)\n",
      "2023-04-17 14:50:25 | INFO | fairseq_cli.train | end of epoch 549 (average epoch stats below)\n",
      "2023-04-17 14:50:25 | INFO | train | epoch 549 | loss 2.385 | nll_loss 0.228 | ppl 1.17 | wps 927.6 | ups 1.37 | wpb 678.5 | bsz 2 | num_updates 17559 | lr 2.35626e-05 | gnorm 2.679 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 14305\n",
      "2023-04-17 14:50:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:50:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:50:25 | INFO | fairseq.trainer | begin training epoch 550\n",
      "2023-04-17 14:50:25 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:50:36 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:50:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:50:36 | INFO | valid | epoch 550 | valid on 'valid' subset | loss 5.973 | nll_loss 4.272 | ppl 19.32 | wps 7189.2 | wpb 290.8 | bsz 1 | num_updates 17591 | best_loss 3.979\n",
      "2023-04-17 14:50:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 550 @ 17591 updates\n",
      "2023-04-17 14:50:36 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:50:48 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:50:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 550 @ 17591 updates, score 5.973) (writing took 11.992660547955893 seconds)\n",
      "2023-04-17 14:50:48 | INFO | fairseq_cli.train | end of epoch 550 (average epoch stats below)\n",
      "2023-04-17 14:50:48 | INFO | train | epoch 550 | loss 2.384 | nll_loss 0.228 | ppl 1.17 | wps 929.9 | ups 1.37 | wpb 678.5 | bsz 2 | num_updates 17591 | lr 2.35506e-05 | gnorm 2.827 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 14328\n",
      "2023-04-17 14:50:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:50:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:50:48 | INFO | fairseq.trainer | begin training epoch 551\n",
      "2023-04-17 14:50:48 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:50:51 | INFO | train_inner | epoch 551:      9 / 32 loss=2.385, nll_loss=0.229, ppl=1.17, wps=943.1, ups=1.4, wpb=675.6, bsz=2, num_updates=17600, lr=2.35472e-05, gnorm=2.721, clip=100, loss_scale=1, train_wall=34, gb_free=13.9, wall=14331\n",
      "2023-04-17 14:50:59 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:50:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:50:59 | INFO | valid | epoch 551 | valid on 'valid' subset | loss 5.991 | nll_loss 4.291 | ppl 19.57 | wps 7175.2 | wpb 290.8 | bsz 1 | num_updates 17623 | best_loss 3.979\n",
      "2023-04-17 14:50:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 551 @ 17623 updates\n",
      "2023-04-17 14:50:59 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 14:51:11 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:51:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 551 @ 17623 updates, score 5.991) (writing took 12.048129403032362 seconds)\n",
      "2023-04-17 14:51:11 | INFO | fairseq_cli.train | end of epoch 551 (average epoch stats below)\n",
      "2023-04-17 14:51:11 | INFO | train | epoch 551 | loss 2.383 | nll_loss 0.226 | ppl 1.17 | wps 930.4 | ups 1.37 | wpb 678.5 | bsz 2 | num_updates 17623 | lr 2.35385e-05 | gnorm 2.653 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 14352\n",
      "2023-04-17 14:51:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:51:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:51:11 | INFO | fairseq.trainer | begin training epoch 552\n",
      "2023-04-17 14:51:11 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:51:22 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:51:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:51:23 | INFO | valid | epoch 552 | valid on 'valid' subset | loss 5.933 | nll_loss 4.219 | ppl 18.62 | wps 7245.8 | wpb 290.8 | bsz 1 | num_updates 17655 | best_loss 3.979\n",
      "2023-04-17 14:51:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 552 @ 17655 updates\n",
      "2023-04-17 14:51:23 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:51:35 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:51:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 552 @ 17655 updates, score 5.933) (writing took 12.043158235959709 seconds)\n",
      "2023-04-17 14:51:35 | INFO | fairseq_cli.train | end of epoch 552 (average epoch stats below)\n",
      "2023-04-17 14:51:35 | INFO | train | epoch 552 | loss 2.384 | nll_loss 0.229 | ppl 1.17 | wps 928.9 | ups 1.37 | wpb 678.5 | bsz 2 | num_updates 17655 | lr 2.35264e-05 | gnorm 2.548 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 14375\n",
      "2023-04-17 14:51:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:51:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:51:35 | INFO | fairseq.trainer | begin training epoch 553\n",
      "2023-04-17 14:51:35 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:51:46 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:51:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:51:46 | INFO | valid | epoch 553 | valid on 'valid' subset | loss 5.976 | nll_loss 4.272 | ppl 19.32 | wps 7184.6 | wpb 290.8 | bsz 1 | num_updates 17687 | best_loss 3.979\n",
      "2023-04-17 14:51:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 553 @ 17687 updates\n",
      "2023-04-17 14:51:46 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:51:58 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:51:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 553 @ 17687 updates, score 5.976) (writing took 12.113826301065274 seconds)\n",
      "2023-04-17 14:51:58 | INFO | fairseq_cli.train | end of epoch 553 (average epoch stats below)\n",
      "2023-04-17 14:51:58 | INFO | train | epoch 553 | loss 2.387 | nll_loss 0.231 | ppl 1.17 | wps 924.5 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 17687 | lr 2.35143e-05 | gnorm 2.805 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 14398\n",
      "2023-04-17 14:51:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:51:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:51:58 | INFO | fairseq.trainer | begin training epoch 554\n",
      "2023-04-17 14:51:58 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:52:03 | INFO | train_inner | epoch 554:     13 / 32 loss=2.385, nll_loss=0.228, ppl=1.17, wps=950.4, ups=1.39, wpb=681.3, bsz=2, num_updates=17700, lr=2.35094e-05, gnorm=2.63, clip=100, loss_scale=1, train_wall=34, gb_free=13.9, wall=14403\n",
      "2023-04-17 14:52:10 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:52:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:52:10 | INFO | valid | epoch 554 | valid on 'valid' subset | loss 6.008 | nll_loss 4.308 | ppl 19.81 | wps 7063.9 | wpb 290.8 | bsz 1 | num_updates 17719 | best_loss 3.979\n",
      "2023-04-17 14:52:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 554 @ 17719 updates\n",
      "2023-04-17 14:52:10 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:52:22 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:52:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 554 @ 17719 updates, score 6.008) (writing took 12.248452394967899 seconds)\n",
      "2023-04-17 14:52:22 | INFO | fairseq_cli.train | end of epoch 554 (average epoch stats below)\n",
      "2023-04-17 14:52:22 | INFO | train | epoch 554 | loss 2.379 | nll_loss 0.221 | ppl 1.17 | wps 912.2 | ups 1.34 | wpb 678.5 | bsz 2 | num_updates 17719 | lr 2.35023e-05 | gnorm 2.519 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 14422\n",
      "2023-04-17 14:52:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:52:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:52:22 | INFO | fairseq.trainer | begin training epoch 555\n",
      "2023-04-17 14:52:22 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:52:33 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:52:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:52:33 | INFO | valid | epoch 555 | valid on 'valid' subset | loss 6.011 | nll_loss 4.3 | ppl 19.7 | wps 7220.4 | wpb 290.8 | bsz 1 | num_updates 17751 | best_loss 3.979\n",
      "2023-04-17 14:52:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 555 @ 17751 updates\n",
      "2023-04-17 14:52:33 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:52:45 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:52:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 555 @ 17751 updates, score 6.011) (writing took 12.006360450061038 seconds)\n",
      "2023-04-17 14:52:45 | INFO | fairseq_cli.train | end of epoch 555 (average epoch stats below)\n",
      "2023-04-17 14:52:45 | INFO | train | epoch 555 | loss 2.38 | nll_loss 0.223 | ppl 1.17 | wps 929.5 | ups 1.37 | wpb 678.5 | bsz 2 | num_updates 17751 | lr 2.34902e-05 | gnorm 2.663 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 14446\n",
      "2023-04-17 14:52:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:52:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:52:45 | INFO | fairseq.trainer | begin training epoch 556\n",
      "2023-04-17 14:52:45 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:52:56 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:52:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:52:57 | INFO | valid | epoch 556 | valid on 'valid' subset | loss 6.016 | nll_loss 4.316 | ppl 19.91 | wps 7140 | wpb 290.8 | bsz 1 | num_updates 17783 | best_loss 3.979\n",
      "2023-04-17 14:52:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 556 @ 17783 updates\n",
      "2023-04-17 14:52:57 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:53:09 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 14:53:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 556 @ 17783 updates, score 6.016) (writing took 12.350163591909222 seconds)\n",
      "2023-04-17 14:53:09 | INFO | fairseq_cli.train | end of epoch 556 (average epoch stats below)\n",
      "2023-04-17 14:53:09 | INFO | train | epoch 556 | loss 2.383 | nll_loss 0.226 | ppl 1.17 | wps 917.4 | ups 1.35 | wpb 678.5 | bsz 2 | num_updates 17783 | lr 2.34781e-05 | gnorm 2.661 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 14469\n",
      "2023-04-17 14:53:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:53:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:53:09 | INFO | fairseq.trainer | begin training epoch 557\n",
      "2023-04-17 14:53:09 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:53:15 | INFO | train_inner | epoch 557:     17 / 32 loss=2.381, nll_loss=0.224, ppl=1.17, wps=952.3, ups=1.39, wpb=687.4, bsz=2, num_updates=17800, lr=2.34717e-05, gnorm=2.662, clip=100, loss_scale=1, train_wall=34, gb_free=13.9, wall=14475\n",
      "2023-04-17 14:53:20 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:53:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:53:20 | INFO | valid | epoch 557 | valid on 'valid' subset | loss 6.024 | nll_loss 4.323 | ppl 20.02 | wps 7257.1 | wpb 290.8 | bsz 1 | num_updates 17815 | best_loss 3.979\n",
      "2023-04-17 14:53:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 557 @ 17815 updates\n",
      "2023-04-17 14:53:20 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:53:32 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:53:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 557 @ 17815 updates, score 6.024) (writing took 12.12497074296698 seconds)\n",
      "2023-04-17 14:53:33 | INFO | fairseq_cli.train | end of epoch 557 (average epoch stats below)\n",
      "2023-04-17 14:53:33 | INFO | train | epoch 557 | loss 2.38 | nll_loss 0.225 | ppl 1.17 | wps 926.4 | ups 1.37 | wpb 678.5 | bsz 2 | num_updates 17815 | lr 2.3466e-05 | gnorm 2.612 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 14493\n",
      "2023-04-17 14:53:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:53:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:53:33 | INFO | fairseq.trainer | begin training epoch 558\n",
      "2023-04-17 14:53:33 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:53:44 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:53:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:53:44 | INFO | valid | epoch 558 | valid on 'valid' subset | loss 6.039 | nll_loss 4.345 | ppl 20.32 | wps 7241.2 | wpb 290.8 | bsz 1 | num_updates 17847 | best_loss 3.979\n",
      "2023-04-17 14:53:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 558 @ 17847 updates\n",
      "2023-04-17 14:53:44 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:53:56 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:53:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 558 @ 17847 updates, score 6.039) (writing took 12.030113554908894 seconds)\n",
      "2023-04-17 14:53:56 | INFO | fairseq_cli.train | end of epoch 558 (average epoch stats below)\n",
      "2023-04-17 14:53:56 | INFO | train | epoch 558 | loss 2.38 | nll_loss 0.222 | ppl 1.17 | wps 927.1 | ups 1.37 | wpb 678.5 | bsz 2 | num_updates 17847 | lr 2.3454e-05 | gnorm 2.73 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 14516\n",
      "2023-04-17 14:53:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:53:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:53:56 | INFO | fairseq.trainer | begin training epoch 559\n",
      "2023-04-17 14:53:56 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:54:07 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:54:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:54:07 | INFO | valid | epoch 559 | valid on 'valid' subset | loss 5.932 | nll_loss 4.229 | ppl 18.75 | wps 7231.6 | wpb 290.8 | bsz 1 | num_updates 17879 | best_loss 3.979\n",
      "2023-04-17 14:54:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 559 @ 17879 updates\n",
      "2023-04-17 14:54:07 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:54:19 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:54:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 559 @ 17879 updates, score 5.932) (writing took 11.989636293961667 seconds)\n",
      "2023-04-17 14:54:19 | INFO | fairseq_cli.train | end of epoch 559 (average epoch stats below)\n",
      "2023-04-17 14:54:19 | INFO | train | epoch 559 | loss 2.381 | nll_loss 0.226 | ppl 1.17 | wps 928.6 | ups 1.37 | wpb 678.5 | bsz 2 | num_updates 17879 | lr 2.34419e-05 | gnorm 2.823 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 14540\n",
      "2023-04-17 14:54:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:54:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:54:19 | INFO | fairseq.trainer | begin training epoch 560\n",
      "2023-04-17 14:54:19 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:54:27 | INFO | train_inner | epoch 560:     21 / 32 loss=2.382, nll_loss=0.226, ppl=1.17, wps=937.1, ups=1.39, wpb=672.2, bsz=2, num_updates=17900, lr=2.3434e-05, gnorm=2.778, clip=100, loss_scale=1, train_wall=34, gb_free=13.9, wall=14547\n",
      "2023-04-17 14:54:31 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:54:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:54:31 | INFO | valid | epoch 560 | valid on 'valid' subset | loss 5.9 | nll_loss 4.189 | ppl 18.24 | wps 7184.6 | wpb 290.8 | bsz 1 | num_updates 17911 | best_loss 3.979\n",
      "2023-04-17 14:54:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 560 @ 17911 updates\n",
      "2023-04-17 14:54:31 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:54:43 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:54:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 560 @ 17911 updates, score 5.9) (writing took 12.039883090066724 seconds)\n",
      "2023-04-17 14:54:43 | INFO | fairseq_cli.train | end of epoch 560 (average epoch stats below)\n",
      "2023-04-17 14:54:43 | INFO | train | epoch 560 | loss 2.386 | nll_loss 0.231 | ppl 1.17 | wps 925.5 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 17911 | lr 2.34298e-05 | gnorm 2.956 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 14563\n",
      "2023-04-17 14:54:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:54:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:54:43 | INFO | fairseq.trainer | begin training epoch 561\n",
      "2023-04-17 14:54:43 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:54:54 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:54:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:54:54 | INFO | valid | epoch 561 | valid on 'valid' subset | loss 5.918 | nll_loss 4.205 | ppl 18.44 | wps 7176.3 | wpb 290.8 | bsz 1 | num_updates 17943 | best_loss 3.979\n",
      "2023-04-17 14:54:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 561 @ 17943 updates\n",
      "2023-04-17 14:54:54 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 14:55:06 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:55:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 561 @ 17943 updates, score 5.918) (writing took 12.02883257297799 seconds)\n",
      "2023-04-17 14:55:06 | INFO | fairseq_cli.train | end of epoch 561 (average epoch stats below)\n",
      "2023-04-17 14:55:06 | INFO | train | epoch 561 | loss 2.384 | nll_loss 0.229 | ppl 1.17 | wps 924.7 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 17943 | lr 2.34177e-05 | gnorm 2.543 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 14587\n",
      "2023-04-17 14:55:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:55:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:55:06 | INFO | fairseq.trainer | begin training epoch 562\n",
      "2023-04-17 14:55:06 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:55:18 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:55:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:55:18 | INFO | valid | epoch 562 | valid on 'valid' subset | loss 6.025 | nll_loss 4.313 | ppl 19.88 | wps 7084.7 | wpb 290.8 | bsz 1 | num_updates 17975 | best_loss 3.979\n",
      "2023-04-17 14:55:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 562 @ 17975 updates\n",
      "2023-04-17 14:55:18 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:55:30 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:55:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 562 @ 17975 updates, score 6.025) (writing took 12.215676036081277 seconds)\n",
      "2023-04-17 14:55:30 | INFO | fairseq_cli.train | end of epoch 562 (average epoch stats below)\n",
      "2023-04-17 14:55:30 | INFO | train | epoch 562 | loss 2.38 | nll_loss 0.224 | ppl 1.17 | wps 906.9 | ups 1.34 | wpb 678.5 | bsz 2 | num_updates 17975 | lr 2.34057e-05 | gnorm 2.585 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 14610\n",
      "2023-04-17 14:55:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:55:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:55:30 | INFO | fairseq.trainer | begin training epoch 563\n",
      "2023-04-17 14:55:30 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:55:39 | INFO | train_inner | epoch 563:     25 / 32 loss=2.381, nll_loss=0.226, ppl=1.17, wps=939.2, ups=1.38, wpb=679.3, bsz=2, num_updates=18000, lr=2.33962e-05, gnorm=2.607, clip=100, loss_scale=1, train_wall=35, gb_free=13.9, wall=14619\n",
      "2023-04-17 14:55:41 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:55:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:55:42 | INFO | valid | epoch 563 | valid on 'valid' subset | loss 5.974 | nll_loss 4.274 | ppl 19.34 | wps 7245 | wpb 290.8 | bsz 1 | num_updates 18007 | best_loss 3.979\n",
      "2023-04-17 14:55:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 563 @ 18007 updates\n",
      "2023-04-17 14:55:42 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:55:53 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:55:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 563 @ 18007 updates, score 5.974) (writing took 12.034524771035649 seconds)\n",
      "2023-04-17 14:55:54 | INFO | fairseq_cli.train | end of epoch 563 (average epoch stats below)\n",
      "2023-04-17 14:55:54 | INFO | train | epoch 563 | loss 2.377 | nll_loss 0.221 | ppl 1.17 | wps 924.1 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 18007 | lr 2.33936e-05 | gnorm 2.568 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 14634\n",
      "2023-04-17 14:55:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:55:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:55:54 | INFO | fairseq.trainer | begin training epoch 564\n",
      "2023-04-17 14:55:54 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:56:05 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:56:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:56:05 | INFO | valid | epoch 564 | valid on 'valid' subset | loss 5.979 | nll_loss 4.278 | ppl 19.4 | wps 7248.3 | wpb 290.8 | bsz 1 | num_updates 18039 | best_loss 3.979\n",
      "2023-04-17 14:56:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 564 @ 18039 updates\n",
      "2023-04-17 14:56:05 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:56:17 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:56:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 564 @ 18039 updates, score 5.979) (writing took 12.017018488957547 seconds)\n",
      "2023-04-17 14:56:17 | INFO | fairseq_cli.train | end of epoch 564 (average epoch stats below)\n",
      "2023-04-17 14:56:17 | INFO | train | epoch 564 | loss 2.38 | nll_loss 0.225 | ppl 1.17 | wps 918.9 | ups 1.35 | wpb 678.5 | bsz 2 | num_updates 18039 | lr 2.33815e-05 | gnorm 2.636 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 14658\n",
      "2023-04-17 14:56:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:56:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:56:17 | INFO | fairseq.trainer | begin training epoch 565\n",
      "2023-04-17 14:56:17 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:56:29 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:56:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:56:29 | INFO | valid | epoch 565 | valid on 'valid' subset | loss 5.957 | nll_loss 4.251 | ppl 19.04 | wps 7223.6 | wpb 290.8 | bsz 1 | num_updates 18071 | best_loss 3.979\n",
      "2023-04-17 14:56:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 565 @ 18071 updates\n",
      "2023-04-17 14:56:29 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:56:41 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:56:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 565 @ 18071 updates, score 5.957) (writing took 12.024809733964503 seconds)\n",
      "2023-04-17 14:56:41 | INFO | fairseq_cli.train | end of epoch 565 (average epoch stats below)\n",
      "2023-04-17 14:56:41 | INFO | train | epoch 565 | loss 2.38 | nll_loss 0.224 | ppl 1.17 | wps 926.2 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 18071 | lr 2.33694e-05 | gnorm 2.677 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 14681\n",
      "2023-04-17 14:56:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:56:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:56:41 | INFO | fairseq.trainer | begin training epoch 566\n",
      "2023-04-17 14:56:41 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:56:51 | INFO | train_inner | epoch 566:     29 / 32 loss=2.38, nll_loss=0.224, ppl=1.17, wps=944.8, ups=1.39, wpb=678.8, bsz=2, num_updates=18100, lr=2.33585e-05, gnorm=2.627, clip=100, loss_scale=1, train_wall=35, gb_free=13.9, wall=14691\n",
      "2023-04-17 14:56:52 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:56:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:56:52 | INFO | valid | epoch 566 | valid on 'valid' subset | loss 5.992 | nll_loss 4.306 | ppl 19.79 | wps 7218.4 | wpb 290.8 | bsz 1 | num_updates 18103 | best_loss 3.979\n",
      "2023-04-17 14:56:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 566 @ 18103 updates\n",
      "2023-04-17 14:56:52 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 14:57:04 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:57:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 566 @ 18103 updates, score 5.992) (writing took 11.99936701997649 seconds)\n",
      "2023-04-17 14:57:04 | INFO | fairseq_cli.train | end of epoch 566 (average epoch stats below)\n",
      "2023-04-17 14:57:04 | INFO | train | epoch 566 | loss 2.382 | nll_loss 0.226 | ppl 1.17 | wps 929.1 | ups 1.37 | wpb 678.5 | bsz 2 | num_updates 18103 | lr 2.33574e-05 | gnorm 2.626 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 14704\n",
      "2023-04-17 14:57:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:57:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:57:04 | INFO | fairseq.trainer | begin training epoch 567\n",
      "2023-04-17 14:57:04 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:57:15 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:57:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:57:16 | INFO | valid | epoch 567 | valid on 'valid' subset | loss 5.933 | nll_loss 4.237 | ppl 18.86 | wps 7203.3 | wpb 290.8 | bsz 1 | num_updates 18135 | best_loss 3.979\n",
      "2023-04-17 14:57:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 567 @ 18135 updates\n",
      "2023-04-17 14:57:16 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:57:27 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:57:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 567 @ 18135 updates, score 5.933) (writing took 12.111429648008198 seconds)\n",
      "2023-04-17 14:57:28 | INFO | fairseq_cli.train | end of epoch 567 (average epoch stats below)\n",
      "2023-04-17 14:57:28 | INFO | train | epoch 567 | loss 2.381 | nll_loss 0.227 | ppl 1.17 | wps 923.7 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 18135 | lr 2.33453e-05 | gnorm 2.652 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 14728\n",
      "2023-04-17 14:57:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:57:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:57:28 | INFO | fairseq.trainer | begin training epoch 568\n",
      "2023-04-17 14:57:28 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:57:39 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:57:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:57:39 | INFO | valid | epoch 568 | valid on 'valid' subset | loss 6.034 | nll_loss 4.335 | ppl 20.18 | wps 7107.4 | wpb 290.8 | bsz 1 | num_updates 18167 | best_loss 3.979\n",
      "2023-04-17 14:57:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 568 @ 18167 updates\n",
      "2023-04-17 14:57:39 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:57:51 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:57:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 568 @ 18167 updates, score 6.034) (writing took 12.063538818038069 seconds)\n",
      "2023-04-17 14:57:51 | INFO | fairseq_cli.train | end of epoch 568 (average epoch stats below)\n",
      "2023-04-17 14:57:51 | INFO | train | epoch 568 | loss 2.378 | nll_loss 0.222 | ppl 1.17 | wps 923.1 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 18167 | lr 2.33332e-05 | gnorm 2.674 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 14751\n",
      "2023-04-17 14:57:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:57:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:57:51 | INFO | fairseq.trainer | begin training epoch 569\n",
      "2023-04-17 14:57:51 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:58:02 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:58:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:58:03 | INFO | valid | epoch 569 | valid on 'valid' subset | loss 5.99 | nll_loss 4.298 | ppl 19.68 | wps 7175.4 | wpb 290.8 | bsz 1 | num_updates 18199 | best_loss 3.979\n",
      "2023-04-17 14:58:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 569 @ 18199 updates\n",
      "2023-04-17 14:58:03 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:58:15 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:58:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 569 @ 18199 updates, score 5.99) (writing took 12.418507696944289 seconds)\n",
      "2023-04-17 14:58:15 | INFO | fairseq_cli.train | end of epoch 569 (average epoch stats below)\n",
      "2023-04-17 14:58:15 | INFO | train | epoch 569 | loss 2.379 | nll_loss 0.224 | ppl 1.17 | wps 911.6 | ups 1.34 | wpb 678.5 | bsz 2 | num_updates 18199 | lr 2.33211e-05 | gnorm 2.496 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 14775\n",
      "2023-04-17 14:58:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:58:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:58:15 | INFO | fairseq.trainer | begin training epoch 570\n",
      "2023-04-17 14:58:15 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:58:15 | INFO | train_inner | epoch 570:      1 / 32 loss=2.38, nll_loss=0.225, ppl=1.17, wps=800.9, ups=1.19, wpb=675.8, bsz=2, num_updates=18200, lr=2.33208e-05, gnorm=2.618, clip=100, loss_scale=1, train_wall=34, gb_free=13.9, wall=14776\n",
      "2023-04-17 14:58:26 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:58:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:58:26 | INFO | valid | epoch 570 | valid on 'valid' subset | loss 5.984 | nll_loss 4.288 | ppl 19.53 | wps 6818.7 | wpb 290.8 | bsz 1 | num_updates 18231 | best_loss 3.979\n",
      "2023-04-17 14:58:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 570 @ 18231 updates\n",
      "2023-04-17 14:58:26 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:58:38 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:58:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 570 @ 18231 updates, score 5.984) (writing took 12.184467827901244 seconds)\n",
      "2023-04-17 14:58:39 | INFO | fairseq_cli.train | end of epoch 570 (average epoch stats below)\n",
      "2023-04-17 14:58:39 | INFO | train | epoch 570 | loss 2.376 | nll_loss 0.221 | ppl 1.17 | wps 920.9 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 18231 | lr 2.33091e-05 | gnorm 2.739 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 14799\n",
      "2023-04-17 14:58:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:58:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:58:39 | INFO | fairseq.trainer | begin training epoch 571\n",
      "2023-04-17 14:58:39 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:58:50 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:58:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:58:50 | INFO | valid | epoch 571 | valid on 'valid' subset | loss 5.977 | nll_loss 4.275 | ppl 19.36 | wps 7158.6 | wpb 290.8 | bsz 1 | num_updates 18263 | best_loss 3.979\n",
      "2023-04-17 14:58:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 571 @ 18263 updates\n",
      "2023-04-17 14:58:50 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:59:02 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 14:59:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 571 @ 18263 updates, score 5.977) (writing took 12.306195848970674 seconds)\n",
      "2023-04-17 14:59:02 | INFO | fairseq_cli.train | end of epoch 571 (average epoch stats below)\n",
      "2023-04-17 14:59:02 | INFO | train | epoch 571 | loss 2.379 | nll_loss 0.224 | ppl 1.17 | wps 914.4 | ups 1.35 | wpb 678.5 | bsz 2 | num_updates 18263 | lr 2.3297e-05 | gnorm 2.609 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 14823\n",
      "2023-04-17 14:59:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:59:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:59:02 | INFO | fairseq.trainer | begin training epoch 572\n",
      "2023-04-17 14:59:02 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:59:13 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:59:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:59:14 | INFO | valid | epoch 572 | valid on 'valid' subset | loss 5.97 | nll_loss 4.272 | ppl 19.31 | wps 7323.3 | wpb 290.8 | bsz 1 | num_updates 18295 | best_loss 3.979\n",
      "2023-04-17 14:59:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 572 @ 18295 updates\n",
      "2023-04-17 14:59:14 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:59:26 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:59:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 572 @ 18295 updates, score 5.97) (writing took 12.075012694927864 seconds)\n",
      "2023-04-17 14:59:26 | INFO | fairseq_cli.train | end of epoch 572 (average epoch stats below)\n",
      "2023-04-17 14:59:26 | INFO | train | epoch 572 | loss 2.385 | nll_loss 0.229 | ppl 1.17 | wps 924.7 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 18295 | lr 2.32849e-05 | gnorm 3.266 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 14846\n",
      "2023-04-17 14:59:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:59:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:59:26 | INFO | fairseq.trainer | begin training epoch 573\n",
      "2023-04-17 14:59:26 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 14:59:28 | INFO | train_inner | epoch 573:      5 / 32 loss=2.38, nll_loss=0.225, ppl=1.17, wps=939.5, ups=1.38, wpb=678.6, bsz=2, num_updates=18300, lr=2.3283e-05, gnorm=2.888, clip=100, loss_scale=1, train_wall=34, gb_free=13.9, wall=14848\n",
      "2023-04-17 14:59:37 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 14:59:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:59:37 | INFO | valid | epoch 573 | valid on 'valid' subset | loss 5.941 | nll_loss 4.225 | ppl 18.71 | wps 7240.5 | wpb 290.8 | bsz 1 | num_updates 18327 | best_loss 3.979\n",
      "2023-04-17 14:59:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 573 @ 18327 updates\n",
      "2023-04-17 14:59:37 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:59:49 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 14:59:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 573 @ 18327 updates, score 5.941) (writing took 12.119226687005721 seconds)\n",
      "2023-04-17 14:59:49 | INFO | fairseq_cli.train | end of epoch 573 (average epoch stats below)\n",
      "2023-04-17 14:59:49 | INFO | train | epoch 573 | loss 2.383 | nll_loss 0.23 | ppl 1.17 | wps 918.5 | ups 1.35 | wpb 678.5 | bsz 2 | num_updates 18327 | lr 2.32728e-05 | gnorm 3.014 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 14870\n",
      "2023-04-17 14:59:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 14:59:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 14:59:49 | INFO | fairseq.trainer | begin training epoch 574\n",
      "2023-04-17 14:59:49 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:00:01 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:00:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:00:01 | INFO | valid | epoch 574 | valid on 'valid' subset | loss 5.987 | nll_loss 4.288 | ppl 19.53 | wps 7234.9 | wpb 290.8 | bsz 1 | num_updates 18359 | best_loss 3.979\n",
      "2023-04-17 15:00:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 574 @ 18359 updates\n",
      "2023-04-17 15:00:01 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:00:13 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:00:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 574 @ 18359 updates, score 5.987) (writing took 12.102555279037915 seconds)\n",
      "2023-04-17 15:00:13 | INFO | fairseq_cli.train | end of epoch 574 (average epoch stats below)\n",
      "2023-04-17 15:00:13 | INFO | train | epoch 574 | loss 2.38 | nll_loss 0.224 | ppl 1.17 | wps 925.3 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 18359 | lr 2.32608e-05 | gnorm 2.988 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 14893\n",
      "2023-04-17 15:00:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:00:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:00:13 | INFO | fairseq.trainer | begin training epoch 575\n",
      "2023-04-17 15:00:13 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:00:24 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:00:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:00:24 | INFO | valid | epoch 575 | valid on 'valid' subset | loss 6.023 | nll_loss 4.329 | ppl 20.09 | wps 7173 | wpb 290.8 | bsz 1 | num_updates 18391 | best_loss 3.979\n",
      "2023-04-17 15:00:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 575 @ 18391 updates\n",
      "2023-04-17 15:00:24 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:00:36 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:00:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 575 @ 18391 updates, score 6.023) (writing took 12.154011547099799 seconds)\n",
      "2023-04-17 15:00:36 | INFO | fairseq_cli.train | end of epoch 575 (average epoch stats below)\n",
      "2023-04-17 15:00:36 | INFO | train | epoch 575 | loss 2.378 | nll_loss 0.222 | ppl 1.17 | wps 920.7 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 18391 | lr 2.32487e-05 | gnorm 2.593 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 14917\n",
      "2023-04-17 15:00:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:00:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:00:36 | INFO | fairseq.trainer | begin training epoch 576\n",
      "2023-04-17 15:00:36 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:00:40 | INFO | train_inner | epoch 576:      9 / 32 loss=2.38, nll_loss=0.225, ppl=1.17, wps=942.8, ups=1.39, wpb=679.8, bsz=2, num_updates=18400, lr=2.32453e-05, gnorm=2.823, clip=100, loss_scale=1, train_wall=35, gb_free=13.9, wall=14920\n",
      "2023-04-17 15:00:48 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:00:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:00:48 | INFO | valid | epoch 576 | valid on 'valid' subset | loss 6.004 | nll_loss 4.306 | ppl 19.78 | wps 6834.7 | wpb 290.8 | bsz 1 | num_updates 18423 | best_loss 3.979\n",
      "2023-04-17 15:00:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 576 @ 18423 updates\n",
      "2023-04-17 15:00:48 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 15:01:00 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:01:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 576 @ 18423 updates, score 6.004) (writing took 12.157937670941465 seconds)\n",
      "2023-04-17 15:01:00 | INFO | fairseq_cli.train | end of epoch 576 (average epoch stats below)\n",
      "2023-04-17 15:01:00 | INFO | train | epoch 576 | loss 2.378 | nll_loss 0.224 | ppl 1.17 | wps 917.4 | ups 1.35 | wpb 678.5 | bsz 2 | num_updates 18423 | lr 2.32366e-05 | gnorm 2.695 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 14940\n",
      "2023-04-17 15:01:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:01:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:01:00 | INFO | fairseq.trainer | begin training epoch 577\n",
      "2023-04-17 15:01:00 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:01:11 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:01:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:01:11 | INFO | valid | epoch 577 | valid on 'valid' subset | loss 6.07 | nll_loss 4.374 | ppl 20.74 | wps 7191.2 | wpb 290.8 | bsz 1 | num_updates 18455 | best_loss 3.979\n",
      "2023-04-17 15:01:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 577 @ 18455 updates\n",
      "2023-04-17 15:01:11 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:01:23 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:01:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 577 @ 18455 updates, score 6.07) (writing took 12.000532494974323 seconds)\n",
      "2023-04-17 15:01:23 | INFO | fairseq_cli.train | end of epoch 577 (average epoch stats below)\n",
      "2023-04-17 15:01:23 | INFO | train | epoch 577 | loss 2.374 | nll_loss 0.221 | ppl 1.17 | wps 930.2 | ups 1.37 | wpb 678.5 | bsz 2 | num_updates 18455 | lr 2.32245e-05 | gnorm 2.477 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 14964\n",
      "2023-04-17 15:01:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:01:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:01:23 | INFO | fairseq.trainer | begin training epoch 578\n",
      "2023-04-17 15:01:23 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:01:35 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:01:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:01:35 | INFO | valid | epoch 578 | valid on 'valid' subset | loss 5.974 | nll_loss 4.264 | ppl 19.22 | wps 7240.6 | wpb 290.8 | bsz 1 | num_updates 18487 | best_loss 3.979\n",
      "2023-04-17 15:01:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 578 @ 18487 updates\n",
      "2023-04-17 15:01:35 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:01:47 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:01:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 578 @ 18487 updates, score 5.974) (writing took 12.04896906693466 seconds)\n",
      "2023-04-17 15:01:47 | INFO | fairseq_cli.train | end of epoch 578 (average epoch stats below)\n",
      "2023-04-17 15:01:47 | INFO | train | epoch 578 | loss 2.379 | nll_loss 0.224 | ppl 1.17 | wps 927.8 | ups 1.37 | wpb 678.5 | bsz 2 | num_updates 18487 | lr 2.32125e-05 | gnorm 2.995 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 14987\n",
      "2023-04-17 15:01:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:01:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:01:47 | INFO | fairseq.trainer | begin training epoch 579\n",
      "2023-04-17 15:01:47 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:01:52 | INFO | train_inner | epoch 579:     13 / 32 loss=2.377, nll_loss=0.223, ppl=1.17, wps=941.6, ups=1.39, wpb=676.1, bsz=2, num_updates=18500, lr=2.32075e-05, gnorm=2.736, clip=100, loss_scale=1, train_wall=34, gb_free=13.9, wall=14992\n",
      "2023-04-17 15:01:58 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:01:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:01:58 | INFO | valid | epoch 579 | valid on 'valid' subset | loss 6.075 | nll_loss 4.377 | ppl 20.78 | wps 7080 | wpb 290.8 | bsz 1 | num_updates 18519 | best_loss 3.979\n",
      "2023-04-17 15:01:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 579 @ 18519 updates\n",
      "2023-04-17 15:01:58 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:02:10 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:02:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 579 @ 18519 updates, score 6.075) (writing took 12.141595012042671 seconds)\n",
      "2023-04-17 15:02:10 | INFO | fairseq_cli.train | end of epoch 579 (average epoch stats below)\n",
      "2023-04-17 15:02:10 | INFO | train | epoch 579 | loss 2.376 | nll_loss 0.22 | ppl 1.16 | wps 922 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 18519 | lr 2.32004e-05 | gnorm 2.692 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 15011\n",
      "2023-04-17 15:02:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:02:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:02:10 | INFO | fairseq.trainer | begin training epoch 580\n",
      "2023-04-17 15:02:10 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:02:22 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:02:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:02:22 | INFO | valid | epoch 580 | valid on 'valid' subset | loss 6.09 | nll_loss 4.392 | ppl 21 | wps 7229.4 | wpb 290.8 | bsz 1 | num_updates 18551 | best_loss 3.979\n",
      "2023-04-17 15:02:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 580 @ 18551 updates\n",
      "2023-04-17 15:02:22 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:02:34 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:02:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 580 @ 18551 updates, score 6.09) (writing took 12.09999023610726 seconds)\n",
      "2023-04-17 15:02:34 | INFO | fairseq_cli.train | end of epoch 580 (average epoch stats below)\n",
      "2023-04-17 15:02:34 | INFO | train | epoch 580 | loss 2.375 | nll_loss 0.22 | ppl 1.17 | wps 921.8 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 18551 | lr 2.31883e-05 | gnorm 2.49 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 15034\n",
      "2023-04-17 15:02:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:02:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:02:34 | INFO | fairseq.trainer | begin training epoch 581\n",
      "2023-04-17 15:02:34 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:02:46 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:02:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:02:46 | INFO | valid | epoch 581 | valid on 'valid' subset | loss 6.07 | nll_loss 4.371 | ppl 20.69 | wps 7104.7 | wpb 290.8 | bsz 1 | num_updates 18583 | best_loss 3.979\n",
      "2023-04-17 15:02:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 581 @ 18583 updates\n",
      "2023-04-17 15:02:46 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:02:58 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 15:02:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 581 @ 18583 updates, score 6.07) (writing took 12.117122083087452 seconds)\n",
      "2023-04-17 15:02:58 | INFO | fairseq_cli.train | end of epoch 581 (average epoch stats below)\n",
      "2023-04-17 15:02:58 | INFO | train | epoch 581 | loss 2.371 | nll_loss 0.215 | ppl 1.16 | wps 903.4 | ups 1.33 | wpb 678.5 | bsz 2 | num_updates 18583 | lr 2.31762e-05 | gnorm 2.462 | clip 100 | loss_scale 1 | train_wall 12 | gb_free 13.9 | wall 15058\n",
      "2023-04-17 15:02:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:02:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:02:58 | INFO | fairseq.trainer | begin training epoch 582\n",
      "2023-04-17 15:02:58 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:03:04 | INFO | train_inner | epoch 582:     17 / 32 loss=2.374, nll_loss=0.218, ppl=1.16, wps=936.3, ups=1.38, wpb=679.3, bsz=2, num_updates=18600, lr=2.31698e-05, gnorm=2.51, clip=100, loss_scale=1, train_wall=35, gb_free=13.9, wall=15064\n",
      "2023-04-17 15:03:09 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:03:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:03:10 | INFO | valid | epoch 582 | valid on 'valid' subset | loss 5.973 | nll_loss 4.271 | ppl 19.3 | wps 7255.8 | wpb 290.8 | bsz 1 | num_updates 18615 | best_loss 3.979\n",
      "2023-04-17 15:03:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 582 @ 18615 updates\n",
      "2023-04-17 15:03:10 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:03:20 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:03:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 582 @ 18615 updates, score 5.973) (writing took 11.04586949700024 seconds)\n",
      "2023-04-17 15:03:21 | INFO | fairseq_cli.train | end of epoch 582 (average epoch stats below)\n",
      "2023-04-17 15:03:21 | INFO | train | epoch 582 | loss 2.374 | nll_loss 0.218 | ppl 1.16 | wps 959.7 | ups 1.41 | wpb 678.5 | bsz 2 | num_updates 18615 | lr 2.31642e-05 | gnorm 2.563 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 15081\n",
      "2023-04-17 15:03:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:03:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:03:21 | INFO | fairseq.trainer | begin training epoch 583\n",
      "2023-04-17 15:03:21 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:03:33 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:03:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:03:33 | INFO | valid | epoch 583 | valid on 'valid' subset | loss 5.991 | nll_loss 4.296 | ppl 19.65 | wps 7206.6 | wpb 290.8 | bsz 1 | num_updates 18647 | best_loss 3.979\n",
      "2023-04-17 15:03:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 583 @ 18647 updates\n",
      "2023-04-17 15:03:33 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:03:43 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:03:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 583 @ 18647 updates, score 5.991) (writing took 10.240532493917271 seconds)\n",
      "2023-04-17 15:03:43 | INFO | fairseq_cli.train | end of epoch 583 (average epoch stats below)\n",
      "2023-04-17 15:03:43 | INFO | train | epoch 583 | loss 2.381 | nll_loss 0.227 | ppl 1.17 | wps 955.3 | ups 1.41 | wpb 678.5 | bsz 2 | num_updates 18647 | lr 2.31521e-05 | gnorm 2.875 | clip 100 | loss_scale 1 | train_wall 12 | gb_free 13.9 | wall 15104\n",
      "2023-04-17 15:03:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:03:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:03:43 | INFO | fairseq.trainer | begin training epoch 584\n",
      "2023-04-17 15:03:43 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:03:56 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:03:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:03:56 | INFO | valid | epoch 584 | valid on 'valid' subset | loss 6.018 | nll_loss 4.33 | ppl 20.11 | wps 7232.5 | wpb 290.8 | bsz 1 | num_updates 18679 | best_loss 3.979\n",
      "2023-04-17 15:03:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 584 @ 18679 updates\n",
      "2023-04-17 15:03:56 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:04:08 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:04:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 584 @ 18679 updates, score 6.018) (writing took 12.104580660001375 seconds)\n",
      "2023-04-17 15:04:08 | INFO | fairseq_cli.train | end of epoch 584 (average epoch stats below)\n",
      "2023-04-17 15:04:08 | INFO | train | epoch 584 | loss 2.379 | nll_loss 0.224 | ppl 1.17 | wps 884.7 | ups 1.3 | wpb 678.5 | bsz 2 | num_updates 18679 | lr 2.314e-05 | gnorm 2.77 | clip 100 | loss_scale 1 | train_wall 12 | gb_free 13.9 | wall 15128\n",
      "2023-04-17 15:04:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:04:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:04:08 | INFO | fairseq.trainer | begin training epoch 585\n",
      "2023-04-17 15:04:08 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:04:16 | INFO | train_inner | epoch 585:     21 / 32 loss=2.378, nll_loss=0.223, ppl=1.17, wps=934.1, ups=1.39, wpb=672.6, bsz=2, num_updates=18700, lr=2.31321e-05, gnorm=2.761, clip=100, loss_scale=1, train_wall=37, gb_free=13.9, wall=15136\n",
      "2023-04-17 15:04:20 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:04:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:04:20 | INFO | valid | epoch 585 | valid on 'valid' subset | loss 5.978 | nll_loss 4.291 | ppl 19.57 | wps 7169.4 | wpb 290.8 | bsz 1 | num_updates 18711 | best_loss 3.979\n",
      "2023-04-17 15:04:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 585 @ 18711 updates\n",
      "2023-04-17 15:04:20 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:04:32 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:04:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 585 @ 18711 updates, score 5.978) (writing took 12.03884926601313 seconds)\n",
      "2023-04-17 15:04:32 | INFO | fairseq_cli.train | end of epoch 585 (average epoch stats below)\n",
      "2023-04-17 15:04:32 | INFO | train | epoch 585 | loss 2.377 | nll_loss 0.222 | ppl 1.17 | wps 886.7 | ups 1.31 | wpb 678.5 | bsz 2 | num_updates 18711 | lr 2.31279e-05 | gnorm 2.751 | clip 100 | loss_scale 1 | train_wall 12 | gb_free 13.9 | wall 15153\n",
      "2023-04-17 15:04:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:04:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:04:32 | INFO | fairseq.trainer | begin training epoch 586\n",
      "2023-04-17 15:04:32 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:04:44 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:04:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:04:45 | INFO | valid | epoch 586 | valid on 'valid' subset | loss 6.024 | nll_loss 4.332 | ppl 20.14 | wps 7142 | wpb 290.8 | bsz 1 | num_updates 18743 | best_loss 3.979\n",
      "2023-04-17 15:04:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 586 @ 18743 updates\n",
      "2023-04-17 15:04:45 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 15:04:57 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:04:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 586 @ 18743 updates, score 6.024) (writing took 12.097612168989144 seconds)\n",
      "2023-04-17 15:04:57 | INFO | fairseq_cli.train | end of epoch 586 (average epoch stats below)\n",
      "2023-04-17 15:04:57 | INFO | train | epoch 586 | loss 2.377 | nll_loss 0.223 | ppl 1.17 | wps 893.4 | ups 1.32 | wpb 678.5 | bsz 2 | num_updates 18743 | lr 2.31158e-05 | gnorm 2.701 | clip 100 | loss_scale 1 | train_wall 12 | gb_free 13.9 | wall 15177\n",
      "2023-04-17 15:04:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:04:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:04:57 | INFO | fairseq.trainer | begin training epoch 587\n",
      "2023-04-17 15:04:57 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:05:08 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:05:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:05:08 | INFO | valid | epoch 587 | valid on 'valid' subset | loss 6.05 | nll_loss 4.363 | ppl 20.58 | wps 7088.2 | wpb 290.8 | bsz 1 | num_updates 18775 | best_loss 3.979\n",
      "2023-04-17 15:05:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 587 @ 18775 updates\n",
      "2023-04-17 15:05:08 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:05:20 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:05:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 587 @ 18775 updates, score 6.05) (writing took 12.095685980049893 seconds)\n",
      "2023-04-17 15:05:20 | INFO | fairseq_cli.train | end of epoch 587 (average epoch stats below)\n",
      "2023-04-17 15:05:20 | INFO | train | epoch 587 | loss 2.375 | nll_loss 0.221 | ppl 1.17 | wps 925.1 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 18775 | lr 2.31038e-05 | gnorm 2.608 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 15200\n",
      "2023-04-17 15:05:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:05:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:05:20 | INFO | fairseq.trainer | begin training epoch 588\n",
      "2023-04-17 15:05:20 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:05:30 | INFO | train_inner | epoch 588:     25 / 32 loss=2.377, nll_loss=0.223, ppl=1.17, wps=928.9, ups=1.36, wpb=683.5, bsz=2, num_updates=18800, lr=2.30943e-05, gnorm=2.7, clip=100, loss_scale=1, train_wall=36, gb_free=13.9, wall=15210\n",
      "2023-04-17 15:05:32 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:05:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:05:32 | INFO | valid | epoch 588 | valid on 'valid' subset | loss 6.038 | nll_loss 4.342 | ppl 20.28 | wps 6103.2 | wpb 290.8 | bsz 1 | num_updates 18807 | best_loss 3.979\n",
      "2023-04-17 15:05:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 588 @ 18807 updates\n",
      "2023-04-17 15:05:32 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:05:44 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:05:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 588 @ 18807 updates, score 6.038) (writing took 12.288108658976853 seconds)\n",
      "2023-04-17 15:05:45 | INFO | fairseq_cli.train | end of epoch 588 (average epoch stats below)\n",
      "2023-04-17 15:05:45 | INFO | train | epoch 588 | loss 2.376 | nll_loss 0.221 | ppl 1.17 | wps 887.7 | ups 1.31 | wpb 678.5 | bsz 2 | num_updates 18807 | lr 2.30917e-05 | gnorm 2.689 | clip 100 | loss_scale 1 | train_wall 12 | gb_free 13.9 | wall 15225\n",
      "2023-04-17 15:05:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:05:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:05:45 | INFO | fairseq.trainer | begin training epoch 589\n",
      "2023-04-17 15:05:45 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:05:57 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:05:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:05:57 | INFO | valid | epoch 589 | valid on 'valid' subset | loss 6.1 | nll_loss 4.416 | ppl 21.35 | wps 6257.7 | wpb 290.8 | bsz 1 | num_updates 18839 | best_loss 3.979\n",
      "2023-04-17 15:05:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 589 @ 18839 updates\n",
      "2023-04-17 15:05:57 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:06:09 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:06:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 589 @ 18839 updates, score 6.1) (writing took 12.290610746014863 seconds)\n",
      "2023-04-17 15:06:09 | INFO | fairseq_cli.train | end of epoch 589 (average epoch stats below)\n",
      "2023-04-17 15:06:09 | INFO | train | epoch 589 | loss 2.371 | nll_loss 0.217 | ppl 1.16 | wps 887.6 | ups 1.31 | wpb 678.5 | bsz 2 | num_updates 18839 | lr 2.30796e-05 | gnorm 2.386 | clip 100 | loss_scale 1 | train_wall 12 | gb_free 13.9 | wall 15249\n",
      "2023-04-17 15:06:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:06:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:06:09 | INFO | fairseq.trainer | begin training epoch 590\n",
      "2023-04-17 15:06:09 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:06:20 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:06:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:06:21 | INFO | valid | epoch 590 | valid on 'valid' subset | loss 5.966 | nll_loss 4.26 | ppl 19.16 | wps 7202.9 | wpb 290.8 | bsz 1 | num_updates 18871 | best_loss 3.979\n",
      "2023-04-17 15:06:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 590 @ 18871 updates\n",
      "2023-04-17 15:06:21 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:06:29 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:06:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 590 @ 18871 updates, score 5.966) (writing took 8.731623754021712 seconds)\n",
      "2023-04-17 15:06:29 | INFO | fairseq_cli.train | end of epoch 590 (average epoch stats below)\n",
      "2023-04-17 15:06:29 | INFO | train | epoch 590 | loss 2.376 | nll_loss 0.222 | ppl 1.17 | wps 1072.2 | ups 1.58 | wpb 678.5 | bsz 2 | num_updates 18871 | lr 2.30675e-05 | gnorm 2.653 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 15270\n",
      "2023-04-17 15:06:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:06:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:06:29 | INFO | fairseq.trainer | begin training epoch 591\n",
      "2023-04-17 15:06:29 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:06:40 | INFO | train_inner | epoch 591:     29 / 32 loss=2.374, nll_loss=0.219, ppl=1.16, wps=973.2, ups=1.42, wpb=684.8, bsz=2, num_updates=18900, lr=2.30566e-05, gnorm=2.539, clip=100, loss_scale=1, train_wall=36, gb_free=13.9, wall=15280\n",
      "2023-04-17 15:06:41 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:06:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:06:41 | INFO | valid | epoch 591 | valid on 'valid' subset | loss 5.927 | nll_loss 4.229 | ppl 18.75 | wps 6954.4 | wpb 290.8 | bsz 1 | num_updates 18903 | best_loss 3.979\n",
      "2023-04-17 15:06:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 591 @ 18903 updates\n",
      "2023-04-17 15:06:41 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 15:06:52 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:06:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 591 @ 18903 updates, score 5.927) (writing took 11.078126866021194 seconds)\n",
      "2023-04-17 15:06:52 | INFO | fairseq_cli.train | end of epoch 591 (average epoch stats below)\n",
      "2023-04-17 15:06:52 | INFO | train | epoch 591 | loss 2.375 | nll_loss 0.222 | ppl 1.17 | wps 945.6 | ups 1.39 | wpb 678.5 | bsz 2 | num_updates 18903 | lr 2.30555e-05 | gnorm 2.62 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 15293\n",
      "2023-04-17 15:06:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:06:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:06:52 | INFO | fairseq.trainer | begin training epoch 592\n",
      "2023-04-17 15:06:52 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:07:04 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:07:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:07:04 | INFO | valid | epoch 592 | valid on 'valid' subset | loss 5.956 | nll_loss 4.262 | ppl 19.19 | wps 7142.5 | wpb 290.8 | bsz 1 | num_updates 18935 | best_loss 3.979\n",
      "2023-04-17 15:07:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 592 @ 18935 updates\n",
      "2023-04-17 15:07:04 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:07:16 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:07:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 592 @ 18935 updates, score 5.956) (writing took 12.225532358977944 seconds)\n",
      "2023-04-17 15:07:16 | INFO | fairseq_cli.train | end of epoch 592 (average epoch stats below)\n",
      "2023-04-17 15:07:16 | INFO | train | epoch 592 | loss 2.376 | nll_loss 0.221 | ppl 1.17 | wps 904.1 | ups 1.33 | wpb 678.5 | bsz 2 | num_updates 18935 | lr 2.30434e-05 | gnorm 2.487 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 15317\n",
      "2023-04-17 15:07:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:07:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:07:16 | INFO | fairseq.trainer | begin training epoch 593\n",
      "2023-04-17 15:07:16 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:07:28 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:07:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:07:28 | INFO | valid | epoch 593 | valid on 'valid' subset | loss 6.052 | nll_loss 4.364 | ppl 20.59 | wps 7138.7 | wpb 290.8 | bsz 1 | num_updates 18967 | best_loss 3.979\n",
      "2023-04-17 15:07:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 593 @ 18967 updates\n",
      "2023-04-17 15:07:28 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:07:40 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:07:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 593 @ 18967 updates, score 6.052) (writing took 12.089079873985611 seconds)\n",
      "2023-04-17 15:07:40 | INFO | fairseq_cli.train | end of epoch 593 (average epoch stats below)\n",
      "2023-04-17 15:07:40 | INFO | train | epoch 593 | loss 2.375 | nll_loss 0.22 | ppl 1.16 | wps 924.6 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 18967 | lr 2.30313e-05 | gnorm 2.721 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 15340\n",
      "2023-04-17 15:07:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:07:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:07:40 | INFO | fairseq.trainer | begin training epoch 594\n",
      "2023-04-17 15:07:40 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:07:51 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:07:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:07:51 | INFO | valid | epoch 594 | valid on 'valid' subset | loss 6.042 | nll_loss 4.356 | ppl 20.48 | wps 7223.7 | wpb 290.8 | bsz 1 | num_updates 18999 | best_loss 3.979\n",
      "2023-04-17 15:07:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 594 @ 18999 updates\n",
      "2023-04-17 15:07:51 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:08:03 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:08:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 594 @ 18999 updates, score 6.042) (writing took 12.14112564898096 seconds)\n",
      "2023-04-17 15:08:03 | INFO | fairseq_cli.train | end of epoch 594 (average epoch stats below)\n",
      "2023-04-17 15:08:03 | INFO | train | epoch 594 | loss 2.372 | nll_loss 0.219 | ppl 1.16 | wps 925.2 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 18999 | lr 2.30192e-05 | gnorm 2.497 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 15364\n",
      "2023-04-17 15:08:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:08:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:08:03 | INFO | fairseq.trainer | begin training epoch 595\n",
      "2023-04-17 15:08:03 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:08:04 | INFO | train_inner | epoch 595:      1 / 32 loss=2.374, nll_loss=0.22, ppl=1.17, wps=800.5, ups=1.2, wpb=669.7, bsz=2, num_updates=19000, lr=2.30189e-05, gnorm=2.595, clip=100, loss_scale=1, train_wall=35, gb_free=13.9, wall=15364\n",
      "2023-04-17 15:08:14 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:08:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:08:15 | INFO | valid | epoch 595 | valid on 'valid' subset | loss 6.083 | nll_loss 4.393 | ppl 21.01 | wps 7069.9 | wpb 290.8 | bsz 1 | num_updates 19031 | best_loss 3.979\n",
      "2023-04-17 15:08:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 595 @ 19031 updates\n",
      "2023-04-17 15:08:15 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:08:27 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:08:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 595 @ 19031 updates, score 6.083) (writing took 12.227145154960454 seconds)\n",
      "2023-04-17 15:08:27 | INFO | fairseq_cli.train | end of epoch 595 (average epoch stats below)\n",
      "2023-04-17 15:08:27 | INFO | train | epoch 595 | loss 2.369 | nll_loss 0.212 | ppl 1.16 | wps 920 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 19031 | lr 2.30072e-05 | gnorm 2.392 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 15387\n",
      "2023-04-17 15:08:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:08:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:08:27 | INFO | fairseq.trainer | begin training epoch 596\n",
      "2023-04-17 15:08:27 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:08:38 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:08:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:08:39 | INFO | valid | epoch 596 | valid on 'valid' subset | loss 6.023 | nll_loss 4.331 | ppl 20.13 | wps 6906 | wpb 290.8 | bsz 1 | num_updates 19063 | best_loss 3.979\n",
      "2023-04-17 15:08:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 596 @ 19063 updates\n",
      "2023-04-17 15:08:39 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:08:52 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 15:08:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 596 @ 19063 updates, score 6.023) (writing took 13.29884875100106 seconds)\n",
      "2023-04-17 15:08:52 | INFO | fairseq_cli.train | end of epoch 596 (average epoch stats below)\n",
      "2023-04-17 15:08:52 | INFO | train | epoch 596 | loss 2.371 | nll_loss 0.218 | ppl 1.16 | wps 867.5 | ups 1.28 | wpb 678.5 | bsz 2 | num_updates 19063 | lr 2.29951e-05 | gnorm 2.545 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 15412\n",
      "2023-04-17 15:08:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:08:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:08:52 | INFO | fairseq.trainer | begin training epoch 597\n",
      "2023-04-17 15:08:52 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:09:03 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:09:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:09:03 | INFO | valid | epoch 597 | valid on 'valid' subset | loss 6.053 | nll_loss 4.363 | ppl 20.57 | wps 7257.2 | wpb 290.8 | bsz 1 | num_updates 19095 | best_loss 3.979\n",
      "2023-04-17 15:09:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 597 @ 19095 updates\n",
      "2023-04-17 15:09:03 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:09:16 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:09:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 597 @ 19095 updates, score 6.053) (writing took 12.351253394968808 seconds)\n",
      "2023-04-17 15:09:16 | INFO | fairseq_cli.train | end of epoch 597 (average epoch stats below)\n",
      "2023-04-17 15:09:16 | INFO | train | epoch 597 | loss 2.372 | nll_loss 0.218 | ppl 1.16 | wps 912.8 | ups 1.35 | wpb 678.5 | bsz 2 | num_updates 19095 | lr 2.2983e-05 | gnorm 2.938 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 15436\n",
      "2023-04-17 15:09:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:09:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:09:16 | INFO | fairseq.trainer | begin training epoch 598\n",
      "2023-04-17 15:09:16 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:09:18 | INFO | train_inner | epoch 598:      5 / 32 loss=2.371, nll_loss=0.216, ppl=1.16, wps=918.5, ups=1.35, wpb=677.9, bsz=2, num_updates=19100, lr=2.29811e-05, gnorm=2.618, clip=100, loss_scale=1, train_wall=35, gb_free=13.9, wall=15438\n",
      "2023-04-17 15:09:27 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:09:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:09:27 | INFO | valid | epoch 598 | valid on 'valid' subset | loss 6.01 | nll_loss 4.31 | ppl 19.84 | wps 7116 | wpb 290.8 | bsz 1 | num_updates 19127 | best_loss 3.979\n",
      "2023-04-17 15:09:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 598 @ 19127 updates\n",
      "2023-04-17 15:09:27 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:09:39 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:09:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 598 @ 19127 updates, score 6.01) (writing took 12.25357088597957 seconds)\n",
      "2023-04-17 15:09:39 | INFO | fairseq_cli.train | end of epoch 598 (average epoch stats below)\n",
      "2023-04-17 15:09:39 | INFO | train | epoch 598 | loss 2.374 | nll_loss 0.221 | ppl 1.17 | wps 920.3 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 19127 | lr 2.29709e-05 | gnorm 3.268 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 15460\n",
      "2023-04-17 15:09:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:09:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:09:39 | INFO | fairseq.trainer | begin training epoch 599\n",
      "2023-04-17 15:09:39 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:09:50 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:09:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:09:51 | INFO | valid | epoch 599 | valid on 'valid' subset | loss 6.052 | nll_loss 4.355 | ppl 20.47 | wps 7202 | wpb 290.8 | bsz 1 | num_updates 19159 | best_loss 3.979\n",
      "2023-04-17 15:09:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 599 @ 19159 updates\n",
      "2023-04-17 15:09:51 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:10:02 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:10:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 599 @ 19159 updates, score 6.052) (writing took 12.07925619801972 seconds)\n",
      "2023-04-17 15:10:03 | INFO | fairseq_cli.train | end of epoch 599 (average epoch stats below)\n",
      "2023-04-17 15:10:03 | INFO | train | epoch 599 | loss 2.371 | nll_loss 0.215 | ppl 1.16 | wps 928.2 | ups 1.37 | wpb 678.5 | bsz 2 | num_updates 19159 | lr 2.29589e-05 | gnorm 2.573 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 15483\n",
      "2023-04-17 15:10:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:10:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:10:03 | INFO | fairseq.trainer | begin training epoch 600\n",
      "2023-04-17 15:10:03 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:10:14 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:10:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:10:14 | INFO | valid | epoch 600 | valid on 'valid' subset | loss 5.984 | nll_loss 4.28 | ppl 19.43 | wps 6638.4 | wpb 290.8 | bsz 1 | num_updates 19191 | best_loss 3.979\n",
      "2023-04-17 15:10:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 600 @ 19191 updates\n",
      "2023-04-17 15:10:14 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:10:27 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:10:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 600 @ 19191 updates, score 5.984) (writing took 12.846016322029755 seconds)\n",
      "2023-04-17 15:10:27 | INFO | fairseq_cli.train | end of epoch 600 (average epoch stats below)\n",
      "2023-04-17 15:10:27 | INFO | train | epoch 600 | loss 2.372 | nll_loss 0.216 | ppl 1.16 | wps 887.2 | ups 1.31 | wpb 678.5 | bsz 2 | num_updates 19191 | lr 2.29468e-05 | gnorm 2.487 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 15507\n",
      "2023-04-17 15:10:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:10:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:10:27 | INFO | fairseq.trainer | begin training epoch 601\n",
      "2023-04-17 15:10:27 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:10:31 | INFO | train_inner | epoch 601:      9 / 32 loss=2.371, nll_loss=0.217, ppl=1.16, wps=936, ups=1.37, wpb=683.5, bsz=2, num_updates=19200, lr=2.29434e-05, gnorm=2.768, clip=100, loss_scale=1, train_wall=35, gb_free=13.9, wall=15511\n",
      "2023-04-17 15:10:39 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:10:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:10:39 | INFO | valid | epoch 601 | valid on 'valid' subset | loss 6.045 | nll_loss 4.347 | ppl 20.35 | wps 6893.3 | wpb 290.8 | bsz 1 | num_updates 19223 | best_loss 3.979\n",
      "2023-04-17 15:10:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 601 @ 19223 updates\n",
      "2023-04-17 15:10:39 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 15:10:51 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:10:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 601 @ 19223 updates, score 6.045) (writing took 12.355345763033256 seconds)\n",
      "2023-04-17 15:10:52 | INFO | fairseq_cli.train | end of epoch 601 (average epoch stats below)\n",
      "2023-04-17 15:10:52 | INFO | train | epoch 601 | loss 2.367 | nll_loss 0.213 | ppl 1.16 | wps 891.1 | ups 1.31 | wpb 678.5 | bsz 2 | num_updates 19223 | lr 2.29347e-05 | gnorm 2.316 | clip 100 | loss_scale 1 | train_wall 12 | gb_free 13.9 | wall 15532\n",
      "2023-04-17 15:10:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:10:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:10:52 | INFO | fairseq.trainer | begin training epoch 602\n",
      "2023-04-17 15:10:52 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:11:03 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:11:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:11:03 | INFO | valid | epoch 602 | valid on 'valid' subset | loss 6.03 | nll_loss 4.328 | ppl 20.08 | wps 7121.1 | wpb 290.8 | bsz 1 | num_updates 19255 | best_loss 3.979\n",
      "2023-04-17 15:11:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 602 @ 19255 updates\n",
      "2023-04-17 15:11:03 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:11:15 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:11:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 602 @ 19255 updates, score 6.03) (writing took 12.128654025029391 seconds)\n",
      "2023-04-17 15:11:15 | INFO | fairseq_cli.train | end of epoch 602 (average epoch stats below)\n",
      "2023-04-17 15:11:15 | INFO | train | epoch 602 | loss 2.373 | nll_loss 0.219 | ppl 1.16 | wps 923.2 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 19255 | lr 2.29226e-05 | gnorm 2.537 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 15555\n",
      "2023-04-17 15:11:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:11:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:11:15 | INFO | fairseq.trainer | begin training epoch 603\n",
      "2023-04-17 15:11:15 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:11:26 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:11:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:11:26 | INFO | valid | epoch 603 | valid on 'valid' subset | loss 6.028 | nll_loss 4.34 | ppl 20.25 | wps 7178.4 | wpb 290.8 | bsz 1 | num_updates 19287 | best_loss 3.979\n",
      "2023-04-17 15:11:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 603 @ 19287 updates\n",
      "2023-04-17 15:11:26 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:11:38 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:11:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 603 @ 19287 updates, score 6.028) (writing took 12.251214454998262 seconds)\n",
      "2023-04-17 15:11:39 | INFO | fairseq_cli.train | end of epoch 603 (average epoch stats below)\n",
      "2023-04-17 15:11:39 | INFO | train | epoch 603 | loss 2.373 | nll_loss 0.218 | ppl 1.16 | wps 918.7 | ups 1.35 | wpb 678.5 | bsz 2 | num_updates 19287 | lr 2.29106e-05 | gnorm 2.746 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 15579\n",
      "2023-04-17 15:11:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:11:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:11:39 | INFO | fairseq.trainer | begin training epoch 604\n",
      "2023-04-17 15:11:39 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:11:43 | INFO | train_inner | epoch 604:     13 / 32 loss=2.372, nll_loss=0.217, ppl=1.16, wps=941.8, ups=1.37, wpb=685.5, bsz=2, num_updates=19300, lr=2.29057e-05, gnorm=2.516, clip=100, loss_scale=1, train_wall=35, gb_free=13.9, wall=15584\n",
      "2023-04-17 15:11:50 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:11:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:11:50 | INFO | valid | epoch 604 | valid on 'valid' subset | loss 5.954 | nll_loss 4.25 | ppl 19.03 | wps 7196.3 | wpb 290.8 | bsz 1 | num_updates 19319 | best_loss 3.979\n",
      "2023-04-17 15:11:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 604 @ 19319 updates\n",
      "2023-04-17 15:11:50 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:11:58 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:11:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 604 @ 19319 updates, score 5.954) (writing took 8.379276019986719 seconds)\n",
      "2023-04-17 15:11:58 | INFO | fairseq_cli.train | end of epoch 604 (average epoch stats below)\n",
      "2023-04-17 15:11:58 | INFO | train | epoch 604 | loss 2.373 | nll_loss 0.218 | ppl 1.16 | wps 1100.9 | ups 1.62 | wpb 678.5 | bsz 2 | num_updates 19319 | lr 2.28985e-05 | gnorm 2.457 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 15599\n",
      "2023-04-17 15:11:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:11:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:11:58 | INFO | fairseq.trainer | begin training epoch 605\n",
      "2023-04-17 15:11:58 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:12:10 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:12:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:12:11 | INFO | valid | epoch 605 | valid on 'valid' subset | loss 6.004 | nll_loss 4.31 | ppl 19.83 | wps 6171.6 | wpb 290.8 | bsz 1 | num_updates 19351 | best_loss 3.979\n",
      "2023-04-17 15:12:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 605 @ 19351 updates\n",
      "2023-04-17 15:12:11 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:12:23 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:12:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 605 @ 19351 updates, score 6.004) (writing took 12.258032141951844 seconds)\n",
      "2023-04-17 15:12:23 | INFO | fairseq_cli.train | end of epoch 605 (average epoch stats below)\n",
      "2023-04-17 15:12:23 | INFO | train | epoch 605 | loss 2.369 | nll_loss 0.215 | ppl 1.16 | wps 887.9 | ups 1.31 | wpb 678.5 | bsz 2 | num_updates 19351 | lr 2.28864e-05 | gnorm 2.318 | clip 100 | loss_scale 1 | train_wall 12 | gb_free 13.9 | wall 15623\n",
      "2023-04-17 15:12:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:12:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:12:23 | INFO | fairseq.trainer | begin training epoch 606\n",
      "2023-04-17 15:12:23 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:12:34 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:12:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:12:34 | INFO | valid | epoch 606 | valid on 'valid' subset | loss 5.948 | nll_loss 4.246 | ppl 18.98 | wps 7222.1 | wpb 290.8 | bsz 1 | num_updates 19383 | best_loss 3.979\n",
      "2023-04-17 15:12:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 606 @ 19383 updates\n",
      "2023-04-17 15:12:34 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:12:46 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 15:12:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 606 @ 19383 updates, score 5.948) (writing took 12.072119520045817 seconds)\n",
      "2023-04-17 15:12:46 | INFO | fairseq_cli.train | end of epoch 606 (average epoch stats below)\n",
      "2023-04-17 15:12:46 | INFO | train | epoch 606 | loss 2.376 | nll_loss 0.223 | ppl 1.17 | wps 927.3 | ups 1.37 | wpb 678.5 | bsz 2 | num_updates 19383 | lr 2.28743e-05 | gnorm 2.545 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 15647\n",
      "2023-04-17 15:12:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:12:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:12:46 | INFO | fairseq.trainer | begin training epoch 607\n",
      "2023-04-17 15:12:46 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:12:52 | INFO | train_inner | epoch 607:     17 / 32 loss=2.372, nll_loss=0.218, ppl=1.16, wps=973.7, ups=1.45, wpb=672.2, bsz=2, num_updates=19400, lr=2.28679e-05, gnorm=2.442, clip=100, loss_scale=1, train_wall=35, gb_free=13.9, wall=15653\n",
      "2023-04-17 15:12:57 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:12:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:12:58 | INFO | valid | epoch 607 | valid on 'valid' subset | loss 5.952 | nll_loss 4.246 | ppl 18.98 | wps 7140.3 | wpb 290.8 | bsz 1 | num_updates 19415 | best_loss 3.979\n",
      "2023-04-17 15:12:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 607 @ 19415 updates\n",
      "2023-04-17 15:12:58 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:13:10 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:13:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 607 @ 19415 updates, score 5.952) (writing took 12.285690287011676 seconds)\n",
      "2023-04-17 15:13:10 | INFO | fairseq_cli.train | end of epoch 607 (average epoch stats below)\n",
      "2023-04-17 15:13:10 | INFO | train | epoch 607 | loss 2.37 | nll_loss 0.215 | ppl 1.16 | wps 916.1 | ups 1.35 | wpb 678.5 | bsz 2 | num_updates 19415 | lr 2.28623e-05 | gnorm 2.497 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 15670\n",
      "2023-04-17 15:13:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:13:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:13:10 | INFO | fairseq.trainer | begin training epoch 608\n",
      "2023-04-17 15:13:10 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:13:22 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:13:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:13:22 | INFO | valid | epoch 608 | valid on 'valid' subset | loss 6.083 | nll_loss 4.395 | ppl 21.04 | wps 6175.8 | wpb 290.8 | bsz 1 | num_updates 19447 | best_loss 3.979\n",
      "2023-04-17 15:13:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 608 @ 19447 updates\n",
      "2023-04-17 15:13:22 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:13:34 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:13:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 608 @ 19447 updates, score 6.083) (writing took 12.13252718700096 seconds)\n",
      "2023-04-17 15:13:34 | INFO | fairseq_cli.train | end of epoch 608 (average epoch stats below)\n",
      "2023-04-17 15:13:34 | INFO | train | epoch 608 | loss 2.364 | nll_loss 0.208 | ppl 1.15 | wps 894.4 | ups 1.32 | wpb 678.5 | bsz 2 | num_updates 19447 | lr 2.28502e-05 | gnorm 2.375 | clip 100 | loss_scale 1 | train_wall 12 | gb_free 13.9 | wall 15695\n",
      "2023-04-17 15:13:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:13:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:13:34 | INFO | fairseq.trainer | begin training epoch 609\n",
      "2023-04-17 15:13:34 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:13:45 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:13:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:13:46 | INFO | valid | epoch 609 | valid on 'valid' subset | loss 6.076 | nll_loss 4.385 | ppl 20.9 | wps 7283.1 | wpb 290.8 | bsz 1 | num_updates 19479 | best_loss 3.979\n",
      "2023-04-17 15:13:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 609 @ 19479 updates\n",
      "2023-04-17 15:13:46 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:13:58 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:13:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 609 @ 19479 updates, score 6.076) (writing took 12.364574893028475 seconds)\n",
      "2023-04-17 15:13:58 | INFO | fairseq_cli.train | end of epoch 609 (average epoch stats below)\n",
      "2023-04-17 15:13:58 | INFO | train | epoch 609 | loss 2.367 | nll_loss 0.212 | ppl 1.16 | wps 913.3 | ups 1.35 | wpb 678.5 | bsz 2 | num_updates 19479 | lr 2.28381e-05 | gnorm 2.599 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 15718\n",
      "2023-04-17 15:13:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:13:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:13:58 | INFO | fairseq.trainer | begin training epoch 610\n",
      "2023-04-17 15:13:58 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:14:05 | INFO | train_inner | epoch 610:     21 / 32 loss=2.367, nll_loss=0.212, ppl=1.16, wps=914.2, ups=1.37, wpb=668.5, bsz=2, num_updates=19500, lr=2.28302e-05, gnorm=2.498, clip=100, loss_scale=1, train_wall=35, gb_free=13.9, wall=15726\n",
      "2023-04-17 15:14:09 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:14:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:14:09 | INFO | valid | epoch 610 | valid on 'valid' subset | loss 6.017 | nll_loss 4.319 | ppl 19.96 | wps 7188.8 | wpb 290.8 | bsz 1 | num_updates 19511 | best_loss 3.979\n",
      "2023-04-17 15:14:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 610 @ 19511 updates\n",
      "2023-04-17 15:14:09 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:14:21 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:14:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 610 @ 19511 updates, score 6.017) (writing took 12.137396712088957 seconds)\n",
      "2023-04-17 15:14:22 | INFO | fairseq_cli.train | end of epoch 610 (average epoch stats below)\n",
      "2023-04-17 15:14:22 | INFO | train | epoch 610 | loss 2.368 | nll_loss 0.214 | ppl 1.16 | wps 923.2 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 19511 | lr 2.2826e-05 | gnorm 2.385 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 15742\n",
      "2023-04-17 15:14:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:14:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:14:22 | INFO | fairseq.trainer | begin training epoch 611\n",
      "2023-04-17 15:14:22 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:14:33 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:14:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:14:33 | INFO | valid | epoch 611 | valid on 'valid' subset | loss 6.047 | nll_loss 4.367 | ppl 20.63 | wps 7169.3 | wpb 290.8 | bsz 1 | num_updates 19543 | best_loss 3.979\n",
      "2023-04-17 15:14:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 611 @ 19543 updates\n",
      "2023-04-17 15:14:33 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 15:14:45 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:14:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 611 @ 19543 updates, score 6.047) (writing took 12.305691518005915 seconds)\n",
      "2023-04-17 15:14:45 | INFO | fairseq_cli.train | end of epoch 611 (average epoch stats below)\n",
      "2023-04-17 15:14:45 | INFO | train | epoch 611 | loss 2.369 | nll_loss 0.215 | ppl 1.16 | wps 911.4 | ups 1.34 | wpb 678.5 | bsz 2 | num_updates 19543 | lr 2.2814e-05 | gnorm 2.412 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 15766\n",
      "2023-04-17 15:14:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:14:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:14:45 | INFO | fairseq.trainer | begin training epoch 612\n",
      "2023-04-17 15:14:45 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:14:56 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:14:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:14:57 | INFO | valid | epoch 612 | valid on 'valid' subset | loss 6.035 | nll_loss 4.348 | ppl 20.36 | wps 7217.5 | wpb 290.8 | bsz 1 | num_updates 19575 | best_loss 3.979\n",
      "2023-04-17 15:14:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 612 @ 19575 updates\n",
      "2023-04-17 15:14:57 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:15:09 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:15:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 612 @ 19575 updates, score 6.035) (writing took 12.387904881034046 seconds)\n",
      "2023-04-17 15:15:09 | INFO | fairseq_cli.train | end of epoch 612 (average epoch stats below)\n",
      "2023-04-17 15:15:09 | INFO | train | epoch 612 | loss 2.366 | nll_loss 0.212 | ppl 1.16 | wps 916.2 | ups 1.35 | wpb 678.5 | bsz 2 | num_updates 19575 | lr 2.28019e-05 | gnorm 2.462 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 15789\n",
      "2023-04-17 15:15:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:15:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:15:09 | INFO | fairseq.trainer | begin training epoch 613\n",
      "2023-04-17 15:15:09 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:15:18 | INFO | train_inner | epoch 613:     25 / 32 loss=2.368, nll_loss=0.215, ppl=1.16, wps=947.1, ups=1.37, wpb=688.9, bsz=2, num_updates=19600, lr=2.27925e-05, gnorm=2.405, clip=100, loss_scale=1, train_wall=35, gb_free=13.9, wall=15798\n",
      "2023-04-17 15:15:21 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:15:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:15:21 | INFO | valid | epoch 613 | valid on 'valid' subset | loss 6.013 | nll_loss 4.322 | ppl 20 | wps 7178.6 | wpb 290.8 | bsz 1 | num_updates 19607 | best_loss 3.979\n",
      "2023-04-17 15:15:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 613 @ 19607 updates\n",
      "2023-04-17 15:15:21 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:15:33 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:15:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 613 @ 19607 updates, score 6.013) (writing took 12.279370508971624 seconds)\n",
      "2023-04-17 15:15:33 | INFO | fairseq_cli.train | end of epoch 613 (average epoch stats below)\n",
      "2023-04-17 15:15:33 | INFO | train | epoch 613 | loss 2.371 | nll_loss 0.218 | ppl 1.16 | wps 897.1 | ups 1.32 | wpb 678.5 | bsz 2 | num_updates 19607 | lr 2.27898e-05 | gnorm 2.49 | clip 100 | loss_scale 1 | train_wall 12 | gb_free 13.9 | wall 15814\n",
      "2023-04-17 15:15:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:15:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:15:33 | INFO | fairseq.trainer | begin training epoch 614\n",
      "2023-04-17 15:15:33 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:15:45 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:15:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:15:46 | INFO | valid | epoch 614 | valid on 'valid' subset | loss 6.064 | nll_loss 4.373 | ppl 20.72 | wps 7192.3 | wpb 290.8 | bsz 1 | num_updates 19639 | best_loss 3.979\n",
      "2023-04-17 15:15:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 614 @ 19639 updates\n",
      "2023-04-17 15:15:46 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:15:57 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:15:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 614 @ 19639 updates, score 6.064) (writing took 11.940250560059212 seconds)\n",
      "2023-04-17 15:15:58 | INFO | fairseq_cli.train | end of epoch 614 (average epoch stats below)\n",
      "2023-04-17 15:15:58 | INFO | train | epoch 614 | loss 2.369 | nll_loss 0.215 | ppl 1.16 | wps 891.1 | ups 1.31 | wpb 678.5 | bsz 2 | num_updates 19639 | lr 2.27777e-05 | gnorm 2.379 | clip 100 | loss_scale 1 | train_wall 12 | gb_free 13.9 | wall 15838\n",
      "2023-04-17 15:15:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:15:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:15:58 | INFO | fairseq.trainer | begin training epoch 615\n",
      "2023-04-17 15:15:58 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:16:10 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:16:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:16:10 | INFO | valid | epoch 615 | valid on 'valid' subset | loss 5.979 | nll_loss 4.285 | ppl 19.5 | wps 7031.4 | wpb 290.8 | bsz 1 | num_updates 19671 | best_loss 3.979\n",
      "2023-04-17 15:16:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 615 @ 19671 updates\n",
      "2023-04-17 15:16:10 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:16:22 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:16:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 615 @ 19671 updates, score 5.979) (writing took 12.304527153028175 seconds)\n",
      "2023-04-17 15:16:22 | INFO | fairseq_cli.train | end of epoch 615 (average epoch stats below)\n",
      "2023-04-17 15:16:22 | INFO | train | epoch 615 | loss 2.369 | nll_loss 0.215 | ppl 1.16 | wps 877.3 | ups 1.29 | wpb 678.5 | bsz 2 | num_updates 19671 | lr 2.27657e-05 | gnorm 2.407 | clip 100 | loss_scale 1 | train_wall 12 | gb_free 13.9 | wall 15863\n",
      "2023-04-17 15:16:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:16:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:16:22 | INFO | fairseq.trainer | begin training epoch 616\n",
      "2023-04-17 15:16:22 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:16:34 | INFO | train_inner | epoch 616:     29 / 32 loss=2.369, nll_loss=0.215, ppl=1.16, wps=900.5, ups=1.33, wpb=678.9, bsz=2, num_updates=19700, lr=2.27547e-05, gnorm=2.405, clip=100, loss_scale=1, train_wall=38, gb_free=13.7, wall=15874\n",
      "2023-04-17 15:16:35 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:16:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:16:35 | INFO | valid | epoch 616 | valid on 'valid' subset | loss 6.033 | nll_loss 4.34 | ppl 20.25 | wps 7205.8 | wpb 290.8 | bsz 1 | num_updates 19703 | best_loss 3.979\n",
      "2023-04-17 15:16:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 616 @ 19703 updates\n",
      "2023-04-17 15:16:35 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 15:16:47 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:16:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 616 @ 19703 updates, score 6.033) (writing took 12.085013937088661 seconds)\n",
      "2023-04-17 15:16:47 | INFO | fairseq_cli.train | end of epoch 616 (average epoch stats below)\n",
      "2023-04-17 15:16:47 | INFO | train | epoch 616 | loss 2.368 | nll_loss 0.215 | ppl 1.16 | wps 886.2 | ups 1.31 | wpb 678.5 | bsz 2 | num_updates 19703 | lr 2.27536e-05 | gnorm 2.335 | clip 100 | loss_scale 1 | train_wall 12 | gb_free 13.9 | wall 15887\n",
      "2023-04-17 15:16:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:16:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:16:47 | INFO | fairseq.trainer | begin training epoch 617\n",
      "2023-04-17 15:16:47 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:16:59 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:16:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:16:59 | INFO | valid | epoch 617 | valid on 'valid' subset | loss 6.011 | nll_loss 4.316 | ppl 19.92 | wps 7252.2 | wpb 290.8 | bsz 1 | num_updates 19735 | best_loss 3.979\n",
      "2023-04-17 15:16:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 617 @ 19735 updates\n",
      "2023-04-17 15:16:59 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:17:11 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:17:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 617 @ 19735 updates, score 6.011) (writing took 12.223961910931394 seconds)\n",
      "2023-04-17 15:17:12 | INFO | fairseq_cli.train | end of epoch 617 (average epoch stats below)\n",
      "2023-04-17 15:17:12 | INFO | train | epoch 617 | loss 2.373 | nll_loss 0.221 | ppl 1.17 | wps 881.3 | ups 1.3 | wpb 678.5 | bsz 2 | num_updates 19735 | lr 2.27415e-05 | gnorm 2.596 | clip 100 | loss_scale 1 | train_wall 12 | gb_free 13.9 | wall 15912\n",
      "2023-04-17 15:17:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:17:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:17:12 | INFO | fairseq.trainer | begin training epoch 618\n",
      "2023-04-17 15:17:12 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:17:24 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:17:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:17:24 | INFO | valid | epoch 618 | valid on 'valid' subset | loss 5.991 | nll_loss 4.296 | ppl 19.64 | wps 7218.1 | wpb 290.8 | bsz 1 | num_updates 19767 | best_loss 3.979\n",
      "2023-04-17 15:17:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 618 @ 19767 updates\n",
      "2023-04-17 15:17:24 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:17:36 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:17:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 618 @ 19767 updates, score 5.991) (writing took 12.127419997006655 seconds)\n",
      "2023-04-17 15:17:36 | INFO | fairseq_cli.train | end of epoch 618 (average epoch stats below)\n",
      "2023-04-17 15:17:36 | INFO | train | epoch 618 | loss 2.367 | nll_loss 0.214 | ppl 1.16 | wps 885.6 | ups 1.31 | wpb 678.5 | bsz 2 | num_updates 19767 | lr 2.27294e-05 | gnorm 2.454 | clip 100 | loss_scale 1 | train_wall 12 | gb_free 13.9 | wall 15936\n",
      "2023-04-17 15:17:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:17:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:17:36 | INFO | fairseq.trainer | begin training epoch 619\n",
      "2023-04-17 15:17:36 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:17:48 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:17:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:17:49 | INFO | valid | epoch 619 | valid on 'valid' subset | loss 6.057 | nll_loss 4.364 | ppl 20.6 | wps 7262.1 | wpb 290.8 | bsz 1 | num_updates 19799 | best_loss 3.979\n",
      "2023-04-17 15:17:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 619 @ 19799 updates\n",
      "2023-04-17 15:17:49 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:18:00 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:18:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 619 @ 19799 updates, score 6.057) (writing took 12.146210999926552 seconds)\n",
      "2023-04-17 15:18:01 | INFO | fairseq_cli.train | end of epoch 619 (average epoch stats below)\n",
      "2023-04-17 15:18:01 | INFO | train | epoch 619 | loss 2.37 | nll_loss 0.217 | ppl 1.16 | wps 882.5 | ups 1.3 | wpb 678.5 | bsz 2 | num_updates 19799 | lr 2.27174e-05 | gnorm 2.609 | clip 100 | loss_scale 1 | train_wall 12 | gb_free 13.9 | wall 15961\n",
      "2023-04-17 15:18:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:18:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:18:01 | INFO | fairseq.trainer | begin training epoch 620\n",
      "2023-04-17 15:18:01 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:18:01 | INFO | train_inner | epoch 620:      1 / 32 loss=2.37, nll_loss=0.217, ppl=1.16, wps=771.8, ups=1.14, wpb=675.2, bsz=2, num_updates=19800, lr=2.2717e-05, gnorm=2.539, clip=100, loss_scale=1, train_wall=38, gb_free=13.9, wall=15961\n",
      "2023-04-17 15:18:13 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:18:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:18:13 | INFO | valid | epoch 620 | valid on 'valid' subset | loss 6.002 | nll_loss 4.297 | ppl 19.65 | wps 7111.2 | wpb 290.8 | bsz 1 | num_updates 19831 | best_loss 3.979\n",
      "2023-04-17 15:18:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 620 @ 19831 updates\n",
      "2023-04-17 15:18:13 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:18:25 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:18:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 620 @ 19831 updates, score 6.002) (writing took 12.152232764055952 seconds)\n",
      "2023-04-17 15:18:25 | INFO | fairseq_cli.train | end of epoch 620 (average epoch stats below)\n",
      "2023-04-17 15:18:25 | INFO | train | epoch 620 | loss 2.367 | nll_loss 0.214 | ppl 1.16 | wps 878.1 | ups 1.29 | wpb 678.5 | bsz 2 | num_updates 19831 | lr 2.27053e-05 | gnorm 2.332 | clip 100 | loss_scale 1 | train_wall 12 | gb_free 13.9 | wall 15986\n",
      "2023-04-17 15:18:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:18:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:18:25 | INFO | fairseq.trainer | begin training epoch 621\n",
      "2023-04-17 15:18:25 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:18:38 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:18:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:18:38 | INFO | valid | epoch 621 | valid on 'valid' subset | loss 6.057 | nll_loss 4.365 | ppl 20.61 | wps 7211 | wpb 290.8 | bsz 1 | num_updates 19863 | best_loss 3.979\n",
      "2023-04-17 15:18:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 621 @ 19863 updates\n",
      "2023-04-17 15:18:38 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:18:50 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 15:18:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 621 @ 19863 updates, score 6.057) (writing took 12.289655706030317 seconds)\n",
      "2023-04-17 15:18:50 | INFO | fairseq_cli.train | end of epoch 621 (average epoch stats below)\n",
      "2023-04-17 15:18:50 | INFO | train | epoch 621 | loss 2.368 | nll_loss 0.214 | ppl 1.16 | wps 874.1 | ups 1.29 | wpb 678.5 | bsz 2 | num_updates 19863 | lr 2.26932e-05 | gnorm 2.699 | clip 100 | loss_scale 1 | train_wall 12 | gb_free 13.9 | wall 16011\n",
      "2023-04-17 15:18:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:18:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:18:50 | INFO | fairseq.trainer | begin training epoch 622\n",
      "2023-04-17 15:18:50 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:19:02 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:19:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:19:03 | INFO | valid | epoch 622 | valid on 'valid' subset | loss 6.028 | nll_loss 4.316 | ppl 19.91 | wps 7163.7 | wpb 290.8 | bsz 1 | num_updates 19895 | best_loss 3.979\n",
      "2023-04-17 15:19:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 622 @ 19895 updates\n",
      "2023-04-17 15:19:03 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:19:15 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:19:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 622 @ 19895 updates, score 6.028) (writing took 12.136298750061542 seconds)\n",
      "2023-04-17 15:19:15 | INFO | fairseq_cli.train | end of epoch 622 (average epoch stats below)\n",
      "2023-04-17 15:19:15 | INFO | train | epoch 622 | loss 2.367 | nll_loss 0.214 | ppl 1.16 | wps 883.7 | ups 1.3 | wpb 678.5 | bsz 2 | num_updates 19895 | lr 2.26811e-05 | gnorm 2.553 | clip 100 | loss_scale 1 | train_wall 12 | gb_free 13.9 | wall 16035\n",
      "2023-04-17 15:19:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:19:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:19:15 | INFO | fairseq.trainer | begin training epoch 623\n",
      "2023-04-17 15:19:15 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:19:17 | INFO | train_inner | epoch 623:      5 / 32 loss=2.367, nll_loss=0.214, ppl=1.16, wps=900.2, ups=1.32, wpb=681.5, bsz=2, num_updates=19900, lr=2.26792e-05, gnorm=2.531, clip=100, loss_scale=1, train_wall=38, gb_free=13.9, wall=16037\n",
      "2023-04-17 15:19:27 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:19:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:19:27 | INFO | valid | epoch 623 | valid on 'valid' subset | loss 6.092 | nll_loss 4.402 | ppl 21.14 | wps 7070.6 | wpb 290.8 | bsz 1 | num_updates 19927 | best_loss 3.979\n",
      "2023-04-17 15:19:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 623 @ 19927 updates\n",
      "2023-04-17 15:19:27 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:19:39 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:19:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 623 @ 19927 updates, score 6.092) (writing took 12.229003014042974 seconds)\n",
      "2023-04-17 15:19:39 | INFO | fairseq_cli.train | end of epoch 623 (average epoch stats below)\n",
      "2023-04-17 15:19:39 | INFO | train | epoch 623 | loss 2.369 | nll_loss 0.215 | ppl 1.16 | wps 881 | ups 1.3 | wpb 678.5 | bsz 2 | num_updates 19927 | lr 2.26691e-05 | gnorm 2.61 | clip 100 | loss_scale 1 | train_wall 12 | gb_free 13.9 | wall 16060\n",
      "2023-04-17 15:19:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:19:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:19:39 | INFO | fairseq.trainer | begin training epoch 624\n",
      "2023-04-17 15:19:39 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:19:52 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:19:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:19:52 | INFO | valid | epoch 624 | valid on 'valid' subset | loss 6.138 | nll_loss 4.452 | ppl 21.88 | wps 7259.9 | wpb 290.8 | bsz 1 | num_updates 19959 | best_loss 3.979\n",
      "2023-04-17 15:19:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 624 @ 19959 updates\n",
      "2023-04-17 15:19:52 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:20:04 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:20:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 624 @ 19959 updates, score 6.138) (writing took 12.058656023000367 seconds)\n",
      "2023-04-17 15:20:04 | INFO | fairseq_cli.train | end of epoch 624 (average epoch stats below)\n",
      "2023-04-17 15:20:04 | INFO | train | epoch 624 | loss 2.366 | nll_loss 0.213 | ppl 1.16 | wps 884.1 | ups 1.3 | wpb 678.5 | bsz 2 | num_updates 19959 | lr 2.2657e-05 | gnorm 2.489 | clip 100 | loss_scale 1 | train_wall 12 | gb_free 13.9 | wall 16084\n",
      "2023-04-17 15:20:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:20:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:20:04 | INFO | fairseq.trainer | begin training epoch 625\n",
      "2023-04-17 15:20:04 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:20:16 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:20:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:20:17 | INFO | valid | epoch 625 | valid on 'valid' subset | loss 6.085 | nll_loss 4.39 | ppl 20.97 | wps 6895.1 | wpb 290.8 | bsz 1 | num_updates 19991 | best_loss 3.979\n",
      "2023-04-17 15:20:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 625 @ 19991 updates\n",
      "2023-04-17 15:20:17 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:20:29 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:20:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 625 @ 19991 updates, score 6.085) (writing took 12.226021933951415 seconds)\n",
      "2023-04-17 15:20:29 | INFO | fairseq_cli.train | end of epoch 625 (average epoch stats below)\n",
      "2023-04-17 15:20:29 | INFO | train | epoch 625 | loss 2.365 | nll_loss 0.212 | ppl 1.16 | wps 873.1 | ups 1.29 | wpb 678.5 | bsz 2 | num_updates 19991 | lr 2.26449e-05 | gnorm 2.531 | clip 100 | loss_scale 1 | train_wall 12 | gb_free 13.9 | wall 16109\n",
      "2023-04-17 15:20:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:20:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:20:29 | INFO | fairseq.trainer | begin training epoch 626\n",
      "2023-04-17 15:20:29 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:20:32 | INFO | train_inner | epoch 626:      9 / 32 loss=2.366, nll_loss=0.213, ppl=1.16, wps=896.8, ups=1.32, wpb=678.3, bsz=2, num_updates=20000, lr=2.26415e-05, gnorm=2.533, clip=100, loss_scale=1, train_wall=38, gb_free=13.9, wall=16113\n",
      "2023-04-17 15:20:32 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:20:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:20:33 | INFO | valid | epoch 626 | valid on 'valid' subset | loss 6.037 | nll_loss 4.34 | ppl 20.26 | wps 7288.9 | wpb 290.8 | bsz 1 | num_updates 20000 | best_loss 3.979\n",
      "2023-04-17 15:20:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 626 @ 20000 updates\n",
      "2023-04-17 15:20:33 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_626_20000.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 15:20:40 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_626_20000.pt\n",
      "2023-04-17 15:20:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_626_20000.pt (epoch 626 @ 20000 updates, score 6.037) (writing took 14.671132097020745 seconds)\n",
      "2023-04-17 15:20:56 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:20:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:20:56 | INFO | valid | epoch 626 | valid on 'valid' subset | loss 5.965 | nll_loss 4.258 | ppl 19.13 | wps 7217.4 | wpb 290.8 | bsz 1 | num_updates 20023 | best_loss 3.979\n",
      "2023-04-17 15:20:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 626 @ 20023 updates\n",
      "2023-04-17 15:20:56 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:21:08 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:21:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 626 @ 20023 updates, score 5.965) (writing took 12.413416716037318 seconds)\n",
      "2023-04-17 15:21:08 | INFO | fairseq_cli.train | end of epoch 626 (average epoch stats below)\n",
      "2023-04-17 15:21:08 | INFO | train | epoch 626 | loss 2.367 | nll_loss 0.213 | ppl 1.16 | wps 551.9 | ups 0.81 | wpb 678.5 | bsz 2 | num_updates 20023 | lr 2.26328e-05 | gnorm 2.452 | clip 100 | loss_scale 1 | train_wall 12 | gb_free 13.9 | wall 16148\n",
      "2023-04-17 15:21:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:21:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:21:08 | INFO | fairseq.trainer | begin training epoch 627\n",
      "2023-04-17 15:21:08 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:21:19 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:21:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:21:20 | INFO | valid | epoch 627 | valid on 'valid' subset | loss 6.093 | nll_loss 4.4 | ppl 21.11 | wps 7168.3 | wpb 290.8 | bsz 1 | num_updates 20055 | best_loss 3.979\n",
      "2023-04-17 15:21:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 627 @ 20055 updates\n",
      "2023-04-17 15:21:20 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:21:32 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:21:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 627 @ 20055 updates, score 6.093) (writing took 12.18355969991535 seconds)\n",
      "2023-04-17 15:21:32 | INFO | fairseq_cli.train | end of epoch 627 (average epoch stats below)\n",
      "2023-04-17 15:21:32 | INFO | train | epoch 627 | loss 2.363 | nll_loss 0.208 | ppl 1.16 | wps 920.7 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 20055 | lr 2.26208e-05 | gnorm 2.228 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 16172\n",
      "2023-04-17 15:21:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:21:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:21:32 | INFO | fairseq.trainer | begin training epoch 628\n",
      "2023-04-17 15:21:32 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:21:43 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:21:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:21:43 | INFO | valid | epoch 628 | valid on 'valid' subset | loss 6.111 | nll_loss 4.427 | ppl 21.51 | wps 7220.4 | wpb 290.8 | bsz 1 | num_updates 20087 | best_loss 3.979\n",
      "2023-04-17 15:21:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 628 @ 20087 updates\n",
      "2023-04-17 15:21:43 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:21:56 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:21:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 628 @ 20087 updates, score 6.111) (writing took 12.62786703906022 seconds)\n",
      "2023-04-17 15:21:56 | INFO | fairseq_cli.train | end of epoch 628 (average epoch stats below)\n",
      "2023-04-17 15:21:56 | INFO | train | epoch 628 | loss 2.368 | nll_loss 0.215 | ppl 1.16 | wps 904.5 | ups 1.33 | wpb 678.5 | bsz 2 | num_updates 20087 | lr 2.26087e-05 | gnorm 2.455 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 16196\n",
      "2023-04-17 15:21:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:21:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:21:56 | INFO | fairseq.trainer | begin training epoch 629\n",
      "2023-04-17 15:21:56 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:22:00 | INFO | train_inner | epoch 629:     13 / 32 loss=2.366, nll_loss=0.213, ppl=1.16, wps=766.6, ups=1.14, wpb=674.8, bsz=2, num_updates=20100, lr=2.26038e-05, gnorm=2.39, clip=100, loss_scale=1, train_wall=35, gb_free=13.9, wall=16201\n",
      "2023-04-17 15:22:07 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:22:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:22:07 | INFO | valid | epoch 629 | valid on 'valid' subset | loss 6.147 | nll_loss 4.461 | ppl 22.02 | wps 7105.6 | wpb 290.8 | bsz 1 | num_updates 20119 | best_loss 3.979\n",
      "2023-04-17 15:22:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 629 @ 20119 updates\n",
      "2023-04-17 15:22:07 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:22:19 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:22:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 629 @ 20119 updates, score 6.147) (writing took 12.391764810075983 seconds)\n",
      "2023-04-17 15:22:20 | INFO | fairseq_cli.train | end of epoch 629 (average epoch stats below)\n",
      "2023-04-17 15:22:20 | INFO | train | epoch 629 | loss 2.365 | nll_loss 0.212 | ppl 1.16 | wps 912 | ups 1.34 | wpb 678.5 | bsz 2 | num_updates 20119 | lr 2.25966e-05 | gnorm 2.416 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 16220\n",
      "2023-04-17 15:22:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:22:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:22:20 | INFO | fairseq.trainer | begin training epoch 630\n",
      "2023-04-17 15:22:20 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:22:31 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:22:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:22:31 | INFO | valid | epoch 630 | valid on 'valid' subset | loss 6.132 | nll_loss 4.445 | ppl 21.78 | wps 7177.4 | wpb 290.8 | bsz 1 | num_updates 20151 | best_loss 3.979\n",
      "2023-04-17 15:22:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 630 @ 20151 updates\n",
      "2023-04-17 15:22:31 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:22:43 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:22:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 630 @ 20151 updates, score 6.132) (writing took 12.5464780270122 seconds)\n",
      "2023-04-17 15:22:44 | INFO | fairseq_cli.train | end of epoch 630 (average epoch stats below)\n",
      "2023-04-17 15:22:44 | INFO | train | epoch 630 | loss 2.361 | nll_loss 0.207 | ppl 1.15 | wps 903 | ups 1.33 | wpb 678.5 | bsz 2 | num_updates 20151 | lr 2.25845e-05 | gnorm 2.315 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 16244\n",
      "2023-04-17 15:22:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:22:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:22:44 | INFO | fairseq.trainer | begin training epoch 631\n",
      "2023-04-17 15:22:44 | INFO | fairseq_cli.train | Start iterating over samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 15:22:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:22:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:22:55 | INFO | valid | epoch 631 | valid on 'valid' subset | loss 6.115 | nll_loss 4.422 | ppl 21.44 | wps 7230.1 | wpb 290.8 | bsz 1 | num_updates 20183 | best_loss 3.979\n",
      "2023-04-17 15:22:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 631 @ 20183 updates\n",
      "2023-04-17 15:22:55 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:23:07 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:23:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 631 @ 20183 updates, score 6.115) (writing took 12.381249911966734 seconds)\n",
      "2023-04-17 15:23:07 | INFO | fairseq_cli.train | end of epoch 631 (average epoch stats below)\n",
      "2023-04-17 15:23:07 | INFO | train | epoch 631 | loss 2.362 | nll_loss 0.209 | ppl 1.16 | wps 916.6 | ups 1.35 | wpb 678.5 | bsz 2 | num_updates 20183 | lr 2.25725e-05 | gnorm 2.291 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 16268\n",
      "2023-04-17 15:23:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:23:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:23:07 | INFO | fairseq.trainer | begin training epoch 632\n",
      "2023-04-17 15:23:07 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:23:13 | INFO | train_inner | epoch 632:     17 / 32 loss=2.362, nll_loss=0.209, ppl=1.16, wps=944.2, ups=1.37, wpb=688.9, bsz=2, num_updates=20200, lr=2.2566e-05, gnorm=2.346, clip=100, loss_scale=1, train_wall=35, gb_free=13.9, wall=16274\n",
      "2023-04-17 15:23:19 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:23:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:23:19 | INFO | valid | epoch 632 | valid on 'valid' subset | loss 6.123 | nll_loss 4.439 | ppl 21.68 | wps 7262.2 | wpb 290.8 | bsz 1 | num_updates 20215 | best_loss 3.979\n",
      "2023-04-17 15:23:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 632 @ 20215 updates\n",
      "2023-04-17 15:23:19 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:23:31 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:23:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 632 @ 20215 updates, score 6.123) (writing took 12.624241477926262 seconds)\n",
      "2023-04-17 15:23:31 | INFO | fairseq_cli.train | end of epoch 632 (average epoch stats below)\n",
      "2023-04-17 15:23:31 | INFO | train | epoch 632 | loss 2.362 | nll_loss 0.208 | ppl 1.16 | wps 904.7 | ups 1.33 | wpb 678.5 | bsz 2 | num_updates 20215 | lr 2.25604e-05 | gnorm 2.496 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 16292\n",
      "2023-04-17 15:23:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:23:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:23:31 | INFO | fairseq.trainer | begin training epoch 633\n",
      "2023-04-17 15:23:31 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:23:43 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:23:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:23:43 | INFO | valid | epoch 633 | valid on 'valid' subset | loss 6.079 | nll_loss 4.393 | ppl 21.01 | wps 7253.7 | wpb 290.8 | bsz 1 | num_updates 20247 | best_loss 3.979\n",
      "2023-04-17 15:23:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 633 @ 20247 updates\n",
      "2023-04-17 15:23:43 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:23:55 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:23:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 633 @ 20247 updates, score 6.079) (writing took 12.252370006986894 seconds)\n",
      "2023-04-17 15:23:55 | INFO | fairseq_cli.train | end of epoch 633 (average epoch stats below)\n",
      "2023-04-17 15:23:55 | INFO | train | epoch 633 | loss 2.366 | nll_loss 0.214 | ppl 1.16 | wps 919.4 | ups 1.35 | wpb 678.5 | bsz 2 | num_updates 20247 | lr 2.25483e-05 | gnorm 2.501 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 16315\n",
      "2023-04-17 15:23:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:23:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:23:55 | INFO | fairseq.trainer | begin training epoch 634\n",
      "2023-04-17 15:23:55 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:24:06 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:24:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:24:06 | INFO | valid | epoch 634 | valid on 'valid' subset | loss 6.016 | nll_loss 4.319 | ppl 19.96 | wps 7245.9 | wpb 290.8 | bsz 1 | num_updates 20279 | best_loss 3.979\n",
      "2023-04-17 15:24:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 634 @ 20279 updates\n",
      "2023-04-17 15:24:06 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:24:19 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:24:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 634 @ 20279 updates, score 6.016) (writing took 12.47578792204149 seconds)\n",
      "2023-04-17 15:24:19 | INFO | fairseq_cli.train | end of epoch 634 (average epoch stats below)\n",
      "2023-04-17 15:24:19 | INFO | train | epoch 634 | loss 2.363 | nll_loss 0.21 | ppl 1.16 | wps 911.4 | ups 1.34 | wpb 678.5 | bsz 2 | num_updates 20279 | lr 2.25362e-05 | gnorm 2.474 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 16339\n",
      "2023-04-17 15:24:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:24:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:24:19 | INFO | fairseq.trainer | begin training epoch 635\n",
      "2023-04-17 15:24:19 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:24:27 | INFO | train_inner | epoch 635:     21 / 32 loss=2.363, nll_loss=0.21, ppl=1.16, wps=911.3, ups=1.36, wpb=667.9, bsz=2, num_updates=20300, lr=2.25283e-05, gnorm=2.47, clip=100, loss_scale=1, train_wall=35, gb_free=13.9, wall=16347\n",
      "2023-04-17 15:24:31 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:24:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:24:31 | INFO | valid | epoch 635 | valid on 'valid' subset | loss 6.191 | nll_loss 4.524 | ppl 23.01 | wps 6150.3 | wpb 290.8 | bsz 1 | num_updates 20311 | best_loss 3.979\n",
      "2023-04-17 15:24:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 635 @ 20311 updates\n",
      "2023-04-17 15:24:31 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:24:44 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:24:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 635 @ 20311 updates, score 6.191) (writing took 13.006032489938661 seconds)\n",
      "2023-04-17 15:24:44 | INFO | fairseq_cli.train | end of epoch 635 (average epoch stats below)\n",
      "2023-04-17 15:24:44 | INFO | train | epoch 635 | loss 2.361 | nll_loss 0.208 | ppl 1.15 | wps 864.8 | ups 1.27 | wpb 678.5 | bsz 2 | num_updates 20311 | lr 2.25242e-05 | gnorm 2.491 | clip 100 | loss_scale 1 | train_wall 12 | gb_free 13.9 | wall 16364\n",
      "2023-04-17 15:24:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:24:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:24:44 | INFO | fairseq.trainer | begin training epoch 636\n",
      "2023-04-17 15:24:44 | INFO | fairseq_cli.train | Start iterating over samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 15:24:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:24:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:24:55 | INFO | valid | epoch 636 | valid on 'valid' subset | loss 6.124 | nll_loss 4.439 | ppl 21.69 | wps 7203.6 | wpb 290.8 | bsz 1 | num_updates 20343 | best_loss 3.979\n",
      "2023-04-17 15:24:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 636 @ 20343 updates\n",
      "2023-04-17 15:24:55 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:25:08 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:25:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 636 @ 20343 updates, score 6.124) (writing took 12.561322359018959 seconds)\n",
      "2023-04-17 15:25:08 | INFO | fairseq_cli.train | end of epoch 636 (average epoch stats below)\n",
      "2023-04-17 15:25:08 | INFO | train | epoch 636 | loss 2.364 | nll_loss 0.211 | ppl 1.16 | wps 903.7 | ups 1.33 | wpb 678.5 | bsz 2 | num_updates 20343 | lr 2.25121e-05 | gnorm 2.473 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 16388\n",
      "2023-04-17 15:25:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:25:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:25:08 | INFO | fairseq.trainer | begin training epoch 637\n",
      "2023-04-17 15:25:08 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:25:19 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:25:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:25:19 | INFO | valid | epoch 637 | valid on 'valid' subset | loss 6.011 | nll_loss 4.315 | ppl 19.9 | wps 7212.3 | wpb 290.8 | bsz 1 | num_updates 20375 | best_loss 3.979\n",
      "2023-04-17 15:25:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 637 @ 20375 updates\n",
      "2023-04-17 15:25:19 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:25:32 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:25:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 637 @ 20375 updates, score 6.011) (writing took 12.306624340009876 seconds)\n",
      "2023-04-17 15:25:32 | INFO | fairseq_cli.train | end of epoch 637 (average epoch stats below)\n",
      "2023-04-17 15:25:32 | INFO | train | epoch 637 | loss 2.364 | nll_loss 0.212 | ppl 1.16 | wps 911.6 | ups 1.34 | wpb 678.5 | bsz 2 | num_updates 20375 | lr 2.25e-05 | gnorm 2.453 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 16412\n",
      "2023-04-17 15:25:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:25:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:25:32 | INFO | fairseq.trainer | begin training epoch 638\n",
      "2023-04-17 15:25:32 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:25:41 | INFO | train_inner | epoch 638:     25 / 32 loss=2.362, nll_loss=0.21, ppl=1.16, wps=922.6, ups=1.35, wpb=681.8, bsz=2, num_updates=20400, lr=2.24906e-05, gnorm=2.49, clip=100, loss_scale=1, train_wall=35, gb_free=13.9, wall=16421\n",
      "2023-04-17 15:25:43 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:25:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:25:43 | INFO | valid | epoch 638 | valid on 'valid' subset | loss 6.113 | nll_loss 4.423 | ppl 21.46 | wps 7220.1 | wpb 290.8 | bsz 1 | num_updates 20407 | best_loss 3.979\n",
      "2023-04-17 15:25:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 638 @ 20407 updates\n",
      "2023-04-17 15:25:43 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:25:55 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:25:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 638 @ 20407 updates, score 6.113) (writing took 12.131562330992892 seconds)\n",
      "2023-04-17 15:25:55 | INFO | fairseq_cli.train | end of epoch 638 (average epoch stats below)\n",
      "2023-04-17 15:25:55 | INFO | train | epoch 638 | loss 2.362 | nll_loss 0.209 | ppl 1.16 | wps 923.3 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 20407 | lr 2.24879e-05 | gnorm 2.525 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 16436\n",
      "2023-04-17 15:25:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:25:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:25:55 | INFO | fairseq.trainer | begin training epoch 639\n",
      "2023-04-17 15:25:55 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:26:07 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:26:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:26:07 | INFO | valid | epoch 639 | valid on 'valid' subset | loss 6.113 | nll_loss 4.416 | ppl 21.35 | wps 7187.7 | wpb 290.8 | bsz 1 | num_updates 20439 | best_loss 3.979\n",
      "2023-04-17 15:26:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 639 @ 20439 updates\n",
      "2023-04-17 15:26:07 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:26:19 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:26:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 639 @ 20439 updates, score 6.113) (writing took 12.380331114982255 seconds)\n",
      "2023-04-17 15:26:19 | INFO | fairseq_cli.train | end of epoch 639 (average epoch stats below)\n",
      "2023-04-17 15:26:19 | INFO | train | epoch 639 | loss 2.364 | nll_loss 0.212 | ppl 1.16 | wps 910.1 | ups 1.34 | wpb 678.5 | bsz 2 | num_updates 20439 | lr 2.24758e-05 | gnorm 2.419 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 16459\n",
      "2023-04-17 15:26:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:26:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:26:19 | INFO | fairseq.trainer | begin training epoch 640\n",
      "2023-04-17 15:26:19 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:26:30 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:26:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:26:30 | INFO | valid | epoch 640 | valid on 'valid' subset | loss 6.12 | nll_loss 4.44 | ppl 21.71 | wps 7269.5 | wpb 290.8 | bsz 1 | num_updates 20471 | best_loss 3.979\n",
      "2023-04-17 15:26:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 640 @ 20471 updates\n",
      "2023-04-17 15:26:30 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:26:43 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:26:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 640 @ 20471 updates, score 6.12) (writing took 12.576512160012498 seconds)\n",
      "2023-04-17 15:26:43 | INFO | fairseq_cli.train | end of epoch 640 (average epoch stats below)\n",
      "2023-04-17 15:26:43 | INFO | train | epoch 640 | loss 2.362 | nll_loss 0.21 | ppl 1.16 | wps 909.9 | ups 1.34 | wpb 678.5 | bsz 2 | num_updates 20471 | lr 2.24638e-05 | gnorm 2.555 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 16483\n",
      "2023-04-17 15:26:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:26:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:26:43 | INFO | fairseq.trainer | begin training epoch 641\n",
      "2023-04-17 15:26:43 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:26:53 | INFO | train_inner | epoch 641:     29 / 32 loss=2.364, nll_loss=0.211, ppl=1.16, wps=929.8, ups=1.38, wpb=675.4, bsz=2, num_updates=20500, lr=2.24528e-05, gnorm=2.486, clip=100, loss_scale=1, train_wall=34, gb_free=13.9, wall=16494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 15:26:54 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:26:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:26:54 | INFO | valid | epoch 641 | valid on 'valid' subset | loss 6.213 | nll_loss 4.531 | ppl 23.12 | wps 7163.3 | wpb 290.8 | bsz 1 | num_updates 20503 | best_loss 3.979\n",
      "2023-04-17 15:26:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 641 @ 20503 updates\n",
      "2023-04-17 15:26:54 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:27:06 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:27:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 641 @ 20503 updates, score 6.213) (writing took 12.32022137392778 seconds)\n",
      "2023-04-17 15:27:07 | INFO | fairseq_cli.train | end of epoch 641 (average epoch stats below)\n",
      "2023-04-17 15:27:07 | INFO | train | epoch 641 | loss 2.362 | nll_loss 0.211 | ppl 1.16 | wps 914.8 | ups 1.35 | wpb 678.5 | bsz 2 | num_updates 20503 | lr 2.24517e-05 | gnorm 2.472 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 16507\n",
      "2023-04-17 15:27:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:27:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:27:07 | INFO | fairseq.trainer | begin training epoch 642\n",
      "2023-04-17 15:27:07 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:27:18 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:27:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:27:18 | INFO | valid | epoch 642 | valid on 'valid' subset | loss 6.126 | nll_loss 4.439 | ppl 21.69 | wps 7282.8 | wpb 290.8 | bsz 1 | num_updates 20535 | best_loss 3.979\n",
      "2023-04-17 15:27:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 642 @ 20535 updates\n",
      "2023-04-17 15:27:18 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:27:30 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:27:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 642 @ 20535 updates, score 6.126) (writing took 12.5324691060232 seconds)\n",
      "2023-04-17 15:27:31 | INFO | fairseq_cli.train | end of epoch 642 (average epoch stats below)\n",
      "2023-04-17 15:27:31 | INFO | train | epoch 642 | loss 2.364 | nll_loss 0.209 | ppl 1.16 | wps 906.2 | ups 1.34 | wpb 678.5 | bsz 2 | num_updates 20535 | lr 2.24396e-05 | gnorm 2.514 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 16531\n",
      "2023-04-17 15:27:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:27:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:27:31 | INFO | fairseq.trainer | begin training epoch 643\n",
      "2023-04-17 15:27:31 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:27:43 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:27:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:27:43 | INFO | valid | epoch 643 | valid on 'valid' subset | loss 6.118 | nll_loss 4.419 | ppl 21.39 | wps 6087.9 | wpb 290.8 | bsz 1 | num_updates 20567 | best_loss 3.979\n",
      "2023-04-17 15:27:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 643 @ 20567 updates\n",
      "2023-04-17 15:27:43 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:27:55 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:27:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 643 @ 20567 updates, score 6.118) (writing took 12.3234496160876 seconds)\n",
      "2023-04-17 15:27:55 | INFO | fairseq_cli.train | end of epoch 643 (average epoch stats below)\n",
      "2023-04-17 15:27:55 | INFO | train | epoch 643 | loss 2.361 | nll_loss 0.208 | ppl 1.16 | wps 890 | ups 1.31 | wpb 678.5 | bsz 2 | num_updates 20567 | lr 2.24275e-05 | gnorm 2.299 | clip 100 | loss_scale 1 | train_wall 12 | gb_free 13.9 | wall 16555\n",
      "2023-04-17 15:27:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:27:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:27:55 | INFO | fairseq.trainer | begin training epoch 644\n",
      "2023-04-17 15:27:55 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:28:06 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:28:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:28:07 | INFO | valid | epoch 644 | valid on 'valid' subset | loss 6.078 | nll_loss 4.385 | ppl 20.9 | wps 7138.2 | wpb 290.8 | bsz 1 | num_updates 20599 | best_loss 3.979\n",
      "2023-04-17 15:28:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 644 @ 20599 updates\n",
      "2023-04-17 15:28:07 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:28:19 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:28:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 644 @ 20599 updates, score 6.078) (writing took 12.729470208985731 seconds)\n",
      "2023-04-17 15:28:19 | INFO | fairseq_cli.train | end of epoch 644 (average epoch stats below)\n",
      "2023-04-17 15:28:19 | INFO | train | epoch 644 | loss 2.363 | nll_loss 0.21 | ppl 1.16 | wps 898.1 | ups 1.32 | wpb 678.5 | bsz 2 | num_updates 20599 | lr 2.24155e-05 | gnorm 2.406 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 16580\n",
      "2023-04-17 15:28:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:28:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:28:19 | INFO | fairseq.trainer | begin training epoch 645\n",
      "2023-04-17 15:28:19 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:28:20 | INFO | train_inner | epoch 645:      1 / 32 loss=2.363, nll_loss=0.21, ppl=1.16, wps=788.7, ups=1.16, wpb=681.7, bsz=2, num_updates=20600, lr=2.24151e-05, gnorm=2.421, clip=100, loss_scale=1, train_wall=35, gb_free=13.9, wall=16580\n",
      "2023-04-17 15:28:31 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:28:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:28:31 | INFO | valid | epoch 645 | valid on 'valid' subset | loss 6.024 | nll_loss 4.324 | ppl 20.02 | wps 6176.6 | wpb 290.8 | bsz 1 | num_updates 20631 | best_loss 3.979\n",
      "2023-04-17 15:28:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 645 @ 20631 updates\n",
      "2023-04-17 15:28:31 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:28:44 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:28:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 645 @ 20631 updates, score 6.024) (writing took 12.334087106981315 seconds)\n",
      "2023-04-17 15:28:44 | INFO | fairseq_cli.train | end of epoch 645 (average epoch stats below)\n",
      "2023-04-17 15:28:44 | INFO | train | epoch 645 | loss 2.365 | nll_loss 0.213 | ppl 1.16 | wps 887 | ups 1.31 | wpb 678.5 | bsz 2 | num_updates 20631 | lr 2.24034e-05 | gnorm 2.514 | clip 100 | loss_scale 1 | train_wall 12 | gb_free 13.9 | wall 16604\n",
      "2023-04-17 15:28:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:28:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:28:44 | INFO | fairseq.trainer | begin training epoch 646\n",
      "2023-04-17 15:28:44 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:28:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:28:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 15:28:55 | INFO | valid | epoch 646 | valid on 'valid' subset | loss 6.112 | nll_loss 4.425 | ppl 21.48 | wps 7189 | wpb 290.8 | bsz 1 | num_updates 20663 | best_loss 3.979\n",
      "2023-04-17 15:28:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 646 @ 20663 updates\n",
      "2023-04-17 15:28:55 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:29:07 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:29:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 646 @ 20663 updates, score 6.112) (writing took 12.483122254954651 seconds)\n",
      "2023-04-17 15:29:08 | INFO | fairseq_cli.train | end of epoch 646 (average epoch stats below)\n",
      "2023-04-17 15:29:08 | INFO | train | epoch 646 | loss 2.363 | nll_loss 0.209 | ppl 1.16 | wps 910.2 | ups 1.34 | wpb 678.5 | bsz 2 | num_updates 20663 | lr 2.23913e-05 | gnorm 2.277 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 16628\n",
      "2023-04-17 15:29:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:29:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:29:08 | INFO | fairseq.trainer | begin training epoch 647\n",
      "2023-04-17 15:29:08 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:29:19 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:29:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:29:19 | INFO | valid | epoch 647 | valid on 'valid' subset | loss 6.095 | nll_loss 4.412 | ppl 21.29 | wps 7101.8 | wpb 290.8 | bsz 1 | num_updates 20695 | best_loss 3.979\n",
      "2023-04-17 15:29:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 647 @ 20695 updates\n",
      "2023-04-17 15:29:19 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:29:31 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:29:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 647 @ 20695 updates, score 6.095) (writing took 12.45909663499333 seconds)\n",
      "2023-04-17 15:29:31 | INFO | fairseq_cli.train | end of epoch 647 (average epoch stats below)\n",
      "2023-04-17 15:29:31 | INFO | train | epoch 647 | loss 2.362 | nll_loss 0.209 | ppl 1.16 | wps 910.6 | ups 1.34 | wpb 678.5 | bsz 2 | num_updates 20695 | lr 2.23792e-05 | gnorm 2.524 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 16652\n",
      "2023-04-17 15:29:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:29:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:29:31 | INFO | fairseq.trainer | begin training epoch 648\n",
      "2023-04-17 15:29:31 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:29:33 | INFO | train_inner | epoch 648:      5 / 32 loss=2.363, nll_loss=0.211, ppl=1.16, wps=930.8, ups=1.36, wpb=684.8, bsz=2, num_updates=20700, lr=2.23774e-05, gnorm=2.423, clip=100, loss_scale=1, train_wall=35, gb_free=13.9, wall=16654\n",
      "2023-04-17 15:29:43 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:29:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:29:43 | INFO | valid | epoch 648 | valid on 'valid' subset | loss 6.069 | nll_loss 4.373 | ppl 20.72 | wps 7161 | wpb 290.8 | bsz 1 | num_updates 20727 | best_loss 3.979\n",
      "2023-04-17 15:29:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 648 @ 20727 updates\n",
      "2023-04-17 15:29:43 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:29:55 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:29:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 648 @ 20727 updates, score 6.069) (writing took 12.59910442889668 seconds)\n",
      "2023-04-17 15:29:55 | INFO | fairseq_cli.train | end of epoch 648 (average epoch stats below)\n",
      "2023-04-17 15:29:55 | INFO | train | epoch 648 | loss 2.364 | nll_loss 0.212 | ppl 1.16 | wps 905.7 | ups 1.33 | wpb 678.5 | bsz 2 | num_updates 20727 | lr 2.23672e-05 | gnorm 2.553 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 16676\n",
      "2023-04-17 15:29:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:29:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:29:55 | INFO | fairseq.trainer | begin training epoch 649\n",
      "2023-04-17 15:29:55 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:30:07 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:30:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:30:07 | INFO | valid | epoch 649 | valid on 'valid' subset | loss 6.092 | nll_loss 4.399 | ppl 21.1 | wps 7049.8 | wpb 290.8 | bsz 1 | num_updates 20759 | best_loss 3.979\n",
      "2023-04-17 15:30:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 649 @ 20759 updates\n",
      "2023-04-17 15:30:07 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:30:19 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:30:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 649 @ 20759 updates, score 6.092) (writing took 12.435652718995698 seconds)\n",
      "2023-04-17 15:30:19 | INFO | fairseq_cli.train | end of epoch 649 (average epoch stats below)\n",
      "2023-04-17 15:30:19 | INFO | train | epoch 649 | loss 2.362 | nll_loss 0.208 | ppl 1.16 | wps 905.6 | ups 1.33 | wpb 678.5 | bsz 2 | num_updates 20759 | lr 2.23551e-05 | gnorm 2.283 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 16700\n",
      "2023-04-17 15:30:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:30:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:30:19 | INFO | fairseq.trainer | begin training epoch 650\n",
      "2023-04-17 15:30:19 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:30:31 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:30:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:30:31 | INFO | valid | epoch 650 | valid on 'valid' subset | loss 6.049 | nll_loss 4.356 | ppl 20.47 | wps 7195.9 | wpb 290.8 | bsz 1 | num_updates 20791 | best_loss 3.979\n",
      "2023-04-17 15:30:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 650 @ 20791 updates\n",
      "2023-04-17 15:30:31 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:30:44 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:30:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 650 @ 20791 updates, score 6.049) (writing took 13.43157766200602 seconds)\n",
      "2023-04-17 15:30:44 | INFO | fairseq_cli.train | end of epoch 650 (average epoch stats below)\n",
      "2023-04-17 15:30:44 | INFO | train | epoch 650 | loss 2.362 | nll_loss 0.21 | ppl 1.16 | wps 876 | ups 1.29 | wpb 678.5 | bsz 2 | num_updates 20791 | lr 2.2343e-05 | gnorm 2.439 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 16724\n",
      "2023-04-17 15:30:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:30:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:30:44 | INFO | fairseq.trainer | begin training epoch 651\n",
      "2023-04-17 15:30:44 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:30:47 | INFO | train_inner | epoch 651:      9 / 32 loss=2.363, nll_loss=0.21, ppl=1.16, wps=902.1, ups=1.35, wpb=668.7, bsz=2, num_updates=20800, lr=2.23396e-05, gnorm=2.455, clip=100, loss_scale=1, train_wall=35, gb_free=13.9, wall=16728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 15:30:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:30:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:30:56 | INFO | valid | epoch 651 | valid on 'valid' subset | loss 6.158 | nll_loss 4.473 | ppl 22.22 | wps 7302.9 | wpb 290.8 | bsz 1 | num_updates 20823 | best_loss 3.979\n",
      "2023-04-17 15:30:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 651 @ 20823 updates\n",
      "2023-04-17 15:30:56 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:31:08 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:31:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 651 @ 20823 updates, score 6.158) (writing took 13.010397756937891 seconds)\n",
      "2023-04-17 15:31:09 | INFO | fairseq_cli.train | end of epoch 651 (average epoch stats below)\n",
      "2023-04-17 15:31:09 | INFO | train | epoch 651 | loss 2.362 | nll_loss 0.21 | ppl 1.16 | wps 891.8 | ups 1.31 | wpb 678.5 | bsz 2 | num_updates 20823 | lr 2.23309e-05 | gnorm 2.576 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 16749\n",
      "2023-04-17 15:31:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:31:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:31:09 | INFO | fairseq.trainer | begin training epoch 652\n",
      "2023-04-17 15:31:09 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:31:20 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:31:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:31:20 | INFO | valid | epoch 652 | valid on 'valid' subset | loss 6.151 | nll_loss 4.472 | ppl 22.19 | wps 7230.2 | wpb 290.8 | bsz 1 | num_updates 20855 | best_loss 3.979\n",
      "2023-04-17 15:31:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 652 @ 20855 updates\n",
      "2023-04-17 15:31:20 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:31:32 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:31:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 652 @ 20855 updates, score 6.151) (writing took 12.578246394055896 seconds)\n",
      "2023-04-17 15:31:32 | INFO | fairseq_cli.train | end of epoch 652 (average epoch stats below)\n",
      "2023-04-17 15:31:32 | INFO | train | epoch 652 | loss 2.359 | nll_loss 0.205 | ppl 1.15 | wps 908.7 | ups 1.34 | wpb 678.5 | bsz 2 | num_updates 20855 | lr 2.23189e-05 | gnorm 2.159 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 16773\n",
      "2023-04-17 15:31:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:31:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:31:32 | INFO | fairseq.trainer | begin training epoch 653\n",
      "2023-04-17 15:31:32 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:31:44 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:31:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:31:44 | INFO | valid | epoch 653 | valid on 'valid' subset | loss 6.134 | nll_loss 4.452 | ppl 21.88 | wps 7224.9 | wpb 290.8 | bsz 1 | num_updates 20887 | best_loss 3.979\n",
      "2023-04-17 15:31:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 653 @ 20887 updates\n",
      "2023-04-17 15:31:44 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:31:56 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:31:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 653 @ 20887 updates, score 6.134) (writing took 12.282138159964234 seconds)\n",
      "2023-04-17 15:31:56 | INFO | fairseq_cli.train | end of epoch 653 (average epoch stats below)\n",
      "2023-04-17 15:31:56 | INFO | train | epoch 653 | loss 2.361 | nll_loss 0.207 | ppl 1.15 | wps 920 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 20887 | lr 2.23068e-05 | gnorm 2.464 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 16796\n",
      "2023-04-17 15:31:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:31:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:31:56 | INFO | fairseq.trainer | begin training epoch 654\n",
      "2023-04-17 15:31:56 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:32:01 | INFO | train_inner | epoch 654:     13 / 32 loss=2.361, nll_loss=0.208, ppl=1.15, wps=918.3, ups=1.37, wpb=672.7, bsz=2, num_updates=20900, lr=2.23019e-05, gnorm=2.383, clip=100, loss_scale=1, train_wall=34, gb_free=13.9, wall=16801\n",
      "2023-04-17 15:32:07 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:32:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:32:07 | INFO | valid | epoch 654 | valid on 'valid' subset | loss 6.121 | nll_loss 4.437 | ppl 21.67 | wps 7234.1 | wpb 290.8 | bsz 1 | num_updates 20919 | best_loss 3.979\n",
      "2023-04-17 15:32:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 654 @ 20919 updates\n",
      "2023-04-17 15:32:07 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:32:20 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:32:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 654 @ 20919 updates, score 6.121) (writing took 12.639547480968758 seconds)\n",
      "2023-04-17 15:32:20 | INFO | fairseq_cli.train | end of epoch 654 (average epoch stats below)\n",
      "2023-04-17 15:32:20 | INFO | train | epoch 654 | loss 2.359 | nll_loss 0.207 | ppl 1.15 | wps 905 | ups 1.33 | wpb 678.5 | bsz 2 | num_updates 20919 | lr 2.22947e-05 | gnorm 2.224 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 16820\n",
      "2023-04-17 15:32:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:32:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:32:20 | INFO | fairseq.trainer | begin training epoch 655\n",
      "2023-04-17 15:32:20 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:32:31 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:32:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:32:31 | INFO | valid | epoch 655 | valid on 'valid' subset | loss 6.149 | nll_loss 4.474 | ppl 22.22 | wps 7156.2 | wpb 290.8 | bsz 1 | num_updates 20951 | best_loss 3.979\n",
      "2023-04-17 15:32:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 655 @ 20951 updates\n",
      "2023-04-17 15:32:31 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:32:44 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:32:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 655 @ 20951 updates, score 6.149) (writing took 12.404738320037723 seconds)\n",
      "2023-04-17 15:32:44 | INFO | fairseq_cli.train | end of epoch 655 (average epoch stats below)\n",
      "2023-04-17 15:32:44 | INFO | train | epoch 655 | loss 2.357 | nll_loss 0.204 | ppl 1.15 | wps 912.1 | ups 1.34 | wpb 678.5 | bsz 2 | num_updates 20951 | lr 2.22826e-05 | gnorm 2.265 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 16844\n",
      "2023-04-17 15:32:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:32:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:32:44 | INFO | fairseq.trainer | begin training epoch 656\n",
      "2023-04-17 15:32:44 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:32:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:32:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 15:32:55 | INFO | valid | epoch 656 | valid on 'valid' subset | loss 6.163 | nll_loss 4.481 | ppl 22.34 | wps 6969.2 | wpb 290.8 | bsz 1 | num_updates 20983 | best_loss 3.979\n",
      "2023-04-17 15:32:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 656 @ 20983 updates\n",
      "2023-04-17 15:32:55 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:33:08 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:33:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 656 @ 20983 updates, score 6.163) (writing took 12.623451501014642 seconds)\n",
      "2023-04-17 15:33:08 | INFO | fairseq_cli.train | end of epoch 656 (average epoch stats below)\n",
      "2023-04-17 15:33:08 | INFO | train | epoch 656 | loss 2.359 | nll_loss 0.206 | ppl 1.15 | wps 903.4 | ups 1.33 | wpb 678.5 | bsz 2 | num_updates 20983 | lr 2.22706e-05 | gnorm 2.252 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 16868\n",
      "2023-04-17 15:33:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:33:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:33:08 | INFO | fairseq.trainer | begin training epoch 657\n",
      "2023-04-17 15:33:08 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:33:14 | INFO | train_inner | epoch 657:     17 / 32 loss=2.359, nll_loss=0.207, ppl=1.15, wps=941.6, ups=1.37, wpb=689.6, bsz=2, num_updates=21000, lr=2.22642e-05, gnorm=2.291, clip=100, loss_scale=1, train_wall=34, gb_free=13.9, wall=16874\n",
      "2023-04-17 15:33:19 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:33:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:33:19 | INFO | valid | epoch 657 | valid on 'valid' subset | loss 6.111 | nll_loss 4.425 | ppl 21.48 | wps 7121.9 | wpb 290.8 | bsz 1 | num_updates 21015 | best_loss 3.979\n",
      "2023-04-17 15:33:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 657 @ 21015 updates\n",
      "2023-04-17 15:33:19 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:33:31 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:33:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 657 @ 21015 updates, score 6.111) (writing took 12.356334354961291 seconds)\n",
      "2023-04-17 15:33:32 | INFO | fairseq_cli.train | end of epoch 657 (average epoch stats below)\n",
      "2023-04-17 15:33:32 | INFO | train | epoch 657 | loss 2.369 | nll_loss 0.218 | ppl 1.16 | wps 916.7 | ups 1.35 | wpb 678.5 | bsz 2 | num_updates 21015 | lr 2.22585e-05 | gnorm 2.631 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 16892\n",
      "2023-04-17 15:33:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:33:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:33:32 | INFO | fairseq.trainer | begin training epoch 658\n",
      "2023-04-17 15:33:32 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:33:43 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:33:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:33:43 | INFO | valid | epoch 658 | valid on 'valid' subset | loss 6.093 | nll_loss 4.403 | ppl 21.16 | wps 7119.5 | wpb 290.8 | bsz 1 | num_updates 21047 | best_loss 3.979\n",
      "2023-04-17 15:33:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 658 @ 21047 updates\n",
      "2023-04-17 15:33:43 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:33:55 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:33:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 658 @ 21047 updates, score 6.093) (writing took 12.58845919102896 seconds)\n",
      "2023-04-17 15:33:56 | INFO | fairseq_cli.train | end of epoch 658 (average epoch stats below)\n",
      "2023-04-17 15:33:56 | INFO | train | epoch 658 | loss 2.36 | nll_loss 0.208 | ppl 1.16 | wps 903.2 | ups 1.33 | wpb 678.5 | bsz 2 | num_updates 21047 | lr 2.22464e-05 | gnorm 2.422 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 16916\n",
      "2023-04-17 15:33:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:33:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:33:56 | INFO | fairseq.trainer | begin training epoch 659\n",
      "2023-04-17 15:33:56 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:34:07 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:34:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:34:07 | INFO | valid | epoch 659 | valid on 'valid' subset | loss 6.216 | nll_loss 4.533 | ppl 23.15 | wps 7149.4 | wpb 290.8 | bsz 1 | num_updates 21079 | best_loss 3.979\n",
      "2023-04-17 15:34:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 659 @ 21079 updates\n",
      "2023-04-17 15:34:07 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:34:19 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:34:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 659 @ 21079 updates, score 6.216) (writing took 12.30244036798831 seconds)\n",
      "2023-04-17 15:34:19 | INFO | fairseq_cli.train | end of epoch 659 (average epoch stats below)\n",
      "2023-04-17 15:34:19 | INFO | train | epoch 659 | loss 2.357 | nll_loss 0.204 | ppl 1.15 | wps 916.7 | ups 1.35 | wpb 678.5 | bsz 2 | num_updates 21079 | lr 2.22343e-05 | gnorm 2.31 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 16940\n",
      "2023-04-17 15:34:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:34:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:34:19 | INFO | fairseq.trainer | begin training epoch 660\n",
      "2023-04-17 15:34:19 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:34:27 | INFO | train_inner | epoch 660:     21 / 32 loss=2.36, nll_loss=0.208, ppl=1.16, wps=930.9, ups=1.37, wpb=678.6, bsz=2, num_updates=21100, lr=2.22264e-05, gnorm=2.371, clip=100, loss_scale=1, train_wall=35, gb_free=13.9, wall=16947\n",
      "2023-04-17 15:34:30 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:34:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:34:31 | INFO | valid | epoch 660 | valid on 'valid' subset | loss 6.082 | nll_loss 4.389 | ppl 20.95 | wps 7007.3 | wpb 290.8 | bsz 1 | num_updates 21111 | best_loss 3.979\n",
      "2023-04-17 15:34:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 660 @ 21111 updates\n",
      "2023-04-17 15:34:31 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:34:43 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:34:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 660 @ 21111 updates, score 6.082) (writing took 12.598069307976402 seconds)\n",
      "2023-04-17 15:34:43 | INFO | fairseq_cli.train | end of epoch 660 (average epoch stats below)\n",
      "2023-04-17 15:34:43 | INFO | train | epoch 660 | loss 2.358 | nll_loss 0.207 | ppl 1.15 | wps 903.6 | ups 1.33 | wpb 678.5 | bsz 2 | num_updates 21111 | lr 2.22223e-05 | gnorm 2.251 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 16964\n",
      "2023-04-17 15:34:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:34:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:34:43 | INFO | fairseq.trainer | begin training epoch 661\n",
      "2023-04-17 15:34:43 | INFO | fairseq_cli.train | Start iterating over samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 15:34:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:34:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:34:55 | INFO | valid | epoch 661 | valid on 'valid' subset | loss 6.116 | nll_loss 4.44 | ppl 21.7 | wps 7091.1 | wpb 290.8 | bsz 1 | num_updates 21143 | best_loss 3.979\n",
      "2023-04-17 15:34:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 661 @ 21143 updates\n",
      "2023-04-17 15:34:55 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:35:07 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:35:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 661 @ 21143 updates, score 6.116) (writing took 12.634794266079552 seconds)\n",
      "2023-04-17 15:35:07 | INFO | fairseq_cli.train | end of epoch 661 (average epoch stats below)\n",
      "2023-04-17 15:35:07 | INFO | train | epoch 661 | loss 2.354 | nll_loss 0.201 | ppl 1.15 | wps 902.9 | ups 1.33 | wpb 678.5 | bsz 2 | num_updates 21143 | lr 2.22102e-05 | gnorm 2.119 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 16988\n",
      "2023-04-17 15:35:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:35:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:35:07 | INFO | fairseq.trainer | begin training epoch 662\n",
      "2023-04-17 15:35:07 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:35:19 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:35:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:35:19 | INFO | valid | epoch 662 | valid on 'valid' subset | loss 6.069 | nll_loss 4.381 | ppl 20.83 | wps 7072.4 | wpb 290.8 | bsz 1 | num_updates 21175 | best_loss 3.979\n",
      "2023-04-17 15:35:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 662 @ 21175 updates\n",
      "2023-04-17 15:35:19 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:35:31 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:35:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 662 @ 21175 updates, score 6.069) (writing took 12.633759810007177 seconds)\n",
      "2023-04-17 15:35:31 | INFO | fairseq_cli.train | end of epoch 662 (average epoch stats below)\n",
      "2023-04-17 15:35:31 | INFO | train | epoch 662 | loss 2.359 | nll_loss 0.207 | ppl 1.15 | wps 900.9 | ups 1.33 | wpb 678.5 | bsz 2 | num_updates 21175 | lr 2.21981e-05 | gnorm 2.428 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 17012\n",
      "2023-04-17 15:35:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:35:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:35:31 | INFO | fairseq.trainer | begin training epoch 663\n",
      "2023-04-17 15:35:31 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:35:40 | INFO | train_inner | epoch 663:     25 / 32 loss=2.358, nll_loss=0.206, ppl=1.15, wps=929.5, ups=1.36, wpb=683.9, bsz=2, num_updates=21200, lr=2.21887e-05, gnorm=2.301, clip=100, loss_scale=1, train_wall=35, gb_free=13.9, wall=17021\n",
      "2023-04-17 15:35:43 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:35:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:35:43 | INFO | valid | epoch 663 | valid on 'valid' subset | loss 6.065 | nll_loss 4.377 | ppl 20.78 | wps 7237.9 | wpb 290.8 | bsz 1 | num_updates 21207 | best_loss 3.979\n",
      "2023-04-17 15:35:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 663 @ 21207 updates\n",
      "2023-04-17 15:35:43 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:35:55 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:35:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 663 @ 21207 updates, score 6.065) (writing took 12.258311879006214 seconds)\n",
      "2023-04-17 15:35:55 | INFO | fairseq_cli.train | end of epoch 663 (average epoch stats below)\n",
      "2023-04-17 15:35:55 | INFO | train | epoch 663 | loss 2.362 | nll_loss 0.21 | ppl 1.16 | wps 918.1 | ups 1.35 | wpb 678.5 | bsz 2 | num_updates 21207 | lr 2.2186e-05 | gnorm 2.459 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 17035\n",
      "2023-04-17 15:35:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:35:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:35:55 | INFO | fairseq.trainer | begin training epoch 664\n",
      "2023-04-17 15:35:55 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:36:06 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:36:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:36:07 | INFO | valid | epoch 664 | valid on 'valid' subset | loss 6.163 | nll_loss 4.478 | ppl 22.29 | wps 7242.8 | wpb 290.8 | bsz 1 | num_updates 21239 | best_loss 3.979\n",
      "2023-04-17 15:36:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 664 @ 21239 updates\n",
      "2023-04-17 15:36:07 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:36:19 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:36:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 664 @ 21239 updates, score 6.163) (writing took 12.66429633507505 seconds)\n",
      "2023-04-17 15:36:19 | INFO | fairseq_cli.train | end of epoch 664 (average epoch stats below)\n",
      "2023-04-17 15:36:19 | INFO | train | epoch 664 | loss 2.361 | nll_loss 0.209 | ppl 1.16 | wps 900.6 | ups 1.33 | wpb 678.5 | bsz 2 | num_updates 21239 | lr 2.2174e-05 | gnorm 2.3 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 17059\n",
      "2023-04-17 15:36:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:36:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:36:19 | INFO | fairseq.trainer | begin training epoch 665\n",
      "2023-04-17 15:36:19 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:36:30 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:36:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:36:31 | INFO | valid | epoch 665 | valid on 'valid' subset | loss 6.131 | nll_loss 4.447 | ppl 21.81 | wps 7213 | wpb 290.8 | bsz 1 | num_updates 21271 | best_loss 3.979\n",
      "2023-04-17 15:36:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 665 @ 21271 updates\n",
      "2023-04-17 15:36:31 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:36:43 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:36:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 665 @ 21271 updates, score 6.131) (writing took 12.277814958943054 seconds)\n",
      "2023-04-17 15:36:43 | INFO | fairseq_cli.train | end of epoch 665 (average epoch stats below)\n",
      "2023-04-17 15:36:43 | INFO | train | epoch 665 | loss 2.359 | nll_loss 0.206 | ppl 1.15 | wps 918.3 | ups 1.35 | wpb 678.5 | bsz 2 | num_updates 21271 | lr 2.21619e-05 | gnorm 2.475 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 17083\n",
      "2023-04-17 15:36:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:36:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:36:43 | INFO | fairseq.trainer | begin training epoch 666\n",
      "2023-04-17 15:36:43 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:36:53 | INFO | train_inner | epoch 666:     29 / 32 loss=2.359, nll_loss=0.207, ppl=1.15, wps=923, ups=1.38, wpb=671.2, bsz=2, num_updates=21300, lr=2.21509e-05, gnorm=2.395, clip=100, loss_scale=1, train_wall=34, gb_free=13.9, wall=17093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 15:36:54 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:36:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:36:54 | INFO | valid | epoch 666 | valid on 'valid' subset | loss 6.13 | nll_loss 4.447 | ppl 21.81 | wps 7151.4 | wpb 290.8 | bsz 1 | num_updates 21303 | best_loss 3.979\n",
      "2023-04-17 15:36:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 666 @ 21303 updates\n",
      "2023-04-17 15:36:54 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:37:07 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:37:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 666 @ 21303 updates, score 6.13) (writing took 12.53352277702652 seconds)\n",
      "2023-04-17 15:37:07 | INFO | fairseq_cli.train | end of epoch 666 (average epoch stats below)\n",
      "2023-04-17 15:37:07 | INFO | train | epoch 666 | loss 2.358 | nll_loss 0.205 | ppl 1.15 | wps 909.1 | ups 1.34 | wpb 678.5 | bsz 2 | num_updates 21303 | lr 2.21498e-05 | gnorm 2.272 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 17107\n",
      "2023-04-17 15:37:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:37:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:37:07 | INFO | fairseq.trainer | begin training epoch 667\n",
      "2023-04-17 15:37:07 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:37:18 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:37:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:37:18 | INFO | valid | epoch 667 | valid on 'valid' subset | loss 6.139 | nll_loss 4.45 | ppl 21.85 | wps 7159 | wpb 290.8 | bsz 1 | num_updates 21335 | best_loss 3.979\n",
      "2023-04-17 15:37:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 667 @ 21335 updates\n",
      "2023-04-17 15:37:18 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:37:30 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:37:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 667 @ 21335 updates, score 6.139) (writing took 12.470845640054904 seconds)\n",
      "2023-04-17 15:37:31 | INFO | fairseq_cli.train | end of epoch 667 (average epoch stats below)\n",
      "2023-04-17 15:37:31 | INFO | train | epoch 667 | loss 2.36 | nll_loss 0.208 | ppl 1.16 | wps 909.7 | ups 1.34 | wpb 678.5 | bsz 2 | num_updates 21335 | lr 2.21377e-05 | gnorm 2.507 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 17131\n",
      "2023-04-17 15:37:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:37:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:37:31 | INFO | fairseq.trainer | begin training epoch 668\n",
      "2023-04-17 15:37:31 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:37:42 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:37:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:37:42 | INFO | valid | epoch 668 | valid on 'valid' subset | loss 6.13 | nll_loss 4.443 | ppl 21.74 | wps 7060.1 | wpb 290.8 | bsz 1 | num_updates 21367 | best_loss 3.979\n",
      "2023-04-17 15:37:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 668 @ 21367 updates\n",
      "2023-04-17 15:37:42 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:37:55 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:37:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 668 @ 21367 updates, score 6.13) (writing took 12.43815631698817 seconds)\n",
      "2023-04-17 15:37:55 | INFO | fairseq_cli.train | end of epoch 668 (average epoch stats below)\n",
      "2023-04-17 15:37:55 | INFO | train | epoch 668 | loss 2.36 | nll_loss 0.208 | ppl 1.16 | wps 895.8 | ups 1.32 | wpb 678.5 | bsz 2 | num_updates 21367 | lr 2.21257e-05 | gnorm 2.422 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 17155\n",
      "2023-04-17 15:37:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:37:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:37:55 | INFO | fairseq.trainer | begin training epoch 669\n",
      "2023-04-17 15:37:55 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:38:06 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:38:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:38:06 | INFO | valid | epoch 669 | valid on 'valid' subset | loss 6.104 | nll_loss 4.409 | ppl 21.25 | wps 7233.2 | wpb 290.8 | bsz 1 | num_updates 21399 | best_loss 3.979\n",
      "2023-04-17 15:38:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 669 @ 21399 updates\n",
      "2023-04-17 15:38:06 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:38:18 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:38:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 669 @ 21399 updates, score 6.104) (writing took 12.247913260012865 seconds)\n",
      "2023-04-17 15:38:18 | INFO | fairseq_cli.train | end of epoch 669 (average epoch stats below)\n",
      "2023-04-17 15:38:18 | INFO | train | epoch 669 | loss 2.358 | nll_loss 0.206 | ppl 1.15 | wps 919.4 | ups 1.35 | wpb 678.5 | bsz 2 | num_updates 21399 | lr 2.21136e-05 | gnorm 2.592 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 17179\n",
      "2023-04-17 15:38:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:38:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:38:18 | INFO | fairseq.trainer | begin training epoch 670\n",
      "2023-04-17 15:38:18 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:38:19 | INFO | train_inner | epoch 670:      1 / 32 loss=2.359, nll_loss=0.207, ppl=1.15, wps=786.1, ups=1.17, wpb=674.5, bsz=2, num_updates=21400, lr=2.21132e-05, gnorm=2.494, clip=100, loss_scale=1, train_wall=35, gb_free=13.9, wall=17179\n",
      "2023-04-17 15:38:30 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:38:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:38:30 | INFO | valid | epoch 670 | valid on 'valid' subset | loss 6.122 | nll_loss 4.441 | ppl 21.72 | wps 7035.2 | wpb 290.8 | bsz 1 | num_updates 21431 | best_loss 3.979\n",
      "2023-04-17 15:38:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 670 @ 21431 updates\n",
      "2023-04-17 15:38:30 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:38:42 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:38:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 670 @ 21431 updates, score 6.122) (writing took 12.565492738038301 seconds)\n",
      "2023-04-17 15:38:42 | INFO | fairseq_cli.train | end of epoch 670 (average epoch stats below)\n",
      "2023-04-17 15:38:42 | INFO | train | epoch 670 | loss 2.357 | nll_loss 0.205 | ppl 1.15 | wps 906.1 | ups 1.34 | wpb 678.5 | bsz 2 | num_updates 21431 | lr 2.21015e-05 | gnorm 2.258 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 17203\n",
      "2023-04-17 15:38:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:38:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:38:42 | INFO | fairseq.trainer | begin training epoch 671\n",
      "2023-04-17 15:38:42 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:38:54 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:38:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 15:38:54 | INFO | valid | epoch 671 | valid on 'valid' subset | loss 6.142 | nll_loss 4.454 | ppl 21.92 | wps 7028.7 | wpb 290.8 | bsz 1 | num_updates 21463 | best_loss 3.979\n",
      "2023-04-17 15:38:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 671 @ 21463 updates\n",
      "2023-04-17 15:38:54 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:39:06 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:39:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 671 @ 21463 updates, score 6.142) (writing took 12.209756283089519 seconds)\n",
      "2023-04-17 15:39:06 | INFO | fairseq_cli.train | end of epoch 671 (average epoch stats below)\n",
      "2023-04-17 15:39:06 | INFO | train | epoch 671 | loss 2.357 | nll_loss 0.205 | ppl 1.15 | wps 921.2 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 21463 | lr 2.20894e-05 | gnorm 2.402 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 17226\n",
      "2023-04-17 15:39:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:39:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:39:06 | INFO | fairseq.trainer | begin training epoch 672\n",
      "2023-04-17 15:39:06 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:39:17 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:39:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:39:17 | INFO | valid | epoch 672 | valid on 'valid' subset | loss 6.079 | nll_loss 4.393 | ppl 21.01 | wps 7165.8 | wpb 290.8 | bsz 1 | num_updates 21495 | best_loss 3.979\n",
      "2023-04-17 15:39:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 672 @ 21495 updates\n",
      "2023-04-17 15:39:17 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:39:30 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:39:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 672 @ 21495 updates, score 6.079) (writing took 12.751461214968003 seconds)\n",
      "2023-04-17 15:39:30 | INFO | fairseq_cli.train | end of epoch 672 (average epoch stats below)\n",
      "2023-04-17 15:39:30 | INFO | train | epoch 672 | loss 2.359 | nll_loss 0.207 | ppl 1.15 | wps 900.6 | ups 1.33 | wpb 678.5 | bsz 2 | num_updates 21495 | lr 2.20774e-05 | gnorm 2.316 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 17250\n",
      "2023-04-17 15:39:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:39:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:39:30 | INFO | fairseq.trainer | begin training epoch 673\n",
      "2023-04-17 15:39:30 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:39:32 | INFO | train_inner | epoch 673:      5 / 32 loss=2.357, nll_loss=0.205, ppl=1.15, wps=937, ups=1.37, wpb=684.5, bsz=2, num_updates=21500, lr=2.20755e-05, gnorm=2.284, clip=100, loss_scale=1, train_wall=34, gb_free=13.9, wall=17252\n",
      "2023-04-17 15:39:41 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:39:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:39:41 | INFO | valid | epoch 673 | valid on 'valid' subset | loss 6.154 | nll_loss 4.469 | ppl 22.15 | wps 7130 | wpb 290.8 | bsz 1 | num_updates 21527 | best_loss 3.979\n",
      "2023-04-17 15:39:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 673 @ 21527 updates\n",
      "2023-04-17 15:39:41 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:39:54 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:39:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 673 @ 21527 updates, score 6.154) (writing took 12.471592597896233 seconds)\n",
      "2023-04-17 15:39:54 | INFO | fairseq_cli.train | end of epoch 673 (average epoch stats below)\n",
      "2023-04-17 15:39:54 | INFO | train | epoch 673 | loss 2.356 | nll_loss 0.203 | ppl 1.15 | wps 911.4 | ups 1.34 | wpb 678.5 | bsz 2 | num_updates 21527 | lr 2.20653e-05 | gnorm 2.162 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 17274\n",
      "2023-04-17 15:39:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:39:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:39:54 | INFO | fairseq.trainer | begin training epoch 674\n",
      "2023-04-17 15:39:54 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:40:05 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:40:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:40:05 | INFO | valid | epoch 674 | valid on 'valid' subset | loss 6.176 | nll_loss 4.501 | ppl 22.64 | wps 7190 | wpb 290.8 | bsz 1 | num_updates 21559 | best_loss 3.979\n",
      "2023-04-17 15:40:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 674 @ 21559 updates\n",
      "2023-04-17 15:40:05 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:40:18 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:40:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 674 @ 21559 updates, score 6.176) (writing took 12.68929011002183 seconds)\n",
      "2023-04-17 15:40:18 | INFO | fairseq_cli.train | end of epoch 674 (average epoch stats below)\n",
      "2023-04-17 15:40:18 | INFO | train | epoch 674 | loss 2.357 | nll_loss 0.204 | ppl 1.15 | wps 902.3 | ups 1.33 | wpb 678.5 | bsz 2 | num_updates 21559 | lr 2.20532e-05 | gnorm 2.244 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 17298\n",
      "2023-04-17 15:40:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:40:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:40:18 | INFO | fairseq.trainer | begin training epoch 675\n",
      "2023-04-17 15:40:18 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:40:30 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:40:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:40:30 | INFO | valid | epoch 675 | valid on 'valid' subset | loss 6.21 | nll_loss 4.543 | ppl 23.32 | wps 6260.8 | wpb 290.8 | bsz 1 | num_updates 21591 | best_loss 3.979\n",
      "2023-04-17 15:40:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 675 @ 21591 updates\n",
      "2023-04-17 15:40:30 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:40:42 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:40:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 675 @ 21591 updates, score 6.21) (writing took 12.321890227030963 seconds)\n",
      "2023-04-17 15:40:42 | INFO | fairseq_cli.train | end of epoch 675 (average epoch stats below)\n",
      "2023-04-17 15:40:42 | INFO | train | epoch 675 | loss 2.357 | nll_loss 0.205 | ppl 1.15 | wps 887.1 | ups 1.31 | wpb 678.5 | bsz 2 | num_updates 21591 | lr 2.20411e-05 | gnorm 2.417 | clip 100 | loss_scale 1 | train_wall 12 | gb_free 13.9 | wall 17323\n",
      "2023-04-17 15:40:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:40:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:40:42 | INFO | fairseq.trainer | begin training epoch 676\n",
      "2023-04-17 15:40:42 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:40:46 | INFO | train_inner | epoch 676:      9 / 32 loss=2.357, nll_loss=0.205, ppl=1.15, wps=935.8, ups=1.36, wpb=690.4, bsz=2, num_updates=21600, lr=2.20377e-05, gnorm=2.304, clip=100, loss_scale=1, train_wall=35, gb_free=13.9, wall=17326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 15:40:54 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:40:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:40:54 | INFO | valid | epoch 676 | valid on 'valid' subset | loss 6.138 | nll_loss 4.451 | ppl 21.87 | wps 7202.7 | wpb 290.8 | bsz 1 | num_updates 21623 | best_loss 3.979\n",
      "2023-04-17 15:40:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 676 @ 21623 updates\n",
      "2023-04-17 15:40:54 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:41:06 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:41:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 676 @ 21623 updates, score 6.138) (writing took 12.614602326997556 seconds)\n",
      "2023-04-17 15:41:06 | INFO | fairseq_cli.train | end of epoch 676 (average epoch stats below)\n",
      "2023-04-17 15:41:06 | INFO | train | epoch 676 | loss 2.36 | nll_loss 0.21 | ppl 1.16 | wps 906.7 | ups 1.34 | wpb 678.5 | bsz 2 | num_updates 21623 | lr 2.20291e-05 | gnorm 2.433 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 17347\n",
      "2023-04-17 15:41:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:41:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:41:06 | INFO | fairseq.trainer | begin training epoch 677\n",
      "2023-04-17 15:41:06 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:41:18 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:41:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:41:18 | INFO | valid | epoch 677 | valid on 'valid' subset | loss 6.2 | nll_loss 4.523 | ppl 22.99 | wps 7216 | wpb 290.8 | bsz 1 | num_updates 21655 | best_loss 3.979\n",
      "2023-04-17 15:41:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 677 @ 21655 updates\n",
      "2023-04-17 15:41:18 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:41:30 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:41:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 677 @ 21655 updates, score 6.2) (writing took 12.207695867982693 seconds)\n",
      "2023-04-17 15:41:30 | INFO | fairseq_cli.train | end of epoch 677 (average epoch stats below)\n",
      "2023-04-17 15:41:30 | INFO | train | epoch 677 | loss 2.357 | nll_loss 0.206 | ppl 1.15 | wps 916.2 | ups 1.35 | wpb 678.5 | bsz 2 | num_updates 21655 | lr 2.2017e-05 | gnorm 2.36 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 17370\n",
      "2023-04-17 15:41:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:41:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:41:30 | INFO | fairseq.trainer | begin training epoch 678\n",
      "2023-04-17 15:41:30 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:41:42 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:41:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:41:42 | INFO | valid | epoch 678 | valid on 'valid' subset | loss 6.208 | nll_loss 4.543 | ppl 23.3 | wps 6093.8 | wpb 290.8 | bsz 1 | num_updates 21687 | best_loss 3.979\n",
      "2023-04-17 15:41:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 678 @ 21687 updates\n",
      "2023-04-17 15:41:42 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:41:55 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:41:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 678 @ 21687 updates, score 6.208) (writing took 12.640205477015115 seconds)\n",
      "2023-04-17 15:41:55 | INFO | fairseq_cli.train | end of epoch 678 (average epoch stats below)\n",
      "2023-04-17 15:41:55 | INFO | train | epoch 678 | loss 2.353 | nll_loss 0.202 | ppl 1.15 | wps 875.1 | ups 1.29 | wpb 678.5 | bsz 2 | num_updates 21687 | lr 2.20049e-05 | gnorm 2.137 | clip 100 | loss_scale 1 | train_wall 12 | gb_free 13.9 | wall 17395\n",
      "2023-04-17 15:41:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:41:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:41:55 | INFO | fairseq.trainer | begin training epoch 679\n",
      "2023-04-17 15:41:55 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:42:00 | INFO | train_inner | epoch 679:     13 / 32 loss=2.357, nll_loss=0.206, ppl=1.15, wps=898, ups=1.35, wpb=664, bsz=2, num_updates=21700, lr=2.2e-05, gnorm=2.338, clip=100, loss_scale=1, train_wall=35, gb_free=13.9, wall=17400\n",
      "2023-04-17 15:42:06 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:42:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:42:07 | INFO | valid | epoch 679 | valid on 'valid' subset | loss 6.156 | nll_loss 4.476 | ppl 22.25 | wps 6816.9 | wpb 290.8 | bsz 1 | num_updates 21719 | best_loss 3.979\n",
      "2023-04-17 15:42:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 679 @ 21719 updates\n",
      "2023-04-17 15:42:07 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:42:19 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:42:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 679 @ 21719 updates, score 6.156) (writing took 12.487611131044105 seconds)\n",
      "2023-04-17 15:42:19 | INFO | fairseq_cli.train | end of epoch 679 (average epoch stats below)\n",
      "2023-04-17 15:42:19 | INFO | train | epoch 679 | loss 2.358 | nll_loss 0.207 | ppl 1.15 | wps 897.2 | ups 1.32 | wpb 678.5 | bsz 2 | num_updates 21719 | lr 2.19928e-05 | gnorm 2.387 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 17419\n",
      "2023-04-17 15:42:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:42:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:42:19 | INFO | fairseq.trainer | begin training epoch 680\n",
      "2023-04-17 15:42:19 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:42:31 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:42:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:42:31 | INFO | valid | epoch 680 | valid on 'valid' subset | loss 6.131 | nll_loss 4.441 | ppl 21.72 | wps 5994.6 | wpb 290.8 | bsz 1 | num_updates 21751 | best_loss 3.979\n",
      "2023-04-17 15:42:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 680 @ 21751 updates\n",
      "2023-04-17 15:42:31 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:42:44 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:42:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 680 @ 21751 updates, score 6.131) (writing took 12.645508883986622 seconds)\n",
      "2023-04-17 15:42:44 | INFO | fairseq_cli.train | end of epoch 680 (average epoch stats below)\n",
      "2023-04-17 15:42:44 | INFO | train | epoch 680 | loss 2.356 | nll_loss 0.203 | ppl 1.15 | wps 875.8 | ups 1.29 | wpb 678.5 | bsz 2 | num_updates 21751 | lr 2.19808e-05 | gnorm 2.088 | clip 100 | loss_scale 1 | train_wall 12 | gb_free 13.9 | wall 17444\n",
      "2023-04-17 15:42:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:42:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:42:44 | INFO | fairseq.trainer | begin training epoch 681\n",
      "2023-04-17 15:42:44 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:42:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:42:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 15:42:55 | INFO | valid | epoch 681 | valid on 'valid' subset | loss 6.153 | nll_loss 4.466 | ppl 22.1 | wps 6863.5 | wpb 290.8 | bsz 1 | num_updates 21783 | best_loss 3.979\n",
      "2023-04-17 15:42:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 681 @ 21783 updates\n",
      "2023-04-17 15:42:55 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:43:08 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:43:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 681 @ 21783 updates, score 6.153) (writing took 13.403381559997797 seconds)\n",
      "2023-04-17 15:43:09 | INFO | fairseq_cli.train | end of epoch 681 (average epoch stats below)\n",
      "2023-04-17 15:43:09 | INFO | train | epoch 681 | loss 2.355 | nll_loss 0.206 | ppl 1.15 | wps 879.2 | ups 1.3 | wpb 678.5 | bsz 2 | num_updates 21783 | lr 2.19687e-05 | gnorm 2.245 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 17469\n",
      "2023-04-17 15:43:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:43:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:43:09 | INFO | fairseq.trainer | begin training epoch 682\n",
      "2023-04-17 15:43:09 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:43:15 | INFO | train_inner | epoch 682:     17 / 32 loss=2.356, nll_loss=0.204, ppl=1.15, wps=900.4, ups=1.33, wpb=676.9, bsz=2, num_updates=21800, lr=2.19623e-05, gnorm=2.214, clip=100, loss_scale=1, train_wall=35, gb_free=13.9, wall=17475\n",
      "2023-04-17 15:43:20 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:43:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:43:20 | INFO | valid | epoch 682 | valid on 'valid' subset | loss 6.201 | nll_loss 4.524 | ppl 23 | wps 7281.5 | wpb 290.8 | bsz 1 | num_updates 21815 | best_loss 3.979\n",
      "2023-04-17 15:43:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 682 @ 21815 updates\n",
      "2023-04-17 15:43:20 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:43:32 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:43:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 682 @ 21815 updates, score 6.201) (writing took 12.10911420499906 seconds)\n",
      "2023-04-17 15:43:32 | INFO | fairseq_cli.train | end of epoch 682 (average epoch stats below)\n",
      "2023-04-17 15:43:32 | INFO | train | epoch 682 | loss 2.354 | nll_loss 0.201 | ppl 1.15 | wps 917.9 | ups 1.35 | wpb 678.5 | bsz 2 | num_updates 21815 | lr 2.19566e-05 | gnorm 2.204 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 17493\n",
      "2023-04-17 15:43:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:43:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:43:32 | INFO | fairseq.trainer | begin training epoch 683\n",
      "2023-04-17 15:43:32 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:43:44 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:43:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:43:44 | INFO | valid | epoch 683 | valid on 'valid' subset | loss 6.18 | nll_loss 4.5 | ppl 22.63 | wps 7001.6 | wpb 290.8 | bsz 1 | num_updates 21847 | best_loss 3.979\n",
      "2023-04-17 15:43:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 683 @ 21847 updates\n",
      "2023-04-17 15:43:44 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:43:56 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:43:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 683 @ 21847 updates, score 6.18) (writing took 11.936397725949064 seconds)\n",
      "2023-04-17 15:43:56 | INFO | fairseq_cli.train | end of epoch 683 (average epoch stats below)\n",
      "2023-04-17 15:43:56 | INFO | train | epoch 683 | loss 2.355 | nll_loss 0.203 | ppl 1.15 | wps 927.3 | ups 1.37 | wpb 678.5 | bsz 2 | num_updates 21847 | lr 2.19445e-05 | gnorm 2.514 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 17516\n",
      "2023-04-17 15:43:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:43:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:43:56 | INFO | fairseq.trainer | begin training epoch 684\n",
      "2023-04-17 15:43:56 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:44:07 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:44:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:44:07 | INFO | valid | epoch 684 | valid on 'valid' subset | loss 6.099 | nll_loss 4.409 | ppl 21.24 | wps 7206.9 | wpb 290.8 | bsz 1 | num_updates 21879 | best_loss 3.979\n",
      "2023-04-17 15:44:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 684 @ 21879 updates\n",
      "2023-04-17 15:44:07 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:44:19 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:44:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 684 @ 21879 updates, score 6.099) (writing took 12.26770813006442 seconds)\n",
      "2023-04-17 15:44:20 | INFO | fairseq_cli.train | end of epoch 684 (average epoch stats below)\n",
      "2023-04-17 15:44:20 | INFO | train | epoch 684 | loss 2.358 | nll_loss 0.208 | ppl 1.15 | wps 912.6 | ups 1.34 | wpb 678.5 | bsz 2 | num_updates 21879 | lr 2.19325e-05 | gnorm 2.458 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 17540\n",
      "2023-04-17 15:44:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:44:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:44:20 | INFO | fairseq.trainer | begin training epoch 685\n",
      "2023-04-17 15:44:20 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:44:27 | INFO | train_inner | epoch 685:     21 / 32 loss=2.356, nll_loss=0.204, ppl=1.15, wps=934.1, ups=1.39, wpb=674.1, bsz=2, num_updates=21900, lr=2.19245e-05, gnorm=2.402, clip=100, loss_scale=1, train_wall=35, gb_free=13.9, wall=17547\n",
      "2023-04-17 15:44:31 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:44:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:44:31 | INFO | valid | epoch 685 | valid on 'valid' subset | loss 6.109 | nll_loss 4.43 | ppl 21.56 | wps 7121 | wpb 290.8 | bsz 1 | num_updates 21911 | best_loss 3.979\n",
      "2023-04-17 15:44:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 685 @ 21911 updates\n",
      "2023-04-17 15:44:31 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:44:43 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:44:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 685 @ 21911 updates, score 6.109) (writing took 12.273878704058006 seconds)\n",
      "2023-04-17 15:44:43 | INFO | fairseq_cli.train | end of epoch 685 (average epoch stats below)\n",
      "2023-04-17 15:44:43 | INFO | train | epoch 685 | loss 2.358 | nll_loss 0.206 | ppl 1.15 | wps 917 | ups 1.35 | wpb 678.5 | bsz 2 | num_updates 21911 | lr 2.19204e-05 | gnorm 2.397 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 17563\n",
      "2023-04-17 15:44:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:44:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:44:43 | INFO | fairseq.trainer | begin training epoch 686\n",
      "2023-04-17 15:44:43 | INFO | fairseq_cli.train | Start iterating over samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 15:44:54 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:44:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:44:55 | INFO | valid | epoch 686 | valid on 'valid' subset | loss 6.14 | nll_loss 4.459 | ppl 21.99 | wps 7201 | wpb 290.8 | bsz 1 | num_updates 21943 | best_loss 3.979\n",
      "2023-04-17 15:44:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 686 @ 21943 updates\n",
      "2023-04-17 15:44:55 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:45:07 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:45:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 686 @ 21943 updates, score 6.14) (writing took 12.447938076918945 seconds)\n",
      "2023-04-17 15:45:07 | INFO | fairseq_cli.train | end of epoch 686 (average epoch stats below)\n",
      "2023-04-17 15:45:07 | INFO | train | epoch 686 | loss 2.356 | nll_loss 0.205 | ppl 1.15 | wps 912.2 | ups 1.34 | wpb 678.5 | bsz 2 | num_updates 21943 | lr 2.19083e-05 | gnorm 2.115 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 17587\n",
      "2023-04-17 15:45:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:45:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:45:07 | INFO | fairseq.trainer | begin training epoch 687\n",
      "2023-04-17 15:45:07 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:45:18 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:45:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:45:18 | INFO | valid | epoch 687 | valid on 'valid' subset | loss 6.217 | nll_loss 4.543 | ppl 23.31 | wps 6631.8 | wpb 290.8 | bsz 1 | num_updates 21975 | best_loss 3.979\n",
      "2023-04-17 15:45:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 687 @ 21975 updates\n",
      "2023-04-17 15:45:18 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:45:31 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:45:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 687 @ 21975 updates, score 6.217) (writing took 12.344046610058285 seconds)\n",
      "2023-04-17 15:45:31 | INFO | fairseq_cli.train | end of epoch 687 (average epoch stats below)\n",
      "2023-04-17 15:45:31 | INFO | train | epoch 687 | loss 2.353 | nll_loss 0.202 | ppl 1.15 | wps 911.5 | ups 1.34 | wpb 678.5 | bsz 2 | num_updates 21975 | lr 2.18962e-05 | gnorm 2.273 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 17611\n",
      "2023-04-17 15:45:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:45:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:45:31 | INFO | fairseq.trainer | begin training epoch 688\n",
      "2023-04-17 15:45:31 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:45:40 | INFO | train_inner | epoch 688:     25 / 32 loss=2.356, nll_loss=0.205, ppl=1.15, wps=938.1, ups=1.38, wpb=681.7, bsz=2, num_updates=22000, lr=2.18868e-05, gnorm=2.311, clip=100, loss_scale=1, train_wall=34, gb_free=13.9, wall=17620\n",
      "2023-04-17 15:45:42 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:45:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:45:42 | INFO | valid | epoch 688 | valid on 'valid' subset | loss 6.166 | nll_loss 4.489 | ppl 22.46 | wps 7072.7 | wpb 290.8 | bsz 1 | num_updates 22007 | best_loss 3.979\n",
      "2023-04-17 15:45:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 688 @ 22007 updates\n",
      "2023-04-17 15:45:42 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:45:54 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:45:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 688 @ 22007 updates, score 6.166) (writing took 12.20371673698537 seconds)\n",
      "2023-04-17 15:45:54 | INFO | fairseq_cli.train | end of epoch 688 (average epoch stats below)\n",
      "2023-04-17 15:45:54 | INFO | train | epoch 688 | loss 2.357 | nll_loss 0.205 | ppl 1.15 | wps 920.5 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 22007 | lr 2.18842e-05 | gnorm 2.556 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 17635\n",
      "2023-04-17 15:45:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:45:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:45:54 | INFO | fairseq.trainer | begin training epoch 689\n",
      "2023-04-17 15:45:54 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:46:06 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:46:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:46:06 | INFO | valid | epoch 689 | valid on 'valid' subset | loss 6.179 | nll_loss 4.497 | ppl 22.57 | wps 6029.7 | wpb 290.8 | bsz 1 | num_updates 22039 | best_loss 3.979\n",
      "2023-04-17 15:46:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 689 @ 22039 updates\n",
      "2023-04-17 15:46:06 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:46:18 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:46:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 689 @ 22039 updates, score 6.179) (writing took 12.129447640967555 seconds)\n",
      "2023-04-17 15:46:18 | INFO | fairseq_cli.train | end of epoch 689 (average epoch stats below)\n",
      "2023-04-17 15:46:18 | INFO | train | epoch 689 | loss 2.352 | nll_loss 0.201 | ppl 1.15 | wps 918.1 | ups 1.35 | wpb 678.5 | bsz 2 | num_updates 22039 | lr 2.18721e-05 | gnorm 2.197 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 17658\n",
      "2023-04-17 15:46:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:46:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:46:18 | INFO | fairseq.trainer | begin training epoch 690\n",
      "2023-04-17 15:46:18 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:46:29 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:46:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:46:29 | INFO | valid | epoch 690 | valid on 'valid' subset | loss 6.24 | nll_loss 4.564 | ppl 23.65 | wps 7217.7 | wpb 290.8 | bsz 1 | num_updates 22071 | best_loss 3.979\n",
      "2023-04-17 15:46:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 690 @ 22071 updates\n",
      "2023-04-17 15:46:29 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:46:41 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:46:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 690 @ 22071 updates, score 6.24) (writing took 12.235378911951557 seconds)\n",
      "2023-04-17 15:46:42 | INFO | fairseq_cli.train | end of epoch 690 (average epoch stats below)\n",
      "2023-04-17 15:46:42 | INFO | train | epoch 690 | loss 2.355 | nll_loss 0.203 | ppl 1.15 | wps 920.2 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 22071 | lr 2.186e-05 | gnorm 2.473 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 17682\n",
      "2023-04-17 15:46:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:46:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:46:42 | INFO | fairseq.trainer | begin training epoch 691\n",
      "2023-04-17 15:46:42 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:46:52 | INFO | train_inner | epoch 691:     29 / 32 loss=2.354, nll_loss=0.202, ppl=1.15, wps=951.6, ups=1.38, wpb=687.6, bsz=2, num_updates=22100, lr=2.18491e-05, gnorm=2.326, clip=100, loss_scale=1, train_wall=34, gb_free=13.9, wall=17692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 15:46:53 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:46:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:46:53 | INFO | valid | epoch 691 | valid on 'valid' subset | loss 6.191 | nll_loss 4.509 | ppl 22.76 | wps 7120.6 | wpb 290.8 | bsz 1 | num_updates 22103 | best_loss 3.979\n",
      "2023-04-17 15:46:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 691 @ 22103 updates\n",
      "2023-04-17 15:46:53 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:47:05 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:47:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 691 @ 22103 updates, score 6.191) (writing took 12.158094378071837 seconds)\n",
      "2023-04-17 15:47:05 | INFO | fairseq_cli.train | end of epoch 691 (average epoch stats below)\n",
      "2023-04-17 15:47:05 | INFO | train | epoch 691 | loss 2.354 | nll_loss 0.202 | ppl 1.15 | wps 920.7 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 22103 | lr 2.18479e-05 | gnorm 2.34 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 17705\n",
      "2023-04-17 15:47:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:47:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:47:05 | INFO | fairseq.trainer | begin training epoch 692\n",
      "2023-04-17 15:47:05 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:47:17 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:47:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:47:17 | INFO | valid | epoch 692 | valid on 'valid' subset | loss 6.152 | nll_loss 4.464 | ppl 22.07 | wps 6716.4 | wpb 290.8 | bsz 1 | num_updates 22135 | best_loss 3.979\n",
      "2023-04-17 15:47:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 692 @ 22135 updates\n",
      "2023-04-17 15:47:17 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:47:30 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:47:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 692 @ 22135 updates, score 6.152) (writing took 12.91009670495987 seconds)\n",
      "2023-04-17 15:47:30 | INFO | fairseq_cli.train | end of epoch 692 (average epoch stats below)\n",
      "2023-04-17 15:47:30 | INFO | train | epoch 692 | loss 2.354 | nll_loss 0.203 | ppl 1.15 | wps 886.8 | ups 1.31 | wpb 678.5 | bsz 2 | num_updates 22135 | lr 2.18358e-05 | gnorm 2.302 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 17730\n",
      "2023-04-17 15:47:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:47:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:47:30 | INFO | fairseq.trainer | begin training epoch 693\n",
      "2023-04-17 15:47:30 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:47:41 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:47:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:47:41 | INFO | valid | epoch 693 | valid on 'valid' subset | loss 6.145 | nll_loss 4.457 | ppl 21.97 | wps 6450.9 | wpb 290.8 | bsz 1 | num_updates 22167 | best_loss 3.979\n",
      "2023-04-17 15:47:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 693 @ 22167 updates\n",
      "2023-04-17 15:47:41 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:47:54 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:47:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 693 @ 22167 updates, score 6.145) (writing took 12.52514776494354 seconds)\n",
      "2023-04-17 15:47:54 | INFO | fairseq_cli.train | end of epoch 693 (average epoch stats below)\n",
      "2023-04-17 15:47:54 | INFO | train | epoch 693 | loss 2.353 | nll_loss 0.201 | ppl 1.15 | wps 897.5 | ups 1.32 | wpb 678.5 | bsz 2 | num_updates 22167 | lr 2.18238e-05 | gnorm 2.221 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 17754\n",
      "2023-04-17 15:47:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:47:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:47:54 | INFO | fairseq.trainer | begin training epoch 694\n",
      "2023-04-17 15:47:54 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:48:05 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:48:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:48:05 | INFO | valid | epoch 694 | valid on 'valid' subset | loss 6.119 | nll_loss 4.436 | ppl 21.65 | wps 7254.2 | wpb 290.8 | bsz 1 | num_updates 22199 | best_loss 3.979\n",
      "2023-04-17 15:48:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 694 @ 22199 updates\n",
      "2023-04-17 15:48:05 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:48:17 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:48:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 694 @ 22199 updates, score 6.119) (writing took 12.123607294051908 seconds)\n",
      "2023-04-17 15:48:17 | INFO | fairseq_cli.train | end of epoch 694 (average epoch stats below)\n",
      "2023-04-17 15:48:17 | INFO | train | epoch 694 | loss 2.356 | nll_loss 0.206 | ppl 1.15 | wps 927.5 | ups 1.37 | wpb 678.5 | bsz 2 | num_updates 22199 | lr 2.18117e-05 | gnorm 2.184 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 17778\n",
      "2023-04-17 15:48:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:48:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:48:17 | INFO | fairseq.trainer | begin training epoch 695\n",
      "2023-04-17 15:48:17 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:48:18 | INFO | train_inner | epoch 695:      1 / 32 loss=2.355, nll_loss=0.203, ppl=1.15, wps=782.6, ups=1.17, wpb=671.4, bsz=2, num_updates=22200, lr=2.18113e-05, gnorm=2.269, clip=100, loss_scale=1, train_wall=35, gb_free=13.9, wall=17778\n",
      "2023-04-17 15:48:28 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:48:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:48:29 | INFO | valid | epoch 695 | valid on 'valid' subset | loss 6.139 | nll_loss 4.457 | ppl 21.96 | wps 7204.3 | wpb 290.8 | bsz 1 | num_updates 22231 | best_loss 3.979\n",
      "2023-04-17 15:48:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 695 @ 22231 updates\n",
      "2023-04-17 15:48:29 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:48:41 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:48:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 695 @ 22231 updates, score 6.139) (writing took 12.352557361940853 seconds)\n",
      "2023-04-17 15:48:41 | INFO | fairseq_cli.train | end of epoch 695 (average epoch stats below)\n",
      "2023-04-17 15:48:41 | INFO | train | epoch 695 | loss 2.354 | nll_loss 0.203 | ppl 1.15 | wps 916.7 | ups 1.35 | wpb 678.5 | bsz 2 | num_updates 22231 | lr 2.17996e-05 | gnorm 2.211 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 17801\n",
      "2023-04-17 15:48:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:48:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:48:41 | INFO | fairseq.trainer | begin training epoch 696\n",
      "2023-04-17 15:48:41 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:48:52 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:48:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 15:48:52 | INFO | valid | epoch 696 | valid on 'valid' subset | loss 6.065 | nll_loss 4.38 | ppl 20.83 | wps 7284.4 | wpb 290.8 | bsz 1 | num_updates 22263 | best_loss 3.979\n",
      "2023-04-17 15:48:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 696 @ 22263 updates\n",
      "2023-04-17 15:48:52 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:49:04 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:49:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 696 @ 22263 updates, score 6.065) (writing took 12.163933229981922 seconds)\n",
      "2023-04-17 15:49:05 | INFO | fairseq_cli.train | end of epoch 696 (average epoch stats below)\n",
      "2023-04-17 15:49:05 | INFO | train | epoch 696 | loss 2.355 | nll_loss 0.203 | ppl 1.15 | wps 918.2 | ups 1.35 | wpb 678.5 | bsz 2 | num_updates 22263 | lr 2.17875e-05 | gnorm 2.055 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 17825\n",
      "2023-04-17 15:49:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:49:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:49:05 | INFO | fairseq.trainer | begin training epoch 697\n",
      "2023-04-17 15:49:05 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:49:16 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:49:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:49:16 | INFO | valid | epoch 697 | valid on 'valid' subset | loss 6.086 | nll_loss 4.398 | ppl 21.08 | wps 7303.2 | wpb 290.8 | bsz 1 | num_updates 22295 | best_loss 3.979\n",
      "2023-04-17 15:49:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 697 @ 22295 updates\n",
      "2023-04-17 15:49:16 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:49:28 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:49:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 697 @ 22295 updates, score 6.086) (writing took 12.264701759908348 seconds)\n",
      "2023-04-17 15:49:28 | INFO | fairseq_cli.train | end of epoch 697 (average epoch stats below)\n",
      "2023-04-17 15:49:28 | INFO | train | epoch 697 | loss 2.354 | nll_loss 0.203 | ppl 1.15 | wps 922.4 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 22295 | lr 2.17755e-05 | gnorm 2.277 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 17848\n",
      "2023-04-17 15:49:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:49:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:49:28 | INFO | fairseq.trainer | begin training epoch 698\n",
      "2023-04-17 15:49:28 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:49:30 | INFO | train_inner | epoch 698:      5 / 32 loss=2.354, nll_loss=0.203, ppl=1.15, wps=930.1, ups=1.39, wpb=671.4, bsz=2, num_updates=22300, lr=2.17736e-05, gnorm=2.179, clip=100, loss_scale=1, train_wall=34, gb_free=13.9, wall=17850\n",
      "2023-04-17 15:49:39 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:49:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:49:39 | INFO | valid | epoch 698 | valid on 'valid' subset | loss 6.123 | nll_loss 4.443 | ppl 21.76 | wps 7231.1 | wpb 290.8 | bsz 1 | num_updates 22327 | best_loss 3.979\n",
      "2023-04-17 15:49:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 698 @ 22327 updates\n",
      "2023-04-17 15:49:39 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:49:51 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:49:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 698 @ 22327 updates, score 6.123) (writing took 12.0802186379442 seconds)\n",
      "2023-04-17 15:49:51 | INFO | fairseq_cli.train | end of epoch 698 (average epoch stats below)\n",
      "2023-04-17 15:49:51 | INFO | train | epoch 698 | loss 2.354 | nll_loss 0.202 | ppl 1.15 | wps 945.8 | ups 1.39 | wpb 678.5 | bsz 2 | num_updates 22327 | lr 2.17634e-05 | gnorm 2.176 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 17871\n",
      "2023-04-17 15:49:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:49:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:49:51 | INFO | fairseq.trainer | begin training epoch 699\n",
      "2023-04-17 15:49:51 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:50:02 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:50:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:50:02 | INFO | valid | epoch 699 | valid on 'valid' subset | loss 6.117 | nll_loss 4.433 | ppl 21.6 | wps 7142.1 | wpb 290.8 | bsz 1 | num_updates 22359 | best_loss 3.979\n",
      "2023-04-17 15:50:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 699 @ 22359 updates\n",
      "2023-04-17 15:50:02 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:50:15 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:50:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 699 @ 22359 updates, score 6.117) (writing took 12.380602224031463 seconds)\n",
      "2023-04-17 15:50:15 | INFO | fairseq_cli.train | end of epoch 699 (average epoch stats below)\n",
      "2023-04-17 15:50:15 | INFO | train | epoch 699 | loss 2.351 | nll_loss 0.2 | ppl 1.15 | wps 923.1 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 22359 | lr 2.17513e-05 | gnorm 2.188 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 17895\n",
      "2023-04-17 15:50:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:50:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:50:15 | INFO | fairseq.trainer | begin training epoch 700\n",
      "2023-04-17 15:50:15 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:50:26 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:50:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:50:26 | INFO | valid | epoch 700 | valid on 'valid' subset | loss 6.142 | nll_loss 4.463 | ppl 22.05 | wps 7176.1 | wpb 290.8 | bsz 1 | num_updates 22391 | best_loss 3.979\n",
      "2023-04-17 15:50:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 700 @ 22391 updates\n",
      "2023-04-17 15:50:26 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:50:39 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:50:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 700 @ 22391 updates, score 6.142) (writing took 12.611435335013084 seconds)\n",
      "2023-04-17 15:50:39 | INFO | fairseq_cli.train | end of epoch 700 (average epoch stats below)\n",
      "2023-04-17 15:50:39 | INFO | train | epoch 700 | loss 2.352 | nll_loss 0.201 | ppl 1.15 | wps 902 | ups 1.33 | wpb 678.5 | bsz 2 | num_updates 22391 | lr 2.17392e-05 | gnorm 2.087 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 17919\n",
      "2023-04-17 15:50:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:50:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:50:39 | INFO | fairseq.trainer | begin training epoch 701\n",
      "2023-04-17 15:50:39 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:50:42 | INFO | train_inner | epoch 701:      9 / 32 loss=2.353, nll_loss=0.202, ppl=1.15, wps=946.5, ups=1.39, wpb=682, bsz=2, num_updates=22400, lr=2.17358e-05, gnorm=2.167, clip=100, loss_scale=1, train_wall=34, gb_free=13.9, wall=17922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 15:50:50 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:50:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:50:50 | INFO | valid | epoch 701 | valid on 'valid' subset | loss 6.04 | nll_loss 4.361 | ppl 20.54 | wps 7219.2 | wpb 290.8 | bsz 1 | num_updates 22423 | best_loss 3.979\n",
      "2023-04-17 15:50:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 701 @ 22423 updates\n",
      "2023-04-17 15:50:50 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:51:02 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:51:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 701 @ 22423 updates, score 6.04) (writing took 12.211547677987255 seconds)\n",
      "2023-04-17 15:51:02 | INFO | fairseq_cli.train | end of epoch 701 (average epoch stats below)\n",
      "2023-04-17 15:51:02 | INFO | train | epoch 701 | loss 2.353 | nll_loss 0.202 | ppl 1.15 | wps 921.2 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 22423 | lr 2.17272e-05 | gnorm 2.257 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 17943\n",
      "2023-04-17 15:51:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:51:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:51:02 | INFO | fairseq.trainer | begin training epoch 702\n",
      "2023-04-17 15:51:02 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:51:14 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:51:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:51:14 | INFO | valid | epoch 702 | valid on 'valid' subset | loss 6.2 | nll_loss 4.529 | ppl 23.09 | wps 7134.9 | wpb 290.8 | bsz 1 | num_updates 22455 | best_loss 3.979\n",
      "2023-04-17 15:51:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 702 @ 22455 updates\n",
      "2023-04-17 15:51:14 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:51:25 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:51:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 702 @ 22455 updates, score 6.2) (writing took 11.893807225045748 seconds)\n",
      "2023-04-17 15:51:26 | INFO | fairseq_cli.train | end of epoch 702 (average epoch stats below)\n",
      "2023-04-17 15:51:26 | INFO | train | epoch 702 | loss 2.351 | nll_loss 0.2 | ppl 1.15 | wps 931.2 | ups 1.37 | wpb 678.5 | bsz 2 | num_updates 22455 | lr 2.17151e-05 | gnorm 2.167 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 17966\n",
      "2023-04-17 15:51:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:51:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:51:26 | INFO | fairseq.trainer | begin training epoch 703\n",
      "2023-04-17 15:51:26 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:51:37 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:51:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:51:37 | INFO | valid | epoch 703 | valid on 'valid' subset | loss 6.144 | nll_loss 4.46 | ppl 22.01 | wps 7142.6 | wpb 290.8 | bsz 1 | num_updates 22487 | best_loss 3.979\n",
      "2023-04-17 15:51:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 703 @ 22487 updates\n",
      "2023-04-17 15:51:37 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:51:49 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:51:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 703 @ 22487 updates, score 6.144) (writing took 12.510826347977854 seconds)\n",
      "2023-04-17 15:51:50 | INFO | fairseq_cli.train | end of epoch 703 (average epoch stats below)\n",
      "2023-04-17 15:51:50 | INFO | train | epoch 703 | loss 2.352 | nll_loss 0.2 | ppl 1.15 | wps 907.6 | ups 1.34 | wpb 678.5 | bsz 2 | num_updates 22487 | lr 2.1703e-05 | gnorm 2.023 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 17990\n",
      "2023-04-17 15:51:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:51:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:51:50 | INFO | fairseq.trainer | begin training epoch 704\n",
      "2023-04-17 15:51:50 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:51:54 | INFO | train_inner | epoch 704:     13 / 32 loss=2.351, nll_loss=0.2, ppl=1.15, wps=948, ups=1.38, wpb=685.6, bsz=2, num_updates=22500, lr=2.16981e-05, gnorm=2.13, clip=100, loss_scale=1, train_wall=35, gb_free=13.9, wall=17995\n",
      "2023-04-17 15:52:01 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:52:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:52:01 | INFO | valid | epoch 704 | valid on 'valid' subset | loss 6.147 | nll_loss 4.47 | ppl 22.16 | wps 7169.1 | wpb 290.8 | bsz 1 | num_updates 22519 | best_loss 3.979\n",
      "2023-04-17 15:52:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 704 @ 22519 updates\n",
      "2023-04-17 15:52:01 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:52:13 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:52:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 704 @ 22519 updates, score 6.147) (writing took 12.153945173020475 seconds)\n",
      "2023-04-17 15:52:13 | INFO | fairseq_cli.train | end of epoch 704 (average epoch stats below)\n",
      "2023-04-17 15:52:13 | INFO | train | epoch 704 | loss 2.351 | nll_loss 0.2 | ppl 1.15 | wps 918.3 | ups 1.35 | wpb 678.5 | bsz 2 | num_updates 22519 | lr 2.16909e-05 | gnorm 2.208 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 18013\n",
      "2023-04-17 15:52:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:52:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:52:13 | INFO | fairseq.trainer | begin training epoch 705\n",
      "2023-04-17 15:52:13 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:52:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:52:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:52:25 | INFO | valid | epoch 705 | valid on 'valid' subset | loss 6.115 | nll_loss 4.435 | ppl 21.63 | wps 7004.7 | wpb 290.8 | bsz 1 | num_updates 22551 | best_loss 3.979\n",
      "2023-04-17 15:52:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 705 @ 22551 updates\n",
      "2023-04-17 15:52:25 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:52:37 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:52:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 705 @ 22551 updates, score 6.115) (writing took 12.416079007089138 seconds)\n",
      "2023-04-17 15:52:37 | INFO | fairseq_cli.train | end of epoch 705 (average epoch stats below)\n",
      "2023-04-17 15:52:37 | INFO | train | epoch 705 | loss 2.354 | nll_loss 0.203 | ppl 1.15 | wps 905.6 | ups 1.33 | wpb 678.5 | bsz 2 | num_updates 22551 | lr 2.16789e-05 | gnorm 2.428 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 18037\n",
      "2023-04-17 15:52:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:52:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:52:37 | INFO | fairseq.trainer | begin training epoch 706\n",
      "2023-04-17 15:52:37 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:52:48 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:52:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 15:52:49 | INFO | valid | epoch 706 | valid on 'valid' subset | loss 6.18 | nll_loss 4.503 | ppl 22.68 | wps 7164.8 | wpb 290.8 | bsz 1 | num_updates 22583 | best_loss 3.979\n",
      "2023-04-17 15:52:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 706 @ 22583 updates\n",
      "2023-04-17 15:52:49 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:53:00 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:53:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 706 @ 22583 updates, score 6.18) (writing took 12.082238283008337 seconds)\n",
      "2023-04-17 15:53:01 | INFO | fairseq_cli.train | end of epoch 706 (average epoch stats below)\n",
      "2023-04-17 15:53:01 | INFO | train | epoch 706 | loss 2.353 | nll_loss 0.202 | ppl 1.15 | wps 925.7 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 22583 | lr 2.16668e-05 | gnorm 2.344 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 18061\n",
      "2023-04-17 15:53:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:53:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:53:01 | INFO | fairseq.trainer | begin training epoch 707\n",
      "2023-04-17 15:53:01 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:53:07 | INFO | train_inner | epoch 707:     17 / 32 loss=2.353, nll_loss=0.202, ppl=1.15, wps=939.8, ups=1.38, wpb=681.1, bsz=2, num_updates=22600, lr=2.16604e-05, gnorm=2.302, clip=100, loss_scale=1, train_wall=35, gb_free=13.9, wall=18067\n",
      "2023-04-17 15:53:12 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:53:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:53:12 | INFO | valid | epoch 707 | valid on 'valid' subset | loss 6.04 | nll_loss 4.351 | ppl 20.4 | wps 7116.6 | wpb 290.8 | bsz 1 | num_updates 22615 | best_loss 3.979\n",
      "2023-04-17 15:53:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 707 @ 22615 updates\n",
      "2023-04-17 15:53:12 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:53:24 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:53:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 707 @ 22615 updates, score 6.04) (writing took 12.29673942795489 seconds)\n",
      "2023-04-17 15:53:24 | INFO | fairseq_cli.train | end of epoch 707 (average epoch stats below)\n",
      "2023-04-17 15:53:24 | INFO | train | epoch 707 | loss 2.353 | nll_loss 0.201 | ppl 1.15 | wps 912.7 | ups 1.35 | wpb 678.5 | bsz 2 | num_updates 22615 | lr 2.16547e-05 | gnorm 2.224 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 18085\n",
      "2023-04-17 15:53:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:53:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:53:24 | INFO | fairseq.trainer | begin training epoch 708\n",
      "2023-04-17 15:53:24 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:53:36 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:53:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:53:36 | INFO | valid | epoch 708 | valid on 'valid' subset | loss 6.087 | nll_loss 4.397 | ppl 21.06 | wps 7122.7 | wpb 290.8 | bsz 1 | num_updates 22647 | best_loss 3.979\n",
      "2023-04-17 15:53:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 708 @ 22647 updates\n",
      "2023-04-17 15:53:36 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:53:48 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:53:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 708 @ 22647 updates, score 6.087) (writing took 12.478154453914613 seconds)\n",
      "2023-04-17 15:53:48 | INFO | fairseq_cli.train | end of epoch 708 (average epoch stats below)\n",
      "2023-04-17 15:53:48 | INFO | train | epoch 708 | loss 2.352 | nll_loss 0.202 | ppl 1.15 | wps 906.3 | ups 1.34 | wpb 678.5 | bsz 2 | num_updates 22647 | lr 2.16426e-05 | gnorm 2.169 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 18109\n",
      "2023-04-17 15:53:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:53:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:53:48 | INFO | fairseq.trainer | begin training epoch 709\n",
      "2023-04-17 15:53:48 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:54:00 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:54:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:54:00 | INFO | valid | epoch 709 | valid on 'valid' subset | loss 6.136 | nll_loss 4.454 | ppl 21.92 | wps 7197 | wpb 290.8 | bsz 1 | num_updates 22679 | best_loss 3.979\n",
      "2023-04-17 15:54:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 709 @ 22679 updates\n",
      "2023-04-17 15:54:00 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:54:12 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:54:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 709 @ 22679 updates, score 6.136) (writing took 12.56711780501064 seconds)\n",
      "2023-04-17 15:54:12 | INFO | fairseq_cli.train | end of epoch 709 (average epoch stats below)\n",
      "2023-04-17 15:54:12 | INFO | train | epoch 709 | loss 2.35 | nll_loss 0.199 | ppl 1.15 | wps 906.3 | ups 1.34 | wpb 678.5 | bsz 2 | num_updates 22679 | lr 2.16306e-05 | gnorm 2.179 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 18133\n",
      "2023-04-17 15:54:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:54:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:54:12 | INFO | fairseq.trainer | begin training epoch 710\n",
      "2023-04-17 15:54:12 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:54:20 | INFO | train_inner | epoch 710:     21 / 32 loss=2.352, nll_loss=0.201, ppl=1.15, wps=923, ups=1.37, wpb=674.5, bsz=2, num_updates=22700, lr=2.16226e-05, gnorm=2.233, clip=100, loss_scale=1, train_wall=35, gb_free=13.9, wall=18140\n",
      "2023-04-17 15:54:24 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:54:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:54:24 | INFO | valid | epoch 710 | valid on 'valid' subset | loss 6.046 | nll_loss 4.367 | ppl 20.64 | wps 7173.1 | wpb 290.8 | bsz 1 | num_updates 22711 | best_loss 3.979\n",
      "2023-04-17 15:54:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 710 @ 22711 updates\n",
      "2023-04-17 15:54:24 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:54:36 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:54:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 710 @ 22711 updates, score 6.046) (writing took 12.109642481897026 seconds)\n",
      "2023-04-17 15:54:36 | INFO | fairseq_cli.train | end of epoch 710 (average epoch stats below)\n",
      "2023-04-17 15:54:36 | INFO | train | epoch 710 | loss 2.352 | nll_loss 0.201 | ppl 1.15 | wps 922.1 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 22711 | lr 2.16185e-05 | gnorm 2.115 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 18156\n",
      "2023-04-17 15:54:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:54:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:54:36 | INFO | fairseq.trainer | begin training epoch 711\n",
      "2023-04-17 15:54:36 | INFO | fairseq_cli.train | Start iterating over samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 15:54:47 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:54:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:54:47 | INFO | valid | epoch 711 | valid on 'valid' subset | loss 6.143 | nll_loss 4.462 | ppl 22.04 | wps 7130.7 | wpb 290.8 | bsz 1 | num_updates 22743 | best_loss 3.979\n",
      "2023-04-17 15:54:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 711 @ 22743 updates\n",
      "2023-04-17 15:54:47 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:54:59 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:55:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 711 @ 22743 updates, score 6.143) (writing took 12.29236940399278 seconds)\n",
      "2023-04-17 15:55:00 | INFO | fairseq_cli.train | end of epoch 711 (average epoch stats below)\n",
      "2023-04-17 15:55:00 | INFO | train | epoch 711 | loss 2.351 | nll_loss 0.201 | ppl 1.15 | wps 916.3 | ups 1.35 | wpb 678.5 | bsz 2 | num_updates 22743 | lr 2.16064e-05 | gnorm 2.232 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 18180\n",
      "2023-04-17 15:55:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:55:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:55:00 | INFO | fairseq.trainer | begin training epoch 712\n",
      "2023-04-17 15:55:00 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:55:11 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:55:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:55:11 | INFO | valid | epoch 712 | valid on 'valid' subset | loss 6.089 | nll_loss 4.384 | ppl 20.88 | wps 7165.3 | wpb 290.8 | bsz 1 | num_updates 22775 | best_loss 3.979\n",
      "2023-04-17 15:55:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 712 @ 22775 updates\n",
      "2023-04-17 15:55:11 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:55:23 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:55:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 712 @ 22775 updates, score 6.089) (writing took 12.211192989023402 seconds)\n",
      "2023-04-17 15:55:23 | INFO | fairseq_cli.train | end of epoch 712 (average epoch stats below)\n",
      "2023-04-17 15:55:23 | INFO | train | epoch 712 | loss 2.355 | nll_loss 0.206 | ppl 1.15 | wps 920.4 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 22775 | lr 2.15943e-05 | gnorm 2.272 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 18203\n",
      "2023-04-17 15:55:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:55:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:55:23 | INFO | fairseq.trainer | begin training epoch 713\n",
      "2023-04-17 15:55:23 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:55:32 | INFO | train_inner | epoch 713:     25 / 32 loss=2.353, nll_loss=0.203, ppl=1.15, wps=943.2, ups=1.38, wpb=682, bsz=2, num_updates=22800, lr=2.15849e-05, gnorm=2.188, clip=100, loss_scale=1, train_wall=35, gb_free=13.9, wall=18212\n",
      "2023-04-17 15:55:34 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:55:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:55:35 | INFO | valid | epoch 713 | valid on 'valid' subset | loss 6.163 | nll_loss 4.477 | ppl 22.27 | wps 7208 | wpb 290.8 | bsz 1 | num_updates 22807 | best_loss 3.979\n",
      "2023-04-17 15:55:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 713 @ 22807 updates\n",
      "2023-04-17 15:55:35 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:55:47 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:55:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 713 @ 22807 updates, score 6.163) (writing took 12.335518473060802 seconds)\n",
      "2023-04-17 15:55:47 | INFO | fairseq_cli.train | end of epoch 713 (average epoch stats below)\n",
      "2023-04-17 15:55:47 | INFO | train | epoch 713 | loss 2.352 | nll_loss 0.203 | ppl 1.15 | wps 911.7 | ups 1.34 | wpb 678.5 | bsz 2 | num_updates 22807 | lr 2.15823e-05 | gnorm 2.151 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 18227\n",
      "2023-04-17 15:55:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:55:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:55:47 | INFO | fairseq.trainer | begin training epoch 714\n",
      "2023-04-17 15:55:47 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:55:58 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:55:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:55:59 | INFO | valid | epoch 714 | valid on 'valid' subset | loss 6.044 | nll_loss 4.351 | ppl 20.41 | wps 7091.5 | wpb 290.8 | bsz 1 | num_updates 22839 | best_loss 3.979\n",
      "2023-04-17 15:55:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 714 @ 22839 updates\n",
      "2023-04-17 15:55:59 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:56:11 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:56:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 714 @ 22839 updates, score 6.044) (writing took 12.186634726938792 seconds)\n",
      "2023-04-17 15:56:11 | INFO | fairseq_cli.train | end of epoch 714 (average epoch stats below)\n",
      "2023-04-17 15:56:11 | INFO | train | epoch 714 | loss 2.351 | nll_loss 0.2 | ppl 1.15 | wps 915.8 | ups 1.35 | wpb 678.5 | bsz 2 | num_updates 22839 | lr 2.15702e-05 | gnorm 2.208 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 18251\n",
      "2023-04-17 15:56:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:56:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:56:11 | INFO | fairseq.trainer | begin training epoch 715\n",
      "2023-04-17 15:56:11 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:56:23 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:56:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:56:23 | INFO | valid | epoch 715 | valid on 'valid' subset | loss 6.166 | nll_loss 4.489 | ppl 22.45 | wps 6193.9 | wpb 290.8 | bsz 1 | num_updates 22871 | best_loss 3.979\n",
      "2023-04-17 15:56:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 715 @ 22871 updates\n",
      "2023-04-17 15:56:23 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:56:35 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:56:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 715 @ 22871 updates, score 6.166) (writing took 12.352792895049788 seconds)\n",
      "2023-04-17 15:56:35 | INFO | fairseq_cli.train | end of epoch 715 (average epoch stats below)\n",
      "2023-04-17 15:56:35 | INFO | train | epoch 715 | loss 2.35 | nll_loss 0.2 | ppl 1.15 | wps 881.2 | ups 1.3 | wpb 678.5 | bsz 2 | num_updates 22871 | lr 2.15581e-05 | gnorm 2.147 | clip 100 | loss_scale 1 | train_wall 12 | gb_free 13.9 | wall 18276\n",
      "2023-04-17 15:56:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:56:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:56:35 | INFO | fairseq.trainer | begin training epoch 716\n",
      "2023-04-17 15:56:35 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:56:46 | INFO | train_inner | epoch 716:     29 / 32 loss=2.351, nll_loss=0.2, ppl=1.15, wps=922.6, ups=1.36, wpb=677.3, bsz=2, num_updates=22900, lr=2.15472e-05, gnorm=2.173, clip=100, loss_scale=1, train_wall=35, gb_free=13.9, wall=18286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 15:56:46 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:56:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:56:47 | INFO | valid | epoch 716 | valid on 'valid' subset | loss 6.112 | nll_loss 4.417 | ppl 21.37 | wps 7181.7 | wpb 290.8 | bsz 1 | num_updates 22903 | best_loss 3.979\n",
      "2023-04-17 15:56:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 716 @ 22903 updates\n",
      "2023-04-17 15:56:47 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:56:59 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:56:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 716 @ 22903 updates, score 6.112) (writing took 12.14241806208156 seconds)\n",
      "2023-04-17 15:56:59 | INFO | fairseq_cli.train | end of epoch 716 (average epoch stats below)\n",
      "2023-04-17 15:56:59 | INFO | train | epoch 716 | loss 2.35 | nll_loss 0.2 | ppl 1.15 | wps 923.7 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 22903 | lr 2.1546e-05 | gnorm 2.289 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 18299\n",
      "2023-04-17 15:56:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:56:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:56:59 | INFO | fairseq.trainer | begin training epoch 717\n",
      "2023-04-17 15:56:59 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:57:10 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:57:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:57:10 | INFO | valid | epoch 717 | valid on 'valid' subset | loss 6.07 | nll_loss 4.384 | ppl 20.88 | wps 7229.2 | wpb 290.8 | bsz 1 | num_updates 22935 | best_loss 3.979\n",
      "2023-04-17 15:57:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 717 @ 22935 updates\n",
      "2023-04-17 15:57:10 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:57:23 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:57:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 717 @ 22935 updates, score 6.07) (writing took 12.7256710260408 seconds)\n",
      "2023-04-17 15:57:23 | INFO | fairseq_cli.train | end of epoch 717 (average epoch stats below)\n",
      "2023-04-17 15:57:23 | INFO | train | epoch 717 | loss 2.35 | nll_loss 0.198 | ppl 1.15 | wps 901.3 | ups 1.33 | wpb 678.5 | bsz 2 | num_updates 22935 | lr 2.1534e-05 | gnorm 2.459 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 18323\n",
      "2023-04-17 15:57:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:57:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:57:23 | INFO | fairseq.trainer | begin training epoch 718\n",
      "2023-04-17 15:57:23 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:57:34 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:57:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:57:34 | INFO | valid | epoch 718 | valid on 'valid' subset | loss 6.123 | nll_loss 4.44 | ppl 21.7 | wps 7182.3 | wpb 290.8 | bsz 1 | num_updates 22967 | best_loss 3.979\n",
      "2023-04-17 15:57:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 718 @ 22967 updates\n",
      "2023-04-17 15:57:34 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:57:46 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:57:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 718 @ 22967 updates, score 6.123) (writing took 12.188959331950173 seconds)\n",
      "2023-04-17 15:57:46 | INFO | fairseq_cli.train | end of epoch 718 (average epoch stats below)\n",
      "2023-04-17 15:57:46 | INFO | train | epoch 718 | loss 2.35 | nll_loss 0.2 | ppl 1.15 | wps 922.8 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 22967 | lr 2.15219e-05 | gnorm 2.293 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 18347\n",
      "2023-04-17 15:57:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:57:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:57:46 | INFO | fairseq.trainer | begin training epoch 719\n",
      "2023-04-17 15:57:46 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:57:58 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:57:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:57:58 | INFO | valid | epoch 719 | valid on 'valid' subset | loss 6.121 | nll_loss 4.437 | ppl 21.66 | wps 7255 | wpb 290.8 | bsz 1 | num_updates 22999 | best_loss 3.979\n",
      "2023-04-17 15:57:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 719 @ 22999 updates\n",
      "2023-04-17 15:57:58 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:58:10 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:58:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 719 @ 22999 updates, score 6.121) (writing took 12.344228103989735 seconds)\n",
      "2023-04-17 15:58:10 | INFO | fairseq_cli.train | end of epoch 719 (average epoch stats below)\n",
      "2023-04-17 15:58:10 | INFO | train | epoch 719 | loss 2.349 | nll_loss 0.199 | ppl 1.15 | wps 914.4 | ups 1.35 | wpb 678.5 | bsz 2 | num_updates 22999 | lr 2.15098e-05 | gnorm 2.102 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 18370\n",
      "2023-04-17 15:58:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:58:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:58:10 | INFO | fairseq.trainer | begin training epoch 720\n",
      "2023-04-17 15:58:10 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:58:11 | INFO | train_inner | epoch 720:      1 / 32 loss=2.35, nll_loss=0.199, ppl=1.15, wps=791.9, ups=1.18, wpb=673.7, bsz=2, num_updates=23000, lr=2.15094e-05, gnorm=2.328, clip=100, loss_scale=1, train_wall=34, gb_free=13.9, wall=18371\n",
      "2023-04-17 15:58:21 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:58:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:58:22 | INFO | valid | epoch 720 | valid on 'valid' subset | loss 6.044 | nll_loss 4.353 | ppl 20.44 | wps 7168.9 | wpb 290.8 | bsz 1 | num_updates 23031 | best_loss 3.979\n",
      "2023-04-17 15:58:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 720 @ 23031 updates\n",
      "2023-04-17 15:58:22 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:58:33 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:58:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 720 @ 23031 updates, score 6.044) (writing took 12.111957896035165 seconds)\n",
      "2023-04-17 15:58:34 | INFO | fairseq_cli.train | end of epoch 720 (average epoch stats below)\n",
      "2023-04-17 15:58:34 | INFO | train | epoch 720 | loss 2.35 | nll_loss 0.2 | ppl 1.15 | wps 925.2 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 23031 | lr 2.14977e-05 | gnorm 2.384 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 18394\n",
      "2023-04-17 15:58:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:58:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:58:34 | INFO | fairseq.trainer | begin training epoch 721\n",
      "2023-04-17 15:58:34 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:58:45 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:58:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 15:58:45 | INFO | valid | epoch 721 | valid on 'valid' subset | loss 6.075 | nll_loss 4.386 | ppl 20.9 | wps 7121.8 | wpb 290.8 | bsz 1 | num_updates 23063 | best_loss 3.979\n",
      "2023-04-17 15:58:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 721 @ 23063 updates\n",
      "2023-04-17 15:58:45 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:58:57 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:58:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 721 @ 23063 updates, score 6.075) (writing took 12.169349396019243 seconds)\n",
      "2023-04-17 15:58:57 | INFO | fairseq_cli.train | end of epoch 721 (average epoch stats below)\n",
      "2023-04-17 15:58:57 | INFO | train | epoch 721 | loss 2.35 | nll_loss 0.201 | ppl 1.15 | wps 922.9 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 23063 | lr 2.14857e-05 | gnorm 2.266 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 18417\n",
      "2023-04-17 15:58:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:58:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:58:57 | INFO | fairseq.trainer | begin training epoch 722\n",
      "2023-04-17 15:58:57 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:59:08 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:59:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:59:09 | INFO | valid | epoch 722 | valid on 'valid' subset | loss 6.095 | nll_loss 4.394 | ppl 21.02 | wps 5885.7 | wpb 290.8 | bsz 1 | num_updates 23095 | best_loss 3.979\n",
      "2023-04-17 15:59:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 722 @ 23095 updates\n",
      "2023-04-17 15:59:09 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:59:21 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:59:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 722 @ 23095 updates, score 6.095) (writing took 12.262939882930368 seconds)\n",
      "2023-04-17 15:59:21 | INFO | fairseq_cli.train | end of epoch 722 (average epoch stats below)\n",
      "2023-04-17 15:59:21 | INFO | train | epoch 722 | loss 2.349 | nll_loss 0.196 | ppl 1.15 | wps 913.4 | ups 1.35 | wpb 678.5 | bsz 2 | num_updates 23095 | lr 2.14736e-05 | gnorm 2.091 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 18441\n",
      "2023-04-17 15:59:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:59:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:59:21 | INFO | fairseq.trainer | begin training epoch 723\n",
      "2023-04-17 15:59:21 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:59:23 | INFO | train_inner | epoch 723:      5 / 32 loss=2.35, nll_loss=0.198, ppl=1.15, wps=944.5, ups=1.38, wpb=682.7, bsz=2, num_updates=23100, lr=2.14717e-05, gnorm=2.248, clip=100, loss_scale=1, train_wall=35, gb_free=13.9, wall=18443\n",
      "2023-04-17 15:59:33 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:59:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:59:33 | INFO | valid | epoch 723 | valid on 'valid' subset | loss 6.099 | nll_loss 4.401 | ppl 21.13 | wps 6154.1 | wpb 290.8 | bsz 1 | num_updates 23127 | best_loss 3.979\n",
      "2023-04-17 15:59:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 723 @ 23127 updates\n",
      "2023-04-17 15:59:33 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:59:45 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 15:59:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 723 @ 23127 updates, score 6.099) (writing took 12.379770366940647 seconds)\n",
      "2023-04-17 15:59:46 | INFO | fairseq_cli.train | end of epoch 723 (average epoch stats below)\n",
      "2023-04-17 15:59:46 | INFO | train | epoch 723 | loss 2.351 | nll_loss 0.201 | ppl 1.15 | wps 885.4 | ups 1.3 | wpb 678.5 | bsz 2 | num_updates 23127 | lr 2.14615e-05 | gnorm 2.29 | clip 100 | loss_scale 1 | train_wall 12 | gb_free 13.9 | wall 18466\n",
      "2023-04-17 15:59:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:59:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 15:59:46 | INFO | fairseq.trainer | begin training epoch 724\n",
      "2023-04-17 15:59:46 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 15:59:57 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 15:59:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 15:59:57 | INFO | valid | epoch 724 | valid on 'valid' subset | loss 6.102 | nll_loss 4.406 | ppl 21.2 | wps 7166.6 | wpb 290.8 | bsz 1 | num_updates 23159 | best_loss 3.979\n",
      "2023-04-17 15:59:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 724 @ 23159 updates\n",
      "2023-04-17 15:59:57 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:00:09 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:00:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 724 @ 23159 updates, score 6.102) (writing took 12.242867331020534 seconds)\n",
      "2023-04-17 16:00:09 | INFO | fairseq_cli.train | end of epoch 724 (average epoch stats below)\n",
      "2023-04-17 16:00:09 | INFO | train | epoch 724 | loss 2.347 | nll_loss 0.196 | ppl 1.15 | wps 912.5 | ups 1.34 | wpb 678.5 | bsz 2 | num_updates 23159 | lr 2.14494e-05 | gnorm 2.626 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 18490\n",
      "2023-04-17 16:00:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:00:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:00:09 | INFO | fairseq.trainer | begin training epoch 725\n",
      "2023-04-17 16:00:09 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:00:22 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:00:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:00:22 | INFO | valid | epoch 725 | valid on 'valid' subset | loss 6.034 | nll_loss 4.331 | ppl 20.13 | wps 7096.8 | wpb 290.8 | bsz 1 | num_updates 23191 | best_loss 3.979\n",
      "2023-04-17 16:00:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 725 @ 23191 updates\n",
      "2023-04-17 16:00:22 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:00:34 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:00:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 725 @ 23191 updates, score 6.034) (writing took 12.301599203026854 seconds)\n",
      "2023-04-17 16:00:34 | INFO | fairseq_cli.train | end of epoch 725 (average epoch stats below)\n",
      "2023-04-17 16:00:34 | INFO | train | epoch 725 | loss 2.35 | nll_loss 0.201 | ppl 1.15 | wps 873.4 | ups 1.29 | wpb 678.5 | bsz 2 | num_updates 23191 | lr 2.14374e-05 | gnorm 2.281 | clip 100 | loss_scale 1 | train_wall 12 | gb_free 13.9 | wall 18514\n",
      "2023-04-17 16:00:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:00:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:00:34 | INFO | fairseq.trainer | begin training epoch 726\n",
      "2023-04-17 16:00:34 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:00:38 | INFO | train_inner | epoch 726:      9 / 32 loss=2.349, nll_loss=0.2, ppl=1.15, wps=911.9, ups=1.34, wpb=682, bsz=2, num_updates=23200, lr=2.1434e-05, gnorm=2.357, clip=100, loss_scale=1, train_wall=37, gb_free=13.9, wall=18518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 16:00:46 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:00:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:00:47 | INFO | valid | epoch 726 | valid on 'valid' subset | loss 6.162 | nll_loss 4.478 | ppl 22.28 | wps 7117.8 | wpb 290.8 | bsz 1 | num_updates 23223 | best_loss 3.979\n",
      "2023-04-17 16:00:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 726 @ 23223 updates\n",
      "2023-04-17 16:00:47 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:00:59 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:00:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 726 @ 23223 updates, score 6.162) (writing took 12.21089910401497 seconds)\n",
      "2023-04-17 16:00:59 | INFO | fairseq_cli.train | end of epoch 726 (average epoch stats below)\n",
      "2023-04-17 16:00:59 | INFO | train | epoch 726 | loss 2.352 | nll_loss 0.203 | ppl 1.15 | wps 880.7 | ups 1.3 | wpb 678.5 | bsz 2 | num_updates 23223 | lr 2.14253e-05 | gnorm 2.263 | clip 100 | loss_scale 1 | train_wall 12 | gb_free 13.9 | wall 18539\n",
      "2023-04-17 16:00:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:00:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:00:59 | INFO | fairseq.trainer | begin training epoch 727\n",
      "2023-04-17 16:00:59 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:01:11 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:01:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:01:11 | INFO | valid | epoch 727 | valid on 'valid' subset | loss 6.074 | nll_loss 4.384 | ppl 20.88 | wps 7068.7 | wpb 290.8 | bsz 1 | num_updates 23255 | best_loss 3.979\n",
      "2023-04-17 16:01:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 727 @ 23255 updates\n",
      "2023-04-17 16:01:11 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:01:23 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:01:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 727 @ 23255 updates, score 6.074) (writing took 12.179682397982106 seconds)\n",
      "2023-04-17 16:01:24 | INFO | fairseq_cli.train | end of epoch 727 (average epoch stats below)\n",
      "2023-04-17 16:01:24 | INFO | train | epoch 727 | loss 2.349 | nll_loss 0.197 | ppl 1.15 | wps 879.5 | ups 1.3 | wpb 678.5 | bsz 2 | num_updates 23255 | lr 2.14132e-05 | gnorm 2.372 | clip 100 | loss_scale 1 | train_wall 12 | gb_free 13.9 | wall 18564\n",
      "2023-04-17 16:01:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:01:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:01:24 | INFO | fairseq.trainer | begin training epoch 728\n",
      "2023-04-17 16:01:24 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:01:35 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:01:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:01:35 | INFO | valid | epoch 728 | valid on 'valid' subset | loss 6.106 | nll_loss 4.417 | ppl 21.36 | wps 7260.8 | wpb 290.8 | bsz 1 | num_updates 23287 | best_loss 3.979\n",
      "2023-04-17 16:01:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 728 @ 23287 updates\n",
      "2023-04-17 16:01:35 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:01:47 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:01:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 728 @ 23287 updates, score 6.106) (writing took 12.195218382054009 seconds)\n",
      "2023-04-17 16:01:48 | INFO | fairseq_cli.train | end of epoch 728 (average epoch stats below)\n",
      "2023-04-17 16:01:48 | INFO | train | epoch 728 | loss 2.351 | nll_loss 0.2 | ppl 1.15 | wps 898.9 | ups 1.32 | wpb 678.5 | bsz 2 | num_updates 23287 | lr 2.14011e-05 | gnorm 2.319 | clip 100 | loss_scale 1 | train_wall 12 | gb_free 13.9 | wall 18588\n",
      "2023-04-17 16:01:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:01:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:01:48 | INFO | fairseq.trainer | begin training epoch 729\n",
      "2023-04-17 16:01:48 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:01:52 | INFO | train_inner | epoch 729:     13 / 32 loss=2.35, nll_loss=0.199, ppl=1.15, wps=905.7, ups=1.34, wpb=675.5, bsz=2, num_updates=23300, lr=2.13962e-05, gnorm=2.297, clip=100, loss_scale=1, train_wall=37, gb_free=13.9, wall=18593\n",
      "2023-04-17 16:01:59 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:01:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:01:59 | INFO | valid | epoch 729 | valid on 'valid' subset | loss 6.081 | nll_loss 4.382 | ppl 20.86 | wps 7078.7 | wpb 290.8 | bsz 1 | num_updates 23319 | best_loss 3.979\n",
      "2023-04-17 16:01:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 729 @ 23319 updates\n",
      "2023-04-17 16:01:59 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:02:11 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:02:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 729 @ 23319 updates, score 6.081) (writing took 12.351036424050108 seconds)\n",
      "2023-04-17 16:02:11 | INFO | fairseq_cli.train | end of epoch 729 (average epoch stats below)\n",
      "2023-04-17 16:02:11 | INFO | train | epoch 729 | loss 2.348 | nll_loss 0.199 | ppl 1.15 | wps 913.8 | ups 1.35 | wpb 678.5 | bsz 2 | num_updates 23319 | lr 2.13891e-05 | gnorm 2.154 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 18612\n",
      "2023-04-17 16:02:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:02:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:02:11 | INFO | fairseq.trainer | begin training epoch 730\n",
      "2023-04-17 16:02:11 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:02:23 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:02:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:02:23 | INFO | valid | epoch 730 | valid on 'valid' subset | loss 6.114 | nll_loss 4.446 | ppl 21.8 | wps 7147.2 | wpb 290.8 | bsz 1 | num_updates 23351 | best_loss 3.979\n",
      "2023-04-17 16:02:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 730 @ 23351 updates\n",
      "2023-04-17 16:02:23 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:02:35 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:02:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 730 @ 23351 updates, score 6.114) (writing took 12.13302954705432 seconds)\n",
      "2023-04-17 16:02:35 | INFO | fairseq_cli.train | end of epoch 730 (average epoch stats below)\n",
      "2023-04-17 16:02:35 | INFO | train | epoch 730 | loss 2.349 | nll_loss 0.199 | ppl 1.15 | wps 915.6 | ups 1.35 | wpb 678.5 | bsz 2 | num_updates 23351 | lr 2.1377e-05 | gnorm 1.971 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 18635\n",
      "2023-04-17 16:02:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:02:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:02:35 | INFO | fairseq.trainer | begin training epoch 731\n",
      "2023-04-17 16:02:35 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:02:46 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:02:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 16:02:47 | INFO | valid | epoch 731 | valid on 'valid' subset | loss 6.069 | nll_loss 4.377 | ppl 20.78 | wps 6128.5 | wpb 290.8 | bsz 1 | num_updates 23383 | best_loss 3.979\n",
      "2023-04-17 16:02:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 731 @ 23383 updates\n",
      "2023-04-17 16:02:47 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:02:59 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:02:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 731 @ 23383 updates, score 6.069) (writing took 12.299537423998117 seconds)\n",
      "2023-04-17 16:02:59 | INFO | fairseq_cli.train | end of epoch 731 (average epoch stats below)\n",
      "2023-04-17 16:02:59 | INFO | train | epoch 731 | loss 2.351 | nll_loss 0.201 | ppl 1.15 | wps 915 | ups 1.35 | wpb 678.5 | bsz 2 | num_updates 23383 | lr 2.13649e-05 | gnorm 2.42 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 18659\n",
      "2023-04-17 16:02:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:02:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:02:59 | INFO | fairseq.trainer | begin training epoch 732\n",
      "2023-04-17 16:02:59 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:03:05 | INFO | train_inner | epoch 732:     17 / 32 loss=2.349, nll_loss=0.2, ppl=1.15, wps=933.1, ups=1.38, wpb=677.7, bsz=2, num_updates=23400, lr=2.13585e-05, gnorm=2.204, clip=100, loss_scale=1, train_wall=35, gb_free=13.9, wall=18665\n",
      "2023-04-17 16:03:10 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:03:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:03:10 | INFO | valid | epoch 732 | valid on 'valid' subset | loss 6.117 | nll_loss 4.444 | ppl 21.76 | wps 7156.4 | wpb 290.8 | bsz 1 | num_updates 23415 | best_loss 3.979\n",
      "2023-04-17 16:03:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 732 @ 23415 updates\n",
      "2023-04-17 16:03:10 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:03:22 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:03:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 732 @ 23415 updates, score 6.117) (writing took 12.044549974030815 seconds)\n",
      "2023-04-17 16:03:22 | INFO | fairseq_cli.train | end of epoch 732 (average epoch stats below)\n",
      "2023-04-17 16:03:22 | INFO | train | epoch 732 | loss 2.348 | nll_loss 0.198 | ppl 1.15 | wps 926.5 | ups 1.37 | wpb 678.5 | bsz 2 | num_updates 23415 | lr 2.13528e-05 | gnorm 2.22 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 18683\n",
      "2023-04-17 16:03:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:03:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:03:22 | INFO | fairseq.trainer | begin training epoch 733\n",
      "2023-04-17 16:03:22 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:03:33 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:03:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:03:34 | INFO | valid | epoch 733 | valid on 'valid' subset | loss 6.119 | nll_loss 4.432 | ppl 21.59 | wps 7199.1 | wpb 290.8 | bsz 1 | num_updates 23447 | best_loss 3.979\n",
      "2023-04-17 16:03:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 733 @ 23447 updates\n",
      "2023-04-17 16:03:34 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:03:46 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:03:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 733 @ 23447 updates, score 6.119) (writing took 12.436911538941786 seconds)\n",
      "2023-04-17 16:03:46 | INFO | fairseq_cli.train | end of epoch 733 (average epoch stats below)\n",
      "2023-04-17 16:03:46 | INFO | train | epoch 733 | loss 2.35 | nll_loss 0.2 | ppl 1.15 | wps 911.3 | ups 1.34 | wpb 678.5 | bsz 2 | num_updates 23447 | lr 2.13408e-05 | gnorm 2.213 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 18706\n",
      "2023-04-17 16:03:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:03:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:03:46 | INFO | fairseq.trainer | begin training epoch 734\n",
      "2023-04-17 16:03:46 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:03:57 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:03:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:03:58 | INFO | valid | epoch 734 | valid on 'valid' subset | loss 6.089 | nll_loss 4.409 | ppl 21.24 | wps 7058.9 | wpb 290.8 | bsz 1 | num_updates 23479 | best_loss 3.979\n",
      "2023-04-17 16:03:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 734 @ 23479 updates\n",
      "2023-04-17 16:03:58 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:04:10 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:04:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 734 @ 23479 updates, score 6.089) (writing took 12.214400568976998 seconds)\n",
      "2023-04-17 16:04:10 | INFO | fairseq_cli.train | end of epoch 734 (average epoch stats below)\n",
      "2023-04-17 16:04:10 | INFO | train | epoch 734 | loss 2.349 | nll_loss 0.199 | ppl 1.15 | wps 918.7 | ups 1.35 | wpb 678.5 | bsz 2 | num_updates 23479 | lr 2.13287e-05 | gnorm 2.122 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 18730\n",
      "2023-04-17 16:04:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:04:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:04:10 | INFO | fairseq.trainer | begin training epoch 735\n",
      "2023-04-17 16:04:10 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:04:17 | INFO | train_inner | epoch 735:     21 / 32 loss=2.35, nll_loss=0.2, ppl=1.15, wps=926.5, ups=1.38, wpb=669.9, bsz=2, num_updates=23500, lr=2.13208e-05, gnorm=2.266, clip=100, loss_scale=1, train_wall=34, gb_free=13.9, wall=18737\n",
      "2023-04-17 16:04:21 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:04:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:04:21 | INFO | valid | epoch 735 | valid on 'valid' subset | loss 6.15 | nll_loss 4.465 | ppl 22.09 | wps 7116.3 | wpb 290.8 | bsz 1 | num_updates 23511 | best_loss 3.979\n",
      "2023-04-17 16:04:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 735 @ 23511 updates\n",
      "2023-04-17 16:04:21 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:04:33 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:04:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 735 @ 23511 updates, score 6.15) (writing took 12.300039350055158 seconds)\n",
      "2023-04-17 16:04:33 | INFO | fairseq_cli.train | end of epoch 735 (average epoch stats below)\n",
      "2023-04-17 16:04:33 | INFO | train | epoch 735 | loss 2.349 | nll_loss 0.2 | ppl 1.15 | wps 917.2 | ups 1.35 | wpb 678.5 | bsz 2 | num_updates 23511 | lr 2.13166e-05 | gnorm 2.388 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 18754\n",
      "2023-04-17 16:04:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:04:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:04:33 | INFO | fairseq.trainer | begin training epoch 736\n",
      "2023-04-17 16:04:33 | INFO | fairseq_cli.train | Start iterating over samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 16:04:45 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:04:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:04:45 | INFO | valid | epoch 736 | valid on 'valid' subset | loss 6.143 | nll_loss 4.477 | ppl 22.27 | wps 6213.3 | wpb 290.8 | bsz 1 | num_updates 23543 | best_loss 3.979\n",
      "2023-04-17 16:04:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 736 @ 23543 updates\n",
      "2023-04-17 16:04:45 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:04:58 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:04:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 736 @ 23543 updates, score 6.143) (writing took 12.463932050974108 seconds)\n",
      "2023-04-17 16:04:58 | INFO | fairseq_cli.train | end of epoch 736 (average epoch stats below)\n",
      "2023-04-17 16:04:58 | INFO | train | epoch 736 | loss 2.349 | nll_loss 0.197 | ppl 1.15 | wps 885.7 | ups 1.31 | wpb 678.5 | bsz 2 | num_updates 23543 | lr 2.13045e-05 | gnorm 2.372 | clip 100 | loss_scale 1 | train_wall 12 | gb_free 13.9 | wall 18778\n",
      "2023-04-17 16:04:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:04:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:04:58 | INFO | fairseq.trainer | begin training epoch 737\n",
      "2023-04-17 16:04:58 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:05:10 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:05:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:05:10 | INFO | valid | epoch 737 | valid on 'valid' subset | loss 6.026 | nll_loss 4.337 | ppl 20.22 | wps 6089.9 | wpb 290.8 | bsz 1 | num_updates 23575 | best_loss 3.979\n",
      "2023-04-17 16:05:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 737 @ 23575 updates\n",
      "2023-04-17 16:05:10 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:05:22 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:05:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 737 @ 23575 updates, score 6.026) (writing took 12.439066190039739 seconds)\n",
      "2023-04-17 16:05:22 | INFO | fairseq_cli.train | end of epoch 737 (average epoch stats below)\n",
      "2023-04-17 16:05:22 | INFO | train | epoch 737 | loss 2.349 | nll_loss 0.197 | ppl 1.15 | wps 887.5 | ups 1.31 | wpb 678.5 | bsz 2 | num_updates 23575 | lr 2.12925e-05 | gnorm 2.165 | clip 100 | loss_scale 1 | train_wall 12 | gb_free 13.9 | wall 18803\n",
      "2023-04-17 16:05:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:05:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:05:22 | INFO | fairseq.trainer | begin training epoch 738\n",
      "2023-04-17 16:05:22 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:05:31 | INFO | train_inner | epoch 738:     25 / 32 loss=2.348, nll_loss=0.197, ppl=1.15, wps=929.7, ups=1.35, wpb=688.5, bsz=2, num_updates=23600, lr=2.1283e-05, gnorm=2.205, clip=100, loss_scale=1, train_wall=36, gb_free=13.9, wall=18812\n",
      "2023-04-17 16:05:34 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:05:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:05:34 | INFO | valid | epoch 738 | valid on 'valid' subset | loss 6.119 | nll_loss 4.433 | ppl 21.6 | wps 7275.6 | wpb 290.8 | bsz 1 | num_updates 23607 | best_loss 3.979\n",
      "2023-04-17 16:05:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 738 @ 23607 updates\n",
      "2023-04-17 16:05:34 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:05:46 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:05:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 738 @ 23607 updates, score 6.119) (writing took 12.064630792010576 seconds)\n",
      "2023-04-17 16:05:46 | INFO | fairseq_cli.train | end of epoch 738 (average epoch stats below)\n",
      "2023-04-17 16:05:46 | INFO | train | epoch 738 | loss 2.347 | nll_loss 0.198 | ppl 1.15 | wps 926.8 | ups 1.37 | wpb 678.5 | bsz 2 | num_updates 23607 | lr 2.12804e-05 | gnorm 2.091 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 18826\n",
      "2023-04-17 16:05:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:05:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:05:46 | INFO | fairseq.trainer | begin training epoch 739\n",
      "2023-04-17 16:05:46 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:05:57 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:05:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:05:57 | INFO | valid | epoch 739 | valid on 'valid' subset | loss 6.04 | nll_loss 4.346 | ppl 20.33 | wps 7123 | wpb 290.8 | bsz 1 | num_updates 23639 | best_loss 3.979\n",
      "2023-04-17 16:05:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 739 @ 23639 updates\n",
      "2023-04-17 16:05:57 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:06:09 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:06:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 739 @ 23639 updates, score 6.04) (writing took 12.333298188983463 seconds)\n",
      "2023-04-17 16:06:09 | INFO | fairseq_cli.train | end of epoch 739 (average epoch stats below)\n",
      "2023-04-17 16:06:09 | INFO | train | epoch 739 | loss 2.351 | nll_loss 0.201 | ppl 1.15 | wps 918.8 | ups 1.35 | wpb 678.5 | bsz 2 | num_updates 23639 | lr 2.12683e-05 | gnorm 2.47 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 18850\n",
      "2023-04-17 16:06:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:06:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:06:09 | INFO | fairseq.trainer | begin training epoch 740\n",
      "2023-04-17 16:06:09 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:06:21 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:06:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:06:21 | INFO | valid | epoch 740 | valid on 'valid' subset | loss 6.008 | nll_loss 4.311 | ppl 19.85 | wps 7294 | wpb 290.8 | bsz 1 | num_updates 23671 | best_loss 3.979\n",
      "2023-04-17 16:06:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 740 @ 23671 updates\n",
      "2023-04-17 16:06:21 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:06:33 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:06:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 740 @ 23671 updates, score 6.008) (writing took 12.155270377988927 seconds)\n",
      "2023-04-17 16:06:33 | INFO | fairseq_cli.train | end of epoch 740 (average epoch stats below)\n",
      "2023-04-17 16:06:33 | INFO | train | epoch 740 | loss 2.348 | nll_loss 0.198 | ppl 1.15 | wps 924.7 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 23671 | lr 2.12562e-05 | gnorm 1.98 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 18873\n",
      "2023-04-17 16:06:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:06:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:06:33 | INFO | fairseq.trainer | begin training epoch 741\n",
      "2023-04-17 16:06:33 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:06:43 | INFO | train_inner | epoch 741:     29 / 32 loss=2.349, nll_loss=0.199, ppl=1.15, wps=942.6, ups=1.39, wpb=677.8, bsz=2, num_updates=23700, lr=2.12453e-05, gnorm=2.165, clip=100, loss_scale=1, train_wall=34, gb_free=13.9, wall=18883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 16:06:44 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:06:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:06:44 | INFO | valid | epoch 741 | valid on 'valid' subset | loss 6.07 | nll_loss 4.381 | ppl 20.84 | wps 7108.7 | wpb 290.8 | bsz 1 | num_updates 23703 | best_loss 3.979\n",
      "2023-04-17 16:06:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 741 @ 23703 updates\n",
      "2023-04-17 16:06:44 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:06:56 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:06:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 741 @ 23703 updates, score 6.07) (writing took 12.204819057020359 seconds)\n",
      "2023-04-17 16:06:57 | INFO | fairseq_cli.train | end of epoch 741 (average epoch stats below)\n",
      "2023-04-17 16:06:57 | INFO | train | epoch 741 | loss 2.347 | nll_loss 0.197 | ppl 1.15 | wps 921.4 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 23703 | lr 2.12442e-05 | gnorm 2.009 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 18897\n",
      "2023-04-17 16:06:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:06:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:06:57 | INFO | fairseq.trainer | begin training epoch 742\n",
      "2023-04-17 16:06:57 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:07:08 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:07:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:07:08 | INFO | valid | epoch 742 | valid on 'valid' subset | loss 6.094 | nll_loss 4.416 | ppl 21.34 | wps 7142 | wpb 290.8 | bsz 1 | num_updates 23735 | best_loss 3.979\n",
      "2023-04-17 16:07:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 742 @ 23735 updates\n",
      "2023-04-17 16:07:08 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:07:20 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:07:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 742 @ 23735 updates, score 6.094) (writing took 12.59790890198201 seconds)\n",
      "2023-04-17 16:07:20 | INFO | fairseq_cli.train | end of epoch 742 (average epoch stats below)\n",
      "2023-04-17 16:07:20 | INFO | train | epoch 742 | loss 2.345 | nll_loss 0.194 | ppl 1.14 | wps 907.1 | ups 1.34 | wpb 678.5 | bsz 2 | num_updates 23735 | lr 2.12321e-05 | gnorm 2.005 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 18921\n",
      "2023-04-17 16:07:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:07:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:07:20 | INFO | fairseq.trainer | begin training epoch 743\n",
      "2023-04-17 16:07:20 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:07:32 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:07:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:07:32 | INFO | valid | epoch 743 | valid on 'valid' subset | loss 6.139 | nll_loss 4.455 | ppl 21.93 | wps 7191 | wpb 290.8 | bsz 1 | num_updates 23767 | best_loss 3.979\n",
      "2023-04-17 16:07:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 743 @ 23767 updates\n",
      "2023-04-17 16:07:32 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:07:44 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:07:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 743 @ 23767 updates, score 6.139) (writing took 12.25430446898099 seconds)\n",
      "2023-04-17 16:07:44 | INFO | fairseq_cli.train | end of epoch 743 (average epoch stats below)\n",
      "2023-04-17 16:07:44 | INFO | train | epoch 743 | loss 2.35 | nll_loss 0.201 | ppl 1.15 | wps 911.5 | ups 1.34 | wpb 678.5 | bsz 2 | num_updates 23767 | lr 2.122e-05 | gnorm 2.358 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 18945\n",
      "2023-04-17 16:07:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:07:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:07:44 | INFO | fairseq.trainer | begin training epoch 744\n",
      "2023-04-17 16:07:44 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:07:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:07:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:07:56 | INFO | valid | epoch 744 | valid on 'valid' subset | loss 6.178 | nll_loss 4.494 | ppl 22.53 | wps 7216.5 | wpb 290.8 | bsz 1 | num_updates 23799 | best_loss 3.979\n",
      "2023-04-17 16:07:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 744 @ 23799 updates\n",
      "2023-04-17 16:07:56 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:08:08 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:08:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 744 @ 23799 updates, score 6.178) (writing took 12.197495154105127 seconds)\n",
      "2023-04-17 16:08:08 | INFO | fairseq_cli.train | end of epoch 744 (average epoch stats below)\n",
      "2023-04-17 16:08:08 | INFO | train | epoch 744 | loss 2.345 | nll_loss 0.195 | ppl 1.15 | wps 920.9 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 23799 | lr 2.12079e-05 | gnorm 1.972 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 18968\n",
      "2023-04-17 16:08:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:08:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:08:08 | INFO | fairseq.trainer | begin training epoch 745\n",
      "2023-04-17 16:08:08 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:08:08 | INFO | train_inner | epoch 745:      1 / 32 loss=2.347, nll_loss=0.197, ppl=1.15, wps=793.2, ups=1.18, wpb=675, bsz=2, num_updates=23800, lr=2.12075e-05, gnorm=2.096, clip=100, loss_scale=1, train_wall=34, gb_free=13.9, wall=18969\n",
      "2023-04-17 16:08:19 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:08:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:08:19 | INFO | valid | epoch 745 | valid on 'valid' subset | loss 6.11 | nll_loss 4.43 | ppl 21.55 | wps 7237.8 | wpb 290.8 | bsz 1 | num_updates 23831 | best_loss 3.979\n",
      "2023-04-17 16:08:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 745 @ 23831 updates\n",
      "2023-04-17 16:08:19 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:08:31 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:08:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 745 @ 23831 updates, score 6.11) (writing took 12.24999599903822 seconds)\n",
      "2023-04-17 16:08:31 | INFO | fairseq_cli.train | end of epoch 745 (average epoch stats below)\n",
      "2023-04-17 16:08:31 | INFO | train | epoch 745 | loss 2.35 | nll_loss 0.199 | ppl 1.15 | wps 920.8 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 23831 | lr 2.11958e-05 | gnorm 2.368 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 18992\n",
      "2023-04-17 16:08:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:08:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:08:31 | INFO | fairseq.trainer | begin training epoch 746\n",
      "2023-04-17 16:08:31 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:08:43 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:08:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 16:08:43 | INFO | valid | epoch 746 | valid on 'valid' subset | loss 6.143 | nll_loss 4.464 | ppl 22.07 | wps 7232.9 | wpb 290.8 | bsz 1 | num_updates 23863 | best_loss 3.979\n",
      "2023-04-17 16:08:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 746 @ 23863 updates\n",
      "2023-04-17 16:08:43 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:08:55 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:08:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 746 @ 23863 updates, score 6.143) (writing took 12.099974898039363 seconds)\n",
      "2023-04-17 16:08:55 | INFO | fairseq_cli.train | end of epoch 746 (average epoch stats below)\n",
      "2023-04-17 16:08:55 | INFO | train | epoch 746 | loss 2.345 | nll_loss 0.196 | ppl 1.15 | wps 925.3 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 23863 | lr 2.11838e-05 | gnorm 2.126 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 19015\n",
      "2023-04-17 16:08:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:08:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:08:55 | INFO | fairseq.trainer | begin training epoch 747\n",
      "2023-04-17 16:08:55 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:09:06 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:09:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:09:07 | INFO | valid | epoch 747 | valid on 'valid' subset | loss 6.117 | nll_loss 4.438 | ppl 21.67 | wps 7115.7 | wpb 290.8 | bsz 1 | num_updates 23895 | best_loss 3.979\n",
      "2023-04-17 16:09:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 747 @ 23895 updates\n",
      "2023-04-17 16:09:07 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:09:15 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:09:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 747 @ 23895 updates, score 6.117) (writing took 8.952787694055587 seconds)\n",
      "2023-04-17 16:09:15 | INFO | fairseq_cli.train | end of epoch 747 (average epoch stats below)\n",
      "2023-04-17 16:09:15 | INFO | train | epoch 747 | loss 2.346 | nll_loss 0.198 | ppl 1.15 | wps 1056.5 | ups 1.56 | wpb 678.5 | bsz 2 | num_updates 23895 | lr 2.11717e-05 | gnorm 2.027 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 19036\n",
      "2023-04-17 16:09:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:09:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:09:15 | INFO | fairseq.trainer | begin training epoch 748\n",
      "2023-04-17 16:09:15 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:09:17 | INFO | train_inner | epoch 748:      5 / 32 loss=2.347, nll_loss=0.198, ppl=1.15, wps=989.6, ups=1.45, wpb=683.4, bsz=2, num_updates=23900, lr=2.11698e-05, gnorm=2.187, clip=100, loss_scale=1, train_wall=35, gb_free=13.9, wall=19038\n",
      "2023-04-17 16:09:27 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:09:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:09:27 | INFO | valid | epoch 748 | valid on 'valid' subset | loss 6.134 | nll_loss 4.445 | ppl 21.78 | wps 7166.9 | wpb 290.8 | bsz 1 | num_updates 23927 | best_loss 3.979\n",
      "2023-04-17 16:09:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 748 @ 23927 updates\n",
      "2023-04-17 16:09:27 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:09:39 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:09:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 748 @ 23927 updates, score 6.134) (writing took 12.201979720033705 seconds)\n",
      "2023-04-17 16:09:39 | INFO | fairseq_cli.train | end of epoch 748 (average epoch stats below)\n",
      "2023-04-17 16:09:39 | INFO | train | epoch 748 | loss 2.346 | nll_loss 0.198 | ppl 1.15 | wps 918.8 | ups 1.35 | wpb 678.5 | bsz 2 | num_updates 23927 | lr 2.11596e-05 | gnorm 2.232 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 19059\n",
      "2023-04-17 16:09:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:09:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:09:39 | INFO | fairseq.trainer | begin training epoch 749\n",
      "2023-04-17 16:09:39 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:09:50 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:09:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:09:51 | INFO | valid | epoch 749 | valid on 'valid' subset | loss 6.115 | nll_loss 4.435 | ppl 21.62 | wps 7139.3 | wpb 290.8 | bsz 1 | num_updates 23959 | best_loss 3.979\n",
      "2023-04-17 16:09:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 749 @ 23959 updates\n",
      "2023-04-17 16:09:51 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:10:03 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:10:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 749 @ 23959 updates, score 6.115) (writing took 12.435245504020713 seconds)\n",
      "2023-04-17 16:10:03 | INFO | fairseq_cli.train | end of epoch 749 (average epoch stats below)\n",
      "2023-04-17 16:10:03 | INFO | train | epoch 749 | loss 2.348 | nll_loss 0.198 | ppl 1.15 | wps 909.7 | ups 1.34 | wpb 678.5 | bsz 2 | num_updates 23959 | lr 2.11475e-05 | gnorm 2.174 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 19083\n",
      "2023-04-17 16:10:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:10:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:10:03 | INFO | fairseq.trainer | begin training epoch 750\n",
      "2023-04-17 16:10:03 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:10:14 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:10:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:10:14 | INFO | valid | epoch 750 | valid on 'valid' subset | loss 6.02 | nll_loss 4.338 | ppl 20.22 | wps 7047.6 | wpb 290.8 | bsz 1 | num_updates 23991 | best_loss 3.979\n",
      "2023-04-17 16:10:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 750 @ 23991 updates\n",
      "2023-04-17 16:10:14 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:10:26 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:10:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 750 @ 23991 updates, score 6.02) (writing took 12.20781979500316 seconds)\n",
      "2023-04-17 16:10:27 | INFO | fairseq_cli.train | end of epoch 750 (average epoch stats below)\n",
      "2023-04-17 16:10:27 | INFO | train | epoch 750 | loss 2.346 | nll_loss 0.197 | ppl 1.15 | wps 916.2 | ups 1.35 | wpb 678.5 | bsz 2 | num_updates 23991 | lr 2.11355e-05 | gnorm 2.344 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 19107\n",
      "2023-04-17 16:10:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:10:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:10:27 | INFO | fairseq.trainer | begin training epoch 751\n",
      "2023-04-17 16:10:27 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:10:30 | INFO | train_inner | epoch 751:      9 / 32 loss=2.346, nll_loss=0.196, ppl=1.15, wps=936.9, ups=1.38, wpb=679.7, bsz=2, num_updates=24000, lr=2.11321e-05, gnorm=2.193, clip=100, loss_scale=1, train_wall=35, gb_free=13.9, wall=19110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 16:10:38 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:10:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:10:38 | INFO | valid | epoch 751 | valid on 'valid' subset | loss 6.033 | nll_loss 4.34 | ppl 20.25 | wps 7182.3 | wpb 290.8 | bsz 1 | num_updates 24023 | best_loss 3.979\n",
      "2023-04-17 16:10:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 751 @ 24023 updates\n",
      "2023-04-17 16:10:38 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:10:50 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:10:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 751 @ 24023 updates, score 6.033) (writing took 12.326913947006688 seconds)\n",
      "2023-04-17 16:10:51 | INFO | fairseq_cli.train | end of epoch 751 (average epoch stats below)\n",
      "2023-04-17 16:10:51 | INFO | train | epoch 751 | loss 2.346 | nll_loss 0.196 | ppl 1.15 | wps 910.8 | ups 1.34 | wpb 678.5 | bsz 2 | num_updates 24023 | lr 2.11234e-05 | gnorm 2.263 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 19131\n",
      "2023-04-17 16:10:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:10:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:10:51 | INFO | fairseq.trainer | begin training epoch 752\n",
      "2023-04-17 16:10:51 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:11:02 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:11:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:11:02 | INFO | valid | epoch 752 | valid on 'valid' subset | loss 6.128 | nll_loss 4.461 | ppl 22.03 | wps 6879.8 | wpb 290.8 | bsz 1 | num_updates 24055 | best_loss 3.979\n",
      "2023-04-17 16:11:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 752 @ 24055 updates\n",
      "2023-04-17 16:11:02 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:11:14 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:11:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 752 @ 24055 updates, score 6.128) (writing took 12.38494171097409 seconds)\n",
      "2023-04-17 16:11:15 | INFO | fairseq_cli.train | end of epoch 752 (average epoch stats below)\n",
      "2023-04-17 16:11:15 | INFO | train | epoch 752 | loss 2.344 | nll_loss 0.194 | ppl 1.14 | wps 902.4 | ups 1.33 | wpb 678.5 | bsz 2 | num_updates 24055 | lr 2.11113e-05 | gnorm 1.984 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 19155\n",
      "2023-04-17 16:11:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:11:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:11:15 | INFO | fairseq.trainer | begin training epoch 753\n",
      "2023-04-17 16:11:15 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:11:26 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:11:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:11:26 | INFO | valid | epoch 753 | valid on 'valid' subset | loss 6.059 | nll_loss 4.366 | ppl 20.63 | wps 7051.8 | wpb 290.8 | bsz 1 | num_updates 24087 | best_loss 3.979\n",
      "2023-04-17 16:11:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 753 @ 24087 updates\n",
      "2023-04-17 16:11:26 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:11:38 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:11:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 753 @ 24087 updates, score 6.059) (writing took 12.314196518040262 seconds)\n",
      "2023-04-17 16:11:38 | INFO | fairseq_cli.train | end of epoch 753 (average epoch stats below)\n",
      "2023-04-17 16:11:38 | INFO | train | epoch 753 | loss 2.347 | nll_loss 0.199 | ppl 1.15 | wps 914.4 | ups 1.35 | wpb 678.5 | bsz 2 | num_updates 24087 | lr 2.10992e-05 | gnorm 2.23 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 19179\n",
      "2023-04-17 16:11:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:11:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:11:38 | INFO | fairseq.trainer | begin training epoch 754\n",
      "2023-04-17 16:11:38 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:11:43 | INFO | train_inner | epoch 754:     13 / 32 loss=2.346, nll_loss=0.197, ppl=1.15, wps=933, ups=1.37, wpb=681.6, bsz=2, num_updates=24100, lr=2.10943e-05, gnorm=2.256, clip=100, loss_scale=1, train_wall=35, gb_free=13.9, wall=19183\n",
      "2023-04-17 16:11:49 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:11:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:11:50 | INFO | valid | epoch 754 | valid on 'valid' subset | loss 6.109 | nll_loss 4.429 | ppl 21.54 | wps 7183.5 | wpb 290.8 | bsz 1 | num_updates 24119 | best_loss 3.979\n",
      "2023-04-17 16:11:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 754 @ 24119 updates\n",
      "2023-04-17 16:11:50 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:12:01 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:12:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 754 @ 24119 updates, score 6.109) (writing took 12.023836838081479 seconds)\n",
      "2023-04-17 16:12:02 | INFO | fairseq_cli.train | end of epoch 754 (average epoch stats below)\n",
      "2023-04-17 16:12:02 | INFO | train | epoch 754 | loss 2.346 | nll_loss 0.197 | ppl 1.15 | wps 928.9 | ups 1.37 | wpb 678.5 | bsz 2 | num_updates 24119 | lr 2.10872e-05 | gnorm 2.275 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 19202\n",
      "2023-04-17 16:12:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:12:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:12:02 | INFO | fairseq.trainer | begin training epoch 755\n",
      "2023-04-17 16:12:02 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:12:13 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:12:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:12:13 | INFO | valid | epoch 755 | valid on 'valid' subset | loss 5.998 | nll_loss 4.29 | ppl 19.56 | wps 7242.7 | wpb 290.8 | bsz 1 | num_updates 24151 | best_loss 3.979\n",
      "2023-04-17 16:12:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 755 @ 24151 updates\n",
      "2023-04-17 16:12:13 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:12:25 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:12:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 755 @ 24151 updates, score 5.998) (writing took 12.311963754007593 seconds)\n",
      "2023-04-17 16:12:25 | INFO | fairseq_cli.train | end of epoch 755 (average epoch stats below)\n",
      "2023-04-17 16:12:25 | INFO | train | epoch 755 | loss 2.349 | nll_loss 0.199 | ppl 1.15 | wps 917.2 | ups 1.35 | wpb 678.5 | bsz 2 | num_updates 24151 | lr 2.10751e-05 | gnorm 2.4 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 19226\n",
      "2023-04-17 16:12:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:12:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:12:25 | INFO | fairseq.trainer | begin training epoch 756\n",
      "2023-04-17 16:12:25 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:12:37 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:12:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 16:12:37 | INFO | valid | epoch 756 | valid on 'valid' subset | loss 6.082 | nll_loss 4.397 | ppl 21.07 | wps 7191 | wpb 290.8 | bsz 1 | num_updates 24183 | best_loss 3.979\n",
      "2023-04-17 16:12:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 756 @ 24183 updates\n",
      "2023-04-17 16:12:37 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:12:49 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:12:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 756 @ 24183 updates, score 6.082) (writing took 12.129850815981627 seconds)\n",
      "2023-04-17 16:12:49 | INFO | fairseq_cli.train | end of epoch 756 (average epoch stats below)\n",
      "2023-04-17 16:12:49 | INFO | train | epoch 756 | loss 2.343 | nll_loss 0.195 | ppl 1.15 | wps 923.6 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 24183 | lr 2.1063e-05 | gnorm 2.007 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 19249\n",
      "2023-04-17 16:12:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:12:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:12:49 | INFO | fairseq.trainer | begin training epoch 757\n",
      "2023-04-17 16:12:49 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:12:55 | INFO | train_inner | epoch 757:     17 / 32 loss=2.346, nll_loss=0.197, ppl=1.15, wps=922.6, ups=1.39, wpb=663.8, bsz=2, num_updates=24200, lr=2.10566e-05, gnorm=2.199, clip=100, loss_scale=1, train_wall=34, gb_free=13.9, wall=19255\n",
      "2023-04-17 16:13:00 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:13:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:13:00 | INFO | valid | epoch 757 | valid on 'valid' subset | loss 6.103 | nll_loss 4.422 | ppl 21.44 | wps 7210.7 | wpb 290.8 | bsz 1 | num_updates 24215 | best_loss 3.979\n",
      "2023-04-17 16:13:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 757 @ 24215 updates\n",
      "2023-04-17 16:13:00 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:13:12 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:13:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 757 @ 24215 updates, score 6.103) (writing took 12.26138381997589 seconds)\n",
      "2023-04-17 16:13:12 | INFO | fairseq_cli.train | end of epoch 757 (average epoch stats below)\n",
      "2023-04-17 16:13:12 | INFO | train | epoch 757 | loss 2.347 | nll_loss 0.197 | ppl 1.15 | wps 920.6 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 24215 | lr 2.10509e-05 | gnorm 2.24 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 19273\n",
      "2023-04-17 16:13:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:13:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:13:12 | INFO | fairseq.trainer | begin training epoch 758\n",
      "2023-04-17 16:13:12 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:13:24 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:13:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:13:25 | INFO | valid | epoch 758 | valid on 'valid' subset | loss 6.11 | nll_loss 4.434 | ppl 21.61 | wps 6128.5 | wpb 290.8 | bsz 1 | num_updates 24247 | best_loss 3.979\n",
      "2023-04-17 16:13:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 758 @ 24247 updates\n",
      "2023-04-17 16:13:25 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:13:36 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:13:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 758 @ 24247 updates, score 6.11) (writing took 12.118021380039863 seconds)\n",
      "2023-04-17 16:13:37 | INFO | fairseq_cli.train | end of epoch 758 (average epoch stats below)\n",
      "2023-04-17 16:13:37 | INFO | train | epoch 758 | loss 2.346 | nll_loss 0.194 | ppl 1.14 | wps 895.6 | ups 1.32 | wpb 678.5 | bsz 2 | num_updates 24247 | lr 2.10389e-05 | gnorm 2.075 | clip 100 | loss_scale 1 | train_wall 12 | gb_free 13.9 | wall 19297\n",
      "2023-04-17 16:13:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:13:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:13:37 | INFO | fairseq.trainer | begin training epoch 759\n",
      "2023-04-17 16:13:37 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:13:48 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:13:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:13:49 | INFO | valid | epoch 759 | valid on 'valid' subset | loss 6.121 | nll_loss 4.432 | ppl 21.59 | wps 6216.3 | wpb 290.8 | bsz 1 | num_updates 24279 | best_loss 3.979\n",
      "2023-04-17 16:13:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 759 @ 24279 updates\n",
      "2023-04-17 16:13:49 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:14:01 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:14:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 759 @ 24279 updates, score 6.121) (writing took 12.431649295031093 seconds)\n",
      "2023-04-17 16:14:01 | INFO | fairseq_cli.train | end of epoch 759 (average epoch stats below)\n",
      "2023-04-17 16:14:01 | INFO | train | epoch 759 | loss 2.345 | nll_loss 0.196 | ppl 1.15 | wps 886.7 | ups 1.31 | wpb 678.5 | bsz 2 | num_updates 24279 | lr 2.10268e-05 | gnorm 1.99 | clip 100 | loss_scale 1 | train_wall 12 | gb_free 13.9 | wall 19321\n",
      "2023-04-17 16:14:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:14:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:14:01 | INFO | fairseq.trainer | begin training epoch 760\n",
      "2023-04-17 16:14:01 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:14:09 | INFO | train_inner | epoch 760:     21 / 32 loss=2.345, nll_loss=0.195, ppl=1.14, wps=924.8, ups=1.35, wpb=686.2, bsz=2, num_updates=24300, lr=2.10189e-05, gnorm=2.006, clip=100, loss_scale=1, train_wall=36, gb_free=13.9, wall=19329\n",
      "2023-04-17 16:14:13 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:14:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:14:13 | INFO | valid | epoch 760 | valid on 'valid' subset | loss 6.049 | nll_loss 4.358 | ppl 20.5 | wps 6161.1 | wpb 290.8 | bsz 1 | num_updates 24311 | best_loss 3.979\n",
      "2023-04-17 16:14:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 760 @ 24311 updates\n",
      "2023-04-17 16:14:13 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:14:25 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:14:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 760 @ 24311 updates, score 6.049) (writing took 12.229012051015161 seconds)\n",
      "2023-04-17 16:14:26 | INFO | fairseq_cli.train | end of epoch 760 (average epoch stats below)\n",
      "2023-04-17 16:14:26 | INFO | train | epoch 760 | loss 2.346 | nll_loss 0.195 | ppl 1.14 | wps 891.6 | ups 1.31 | wpb 678.5 | bsz 2 | num_updates 24311 | lr 2.10147e-05 | gnorm 2.118 | clip 100 | loss_scale 1 | train_wall 12 | gb_free 13.9 | wall 19346\n",
      "2023-04-17 16:14:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:14:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:14:26 | INFO | fairseq.trainer | begin training epoch 761\n",
      "2023-04-17 16:14:26 | INFO | fairseq_cli.train | Start iterating over samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 16:14:36 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:14:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:14:36 | INFO | valid | epoch 761 | valid on 'valid' subset | loss 6.091 | nll_loss 4.411 | ppl 21.27 | wps 7186.3 | wpb 290.8 | bsz 1 | num_updates 24343 | best_loss 3.979\n",
      "2023-04-17 16:14:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 761 @ 24343 updates\n",
      "2023-04-17 16:14:36 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:14:48 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:14:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 761 @ 24343 updates, score 6.091) (writing took 12.311095161014237 seconds)\n",
      "2023-04-17 16:14:49 | INFO | fairseq_cli.train | end of epoch 761 (average epoch stats below)\n",
      "2023-04-17 16:14:49 | INFO | train | epoch 761 | loss 2.346 | nll_loss 0.196 | ppl 1.15 | wps 936.6 | ups 1.38 | wpb 678.5 | bsz 2 | num_updates 24343 | lr 2.10026e-05 | gnorm 2.205 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 19369\n",
      "2023-04-17 16:14:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:14:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:14:49 | INFO | fairseq.trainer | begin training epoch 762\n",
      "2023-04-17 16:14:49 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:15:00 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:15:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:15:00 | INFO | valid | epoch 762 | valid on 'valid' subset | loss 6.093 | nll_loss 4.392 | ppl 21 | wps 7254.9 | wpb 290.8 | bsz 1 | num_updates 24375 | best_loss 3.979\n",
      "2023-04-17 16:15:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 762 @ 24375 updates\n",
      "2023-04-17 16:15:00 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:15:12 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:15:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 762 @ 24375 updates, score 6.093) (writing took 12.040928823989816 seconds)\n",
      "2023-04-17 16:15:12 | INFO | fairseq_cli.train | end of epoch 762 (average epoch stats below)\n",
      "2023-04-17 16:15:12 | INFO | train | epoch 762 | loss 2.347 | nll_loss 0.2 | ppl 1.15 | wps 923 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 24375 | lr 2.09906e-05 | gnorm 2.24 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 19392\n",
      "2023-04-17 16:15:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:15:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:15:12 | INFO | fairseq.trainer | begin training epoch 763\n",
      "2023-04-17 16:15:12 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:15:21 | INFO | train_inner | epoch 763:     25 / 32 loss=2.347, nll_loss=0.197, ppl=1.15, wps=938.1, ups=1.39, wpb=675.9, bsz=2, num_updates=24400, lr=2.09811e-05, gnorm=2.238, clip=100, loss_scale=1, train_wall=34, gb_free=13.9, wall=19401\n",
      "2023-04-17 16:15:23 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:15:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:15:24 | INFO | valid | epoch 763 | valid on 'valid' subset | loss 6.077 | nll_loss 4.383 | ppl 20.87 | wps 7230.4 | wpb 290.8 | bsz 1 | num_updates 24407 | best_loss 3.979\n",
      "2023-04-17 16:15:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 763 @ 24407 updates\n",
      "2023-04-17 16:15:24 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:15:36 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:15:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 763 @ 24407 updates, score 6.077) (writing took 12.15478094201535 seconds)\n",
      "2023-04-17 16:15:36 | INFO | fairseq_cli.train | end of epoch 763 (average epoch stats below)\n",
      "2023-04-17 16:15:36 | INFO | train | epoch 763 | loss 2.346 | nll_loss 0.197 | ppl 1.15 | wps 922.4 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 24407 | lr 2.09785e-05 | gnorm 2.027 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 19416\n",
      "2023-04-17 16:15:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:15:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:15:36 | INFO | fairseq.trainer | begin training epoch 764\n",
      "2023-04-17 16:15:36 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:15:47 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:15:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:15:48 | INFO | valid | epoch 764 | valid on 'valid' subset | loss 6.162 | nll_loss 4.491 | ppl 22.48 | wps 6224.6 | wpb 290.8 | bsz 1 | num_updates 24439 | best_loss 3.979\n",
      "2023-04-17 16:15:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 764 @ 24439 updates\n",
      "2023-04-17 16:15:48 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:16:00 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:16:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 764 @ 24439 updates, score 6.162) (writing took 12.110050780000165 seconds)\n",
      "2023-04-17 16:16:00 | INFO | fairseq_cli.train | end of epoch 764 (average epoch stats below)\n",
      "2023-04-17 16:16:00 | INFO | train | epoch 764 | loss 2.342 | nll_loss 0.192 | ppl 1.14 | wps 907.5 | ups 1.34 | wpb 678.5 | bsz 2 | num_updates 24439 | lr 2.09664e-05 | gnorm 1.911 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 19440\n",
      "2023-04-17 16:16:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:16:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:16:00 | INFO | fairseq.trainer | begin training epoch 765\n",
      "2023-04-17 16:16:00 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:16:11 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:16:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:16:11 | INFO | valid | epoch 765 | valid on 'valid' subset | loss 6.131 | nll_loss 4.453 | ppl 21.91 | wps 7210.1 | wpb 290.8 | bsz 1 | num_updates 24471 | best_loss 3.979\n",
      "2023-04-17 16:16:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 765 @ 24471 updates\n",
      "2023-04-17 16:16:11 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:16:23 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:16:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 765 @ 24471 updates, score 6.131) (writing took 12.285358664928935 seconds)\n",
      "2023-04-17 16:16:23 | INFO | fairseq_cli.train | end of epoch 765 (average epoch stats below)\n",
      "2023-04-17 16:16:23 | INFO | train | epoch 765 | loss 2.343 | nll_loss 0.194 | ppl 1.14 | wps 914.4 | ups 1.35 | wpb 678.5 | bsz 2 | num_updates 24471 | lr 2.09543e-05 | gnorm 2.218 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 19464\n",
      "2023-04-17 16:16:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:16:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:16:23 | INFO | fairseq.trainer | begin training epoch 766\n",
      "2023-04-17 16:16:23 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:16:33 | INFO | train_inner | epoch 766:     29 / 32 loss=2.343, nll_loss=0.195, ppl=1.14, wps=948.9, ups=1.39, wpb=684.4, bsz=2, num_updates=24500, lr=2.09434e-05, gnorm=2.066, clip=100, loss_scale=1, train_wall=34, gb_free=13.9, wall=19474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 16:16:34 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:16:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:16:34 | INFO | valid | epoch 766 | valid on 'valid' subset | loss 6.169 | nll_loss 4.493 | ppl 22.52 | wps 7146.9 | wpb 290.8 | bsz 1 | num_updates 24503 | best_loss 3.979\n",
      "2023-04-17 16:16:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 766 @ 24503 updates\n",
      "2023-04-17 16:16:34 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:16:46 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:16:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 766 @ 24503 updates, score 6.169) (writing took 12.072677270043641 seconds)\n",
      "2023-04-17 16:16:46 | INFO | fairseq_cli.train | end of epoch 766 (average epoch stats below)\n",
      "2023-04-17 16:16:46 | INFO | train | epoch 766 | loss 2.345 | nll_loss 0.197 | ppl 1.15 | wps 945.2 | ups 1.39 | wpb 678.5 | bsz 2 | num_updates 24503 | lr 2.09423e-05 | gnorm 2.226 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 19487\n",
      "2023-04-17 16:16:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:16:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:16:46 | INFO | fairseq.trainer | begin training epoch 767\n",
      "2023-04-17 16:16:46 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:16:58 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:16:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:16:58 | INFO | valid | epoch 767 | valid on 'valid' subset | loss 6.119 | nll_loss 4.432 | ppl 21.59 | wps 7104.1 | wpb 290.8 | bsz 1 | num_updates 24535 | best_loss 3.979\n",
      "2023-04-17 16:16:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 767 @ 24535 updates\n",
      "2023-04-17 16:16:58 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:17:10 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:17:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 767 @ 24535 updates, score 6.119) (writing took 12.254332898999564 seconds)\n",
      "2023-04-17 16:17:10 | INFO | fairseq_cli.train | end of epoch 767 (average epoch stats below)\n",
      "2023-04-17 16:17:10 | INFO | train | epoch 767 | loss 2.344 | nll_loss 0.194 | ppl 1.14 | wps 918.7 | ups 1.35 | wpb 678.5 | bsz 2 | num_updates 24535 | lr 2.09302e-05 | gnorm 2.389 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 19510\n",
      "2023-04-17 16:17:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:17:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:17:10 | INFO | fairseq.trainer | begin training epoch 768\n",
      "2023-04-17 16:17:10 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:17:21 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:17:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:17:21 | INFO | valid | epoch 768 | valid on 'valid' subset | loss 6.168 | nll_loss 4.489 | ppl 22.45 | wps 7076.1 | wpb 290.8 | bsz 1 | num_updates 24567 | best_loss 3.979\n",
      "2023-04-17 16:17:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 768 @ 24567 updates\n",
      "2023-04-17 16:17:21 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:17:33 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:17:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 768 @ 24567 updates, score 6.168) (writing took 12.143974152975716 seconds)\n",
      "2023-04-17 16:17:34 | INFO | fairseq_cli.train | end of epoch 768 (average epoch stats below)\n",
      "2023-04-17 16:17:34 | INFO | train | epoch 768 | loss 2.347 | nll_loss 0.199 | ppl 1.15 | wps 924.6 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 24567 | lr 2.09181e-05 | gnorm 2.357 | clip 100 | loss_scale 1 | train_wall 11 | gb_free 13.9 | wall 19534\n",
      "2023-04-17 16:17:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:17:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:17:34 | INFO | fairseq.trainer | begin training epoch 769\n",
      "2023-04-17 16:17:34 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:17:45 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:17:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:17:45 | INFO | valid | epoch 769 | valid on 'valid' subset | loss 6.139 | nll_loss 4.458 | ppl 21.98 | wps 7100.7 | wpb 290.8 | bsz 1 | num_updates 24599 | best_loss 3.979\n",
      "2023-04-17 16:17:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 769 @ 24599 updates\n",
      "2023-04-17 16:17:45 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:17:57 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:17:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 769 @ 24599 updates, score 6.139) (writing took 12.281879279995337 seconds)\n",
      "2023-04-17 16:17:57 | INFO | fairseq_cli.train | end of epoch 769 (average epoch stats below)\n",
      "2023-04-17 16:17:57 | INFO | train | epoch 769 | loss 2.343 | nll_loss 0.194 | ppl 1.14 | wps 918.1 | ups 1.35 | wpb 678.5 | bsz 2 | num_updates 24599 | lr 2.0906e-05 | gnorm 2.043 | clip 100 | loss_scale 2 | train_wall 11 | gb_free 13.9 | wall 19557\n",
      "2023-04-17 16:17:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:17:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:17:57 | INFO | fairseq.trainer | begin training epoch 770\n",
      "2023-04-17 16:17:57 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:17:58 | INFO | train_inner | epoch 770:      1 / 32 loss=2.345, nll_loss=0.195, ppl=1.14, wps=795.8, ups=1.19, wpb=671.4, bsz=2, num_updates=24600, lr=2.09057e-05, gnorm=2.297, clip=100, loss_scale=2, train_wall=34, gb_free=13.9, wall=19558\n",
      "2023-04-17 16:18:09 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:18:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:18:09 | INFO | valid | epoch 770 | valid on 'valid' subset | loss 6.112 | nll_loss 4.435 | ppl 21.63 | wps 7008 | wpb 290.8 | bsz 1 | num_updates 24631 | best_loss 3.979\n",
      "2023-04-17 16:18:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 770 @ 24631 updates\n",
      "2023-04-17 16:18:09 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:18:21 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:18:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 770 @ 24631 updates, score 6.112) (writing took 12.066554900025949 seconds)\n",
      "2023-04-17 16:18:21 | INFO | fairseq_cli.train | end of epoch 770 (average epoch stats below)\n",
      "2023-04-17 16:18:21 | INFO | train | epoch 770 | loss 2.343 | nll_loss 0.194 | ppl 1.14 | wps 897.9 | ups 1.32 | wpb 678.5 | bsz 2 | num_updates 24631 | lr 2.0894e-05 | gnorm 2.054 | clip 100 | loss_scale 2 | train_wall 12 | gb_free 13.9 | wall 19582\n",
      "2023-04-17 16:18:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:18:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:18:21 | INFO | fairseq.trainer | begin training epoch 771\n",
      "2023-04-17 16:18:21 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:18:32 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:18:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 16:18:32 | INFO | valid | epoch 771 | valid on 'valid' subset | loss 6.09 | nll_loss 4.403 | ppl 21.15 | wps 7241.2 | wpb 290.8 | bsz 1 | num_updates 24663 | best_loss 3.979\n",
      "2023-04-17 16:18:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 771 @ 24663 updates\n",
      "2023-04-17 16:18:32 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:18:45 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:18:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 771 @ 24663 updates, score 6.09) (writing took 12.369480177061632 seconds)\n",
      "2023-04-17 16:18:45 | INFO | fairseq_cli.train | end of epoch 771 (average epoch stats below)\n",
      "2023-04-17 16:18:45 | INFO | train | epoch 771 | loss 2.342 | nll_loss 0.193 | ppl 1.14 | wps 928.1 | ups 1.37 | wpb 678.5 | bsz 2 | num_updates 24663 | lr 2.08819e-05 | gnorm 1.973 | clip 100 | loss_scale 2 | train_wall 11 | gb_free 13.9 | wall 19605\n",
      "2023-04-17 16:18:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:18:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:18:45 | INFO | fairseq.trainer | begin training epoch 772\n",
      "2023-04-17 16:18:45 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:18:56 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:18:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:18:56 | INFO | valid | epoch 772 | valid on 'valid' subset | loss 6.151 | nll_loss 4.473 | ppl 22.2 | wps 6698.8 | wpb 290.8 | bsz 1 | num_updates 24695 | best_loss 3.979\n",
      "2023-04-17 16:18:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 772 @ 24695 updates\n",
      "2023-04-17 16:18:56 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:19:09 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:19:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 772 @ 24695 updates, score 6.151) (writing took 12.963027723017149 seconds)\n",
      "2023-04-17 16:19:09 | INFO | fairseq_cli.train | end of epoch 772 (average epoch stats below)\n",
      "2023-04-17 16:19:09 | INFO | train | epoch 772 | loss 2.344 | nll_loss 0.196 | ppl 1.15 | wps 886.7 | ups 1.31 | wpb 678.5 | bsz 2 | num_updates 24695 | lr 2.08698e-05 | gnorm 2.101 | clip 100 | loss_scale 2 | train_wall 11 | gb_free 13.9 | wall 19630\n",
      "2023-04-17 16:19:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:19:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:19:09 | INFO | fairseq.trainer | begin training epoch 773\n",
      "2023-04-17 16:19:09 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:19:11 | INFO | train_inner | epoch 773:      5 / 32 loss=2.343, nll_loss=0.194, ppl=1.14, wps=930, ups=1.36, wpb=683.5, bsz=2, num_updates=24700, lr=2.08679e-05, gnorm=2.02, clip=100, loss_scale=2, train_wall=35, gb_free=13.9, wall=19631\n",
      "2023-04-17 16:19:21 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:19:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:19:21 | INFO | valid | epoch 773 | valid on 'valid' subset | loss 6.143 | nll_loss 4.462 | ppl 22.04 | wps 6639.2 | wpb 290.8 | bsz 1 | num_updates 24727 | best_loss 3.979\n",
      "2023-04-17 16:19:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 773 @ 24727 updates\n",
      "2023-04-17 16:19:21 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:19:33 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:19:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 773 @ 24727 updates, score 6.143) (writing took 12.249693813035265 seconds)\n",
      "2023-04-17 16:19:33 | INFO | fairseq_cli.train | end of epoch 773 (average epoch stats below)\n",
      "2023-04-17 16:19:33 | INFO | train | epoch 773 | loss 2.341 | nll_loss 0.193 | ppl 1.14 | wps 899.9 | ups 1.33 | wpb 678.5 | bsz 2 | num_updates 24727 | lr 2.08577e-05 | gnorm 1.887 | clip 100 | loss_scale 2 | train_wall 11 | gb_free 13.9 | wall 19654\n",
      "2023-04-17 16:19:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:19:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:19:33 | INFO | fairseq.trainer | begin training epoch 774\n",
      "2023-04-17 16:19:33 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:19:45 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:19:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:19:45 | INFO | valid | epoch 774 | valid on 'valid' subset | loss 6.15 | nll_loss 4.482 | ppl 22.35 | wps 7202.3 | wpb 290.8 | bsz 1 | num_updates 24759 | best_loss 3.979\n",
      "2023-04-17 16:19:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 774 @ 24759 updates\n",
      "2023-04-17 16:19:45 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:19:57 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:19:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 774 @ 24759 updates, score 6.15) (writing took 12.093490013037808 seconds)\n",
      "2023-04-17 16:19:57 | INFO | fairseq_cli.train | end of epoch 774 (average epoch stats below)\n",
      "2023-04-17 16:19:57 | INFO | train | epoch 774 | loss 2.342 | nll_loss 0.191 | ppl 1.14 | wps 923.9 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 24759 | lr 2.08457e-05 | gnorm 2.026 | clip 100 | loss_scale 2 | train_wall 11 | gb_free 13.9 | wall 19677\n",
      "2023-04-17 16:19:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:19:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:19:57 | INFO | fairseq.trainer | begin training epoch 775\n",
      "2023-04-17 16:19:57 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:20:08 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:20:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:20:08 | INFO | valid | epoch 775 | valid on 'valid' subset | loss 6.067 | nll_loss 4.375 | ppl 20.75 | wps 7168 | wpb 290.8 | bsz 1 | num_updates 24791 | best_loss 3.979\n",
      "2023-04-17 16:20:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 775 @ 24791 updates\n",
      "2023-04-17 16:20:08 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:20:20 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:20:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 775 @ 24791 updates, score 6.067) (writing took 12.193259788909927 seconds)\n",
      "2023-04-17 16:20:21 | INFO | fairseq_cli.train | end of epoch 775 (average epoch stats below)\n",
      "2023-04-17 16:20:21 | INFO | train | epoch 775 | loss 2.344 | nll_loss 0.196 | ppl 1.15 | wps 919.5 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 24791 | lr 2.08336e-05 | gnorm 2.079 | clip 100 | loss_scale 2 | train_wall 11 | gb_free 13.9 | wall 19701\n",
      "2023-04-17 16:20:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:20:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:20:21 | INFO | fairseq.trainer | begin training epoch 776\n",
      "2023-04-17 16:20:21 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:20:24 | INFO | train_inner | epoch 776:      9 / 32 loss=2.343, nll_loss=0.194, ppl=1.14, wps=932, ups=1.38, wpb=677.5, bsz=2, num_updates=24800, lr=2.08302e-05, gnorm=2.008, clip=100, loss_scale=2, train_wall=35, gb_free=13.9, wall=19704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 16:20:32 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:20:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:20:32 | INFO | valid | epoch 776 | valid on 'valid' subset | loss 6.117 | nll_loss 4.436 | ppl 21.65 | wps 7082.6 | wpb 290.8 | bsz 1 | num_updates 24823 | best_loss 3.979\n",
      "2023-04-17 16:20:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 776 @ 24823 updates\n",
      "2023-04-17 16:20:32 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:20:44 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:20:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 776 @ 24823 updates, score 6.117) (writing took 12.125574914971367 seconds)\n",
      "2023-04-17 16:20:44 | INFO | fairseq_cli.train | end of epoch 776 (average epoch stats below)\n",
      "2023-04-17 16:20:44 | INFO | train | epoch 776 | loss 2.343 | nll_loss 0.195 | ppl 1.14 | wps 920.2 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 24823 | lr 2.08215e-05 | gnorm 1.927 | clip 100 | loss_scale 2 | train_wall 11 | gb_free 13.9 | wall 19724\n",
      "2023-04-17 16:20:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:20:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:20:44 | INFO | fairseq.trainer | begin training epoch 777\n",
      "2023-04-17 16:20:44 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:20:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:20:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:20:55 | INFO | valid | epoch 777 | valid on 'valid' subset | loss 6.185 | nll_loss 4.507 | ppl 22.73 | wps 7156.1 | wpb 290.8 | bsz 1 | num_updates 24855 | best_loss 3.979\n",
      "2023-04-17 16:20:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 777 @ 24855 updates\n",
      "2023-04-17 16:20:55 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:21:08 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:21:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 777 @ 24855 updates, score 6.185) (writing took 12.299515908933245 seconds)\n",
      "2023-04-17 16:21:08 | INFO | fairseq_cli.train | end of epoch 777 (average epoch stats below)\n",
      "2023-04-17 16:21:08 | INFO | train | epoch 777 | loss 2.344 | nll_loss 0.196 | ppl 1.15 | wps 916.5 | ups 1.35 | wpb 678.5 | bsz 2 | num_updates 24855 | lr 2.08094e-05 | gnorm 1.949 | clip 100 | loss_scale 2 | train_wall 11 | gb_free 13.9 | wall 19748\n",
      "2023-04-17 16:21:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:21:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:21:08 | INFO | fairseq.trainer | begin training epoch 778\n",
      "2023-04-17 16:21:08 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:21:19 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:21:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:21:19 | INFO | valid | epoch 778 | valid on 'valid' subset | loss 6.074 | nll_loss 4.376 | ppl 20.77 | wps 7057.8 | wpb 290.8 | bsz 1 | num_updates 24887 | best_loss 3.979\n",
      "2023-04-17 16:21:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 778 @ 24887 updates\n",
      "2023-04-17 16:21:19 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:21:31 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:21:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 778 @ 24887 updates, score 6.074) (writing took 12.489530739025213 seconds)\n",
      "2023-04-17 16:21:32 | INFO | fairseq_cli.train | end of epoch 778 (average epoch stats below)\n",
      "2023-04-17 16:21:32 | INFO | train | epoch 778 | loss 2.343 | nll_loss 0.195 | ppl 1.14 | wps 907.8 | ups 1.34 | wpb 678.5 | bsz 2 | num_updates 24887 | lr 2.07974e-05 | gnorm 2.033 | clip 100 | loss_scale 2 | train_wall 11 | gb_free 13.9 | wall 19772\n",
      "2023-04-17 16:21:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:21:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:21:32 | INFO | fairseq.trainer | begin training epoch 779\n",
      "2023-04-17 16:21:32 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:21:36 | INFO | train_inner | epoch 779:     13 / 32 loss=2.344, nll_loss=0.195, ppl=1.14, wps=929.3, ups=1.38, wpb=673.9, bsz=2, num_updates=24900, lr=2.07925e-05, gnorm=1.962, clip=100, loss_scale=2, train_wall=34, gb_free=13.9, wall=19777\n",
      "2023-04-17 16:21:43 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:21:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:21:43 | INFO | valid | epoch 779 | valid on 'valid' subset | loss 6.125 | nll_loss 4.444 | ppl 21.77 | wps 7216.6 | wpb 290.8 | bsz 1 | num_updates 24919 | best_loss 3.979\n",
      "2023-04-17 16:21:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 779 @ 24919 updates\n",
      "2023-04-17 16:21:43 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:21:55 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:21:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 779 @ 24919 updates, score 6.125) (writing took 12.42634759703651 seconds)\n",
      "2023-04-17 16:21:55 | INFO | fairseq_cli.train | end of epoch 779 (average epoch stats below)\n",
      "2023-04-17 16:21:55 | INFO | train | epoch 779 | loss 2.344 | nll_loss 0.195 | ppl 1.14 | wps 914.2 | ups 1.35 | wpb 678.5 | bsz 2 | num_updates 24919 | lr 2.07853e-05 | gnorm 1.998 | clip 100 | loss_scale 2 | train_wall 11 | gb_free 13.9 | wall 19796\n",
      "2023-04-17 16:21:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:21:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:21:55 | INFO | fairseq.trainer | begin training epoch 780\n",
      "2023-04-17 16:21:55 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:22:08 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:22:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:22:08 | INFO | valid | epoch 780 | valid on 'valid' subset | loss 6.056 | nll_loss 4.368 | ppl 20.64 | wps 7183.3 | wpb 290.8 | bsz 1 | num_updates 24951 | best_loss 3.979\n",
      "2023-04-17 16:22:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 780 @ 24951 updates\n",
      "2023-04-17 16:22:08 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:22:20 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:22:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 780 @ 24951 updates, score 6.056) (writing took 12.520288168103434 seconds)\n",
      "2023-04-17 16:22:20 | INFO | fairseq_cli.train | end of epoch 780 (average epoch stats below)\n",
      "2023-04-17 16:22:20 | INFO | train | epoch 780 | loss 2.343 | nll_loss 0.195 | ppl 1.14 | wps 873.5 | ups 1.29 | wpb 678.5 | bsz 2 | num_updates 24951 | lr 2.07732e-05 | gnorm 1.962 | clip 100 | loss_scale 2 | train_wall 12 | gb_free 13.9 | wall 19821\n",
      "2023-04-17 16:22:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:22:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:22:20 | INFO | fairseq.trainer | begin training epoch 781\n",
      "2023-04-17 16:22:20 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:22:32 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:22:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 16:22:33 | INFO | valid | epoch 781 | valid on 'valid' subset | loss 6.162 | nll_loss 4.491 | ppl 22.48 | wps 7308.6 | wpb 290.8 | bsz 1 | num_updates 24983 | best_loss 3.979\n",
      "2023-04-17 16:22:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 781 @ 24983 updates\n",
      "2023-04-17 16:22:33 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:22:45 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:22:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 781 @ 24983 updates, score 6.162) (writing took 12.31392991100438 seconds)\n",
      "2023-04-17 16:22:45 | INFO | fairseq_cli.train | end of epoch 781 (average epoch stats below)\n",
      "2023-04-17 16:22:45 | INFO | train | epoch 781 | loss 2.343 | nll_loss 0.194 | ppl 1.14 | wps 878.6 | ups 1.29 | wpb 678.5 | bsz 2 | num_updates 24983 | lr 2.07611e-05 | gnorm 1.811 | clip 100 | loss_scale 2 | train_wall 12 | gb_free 13.9 | wall 19845\n",
      "2023-04-17 16:22:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:22:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:22:45 | INFO | fairseq.trainer | begin training epoch 782\n",
      "2023-04-17 16:22:45 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:22:52 | INFO | train_inner | epoch 782:     17 / 32 loss=2.343, nll_loss=0.194, ppl=1.14, wps=897, ups=1.33, wpb=675.9, bsz=2, num_updates=25000, lr=2.07547e-05, gnorm=1.925, clip=100, loss_scale=2, train_wall=37, gb_free=13.9, wall=19852\n",
      "2023-04-17 16:22:52 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:22:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:22:52 | INFO | valid | epoch 782 | valid on 'valid' subset | loss 6.118 | nll_loss 4.427 | ppl 21.52 | wps 7156.5 | wpb 290.8 | bsz 1 | num_updates 25000 | best_loss 3.979\n",
      "2023-04-17 16:22:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 782 @ 25000 updates\n",
      "2023-04-17 16:22:52 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_782_25000.pt\n",
      "2023-04-17 16:22:59 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_782_25000.pt\n",
      "2023-04-17 16:23:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_782_25000.pt (epoch 782 @ 25000 updates, score 6.118) (writing took 14.845134538947605 seconds)\n",
      "2023-04-17 16:23:13 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:23:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:23:13 | INFO | valid | epoch 782 | valid on 'valid' subset | loss 6.081 | nll_loss 4.383 | ppl 20.86 | wps 7114.2 | wpb 290.8 | bsz 1 | num_updates 25015 | best_loss 3.979\n",
      "2023-04-17 16:23:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 782 @ 25015 updates\n",
      "2023-04-17 16:23:13 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:23:25 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:23:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 782 @ 25015 updates, score 6.081) (writing took 12.614729056949727 seconds)\n",
      "2023-04-17 16:23:25 | INFO | fairseq_cli.train | end of epoch 782 (average epoch stats below)\n",
      "2023-04-17 16:23:25 | INFO | train | epoch 782 | loss 2.343 | nll_loss 0.196 | ppl 1.15 | wps 537.7 | ups 0.79 | wpb 678.5 | bsz 2 | num_updates 25015 | lr 2.07491e-05 | gnorm 1.853 | clip 100 | loss_scale 2 | train_wall 12 | gb_free 13.9 | wall 19886\n",
      "2023-04-17 16:23:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:23:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:23:25 | INFO | fairseq.trainer | begin training epoch 783\n",
      "2023-04-17 16:23:25 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:23:38 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:23:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:23:38 | INFO | valid | epoch 783 | valid on 'valid' subset | loss 6.068 | nll_loss 4.374 | ppl 20.74 | wps 7161 | wpb 290.8 | bsz 1 | num_updates 25047 | best_loss 3.979\n",
      "2023-04-17 16:23:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 783 @ 25047 updates\n",
      "2023-04-17 16:23:38 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:23:50 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:23:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 783 @ 25047 updates, score 6.068) (writing took 12.451659595011733 seconds)\n",
      "2023-04-17 16:23:50 | INFO | fairseq_cli.train | end of epoch 783 (average epoch stats below)\n",
      "2023-04-17 16:23:50 | INFO | train | epoch 783 | loss 2.343 | nll_loss 0.195 | ppl 1.14 | wps 873 | ups 1.29 | wpb 678.5 | bsz 2 | num_updates 25047 | lr 2.0737e-05 | gnorm 1.952 | clip 100 | loss_scale 2 | train_wall 12 | gb_free 13.9 | wall 19911\n",
      "2023-04-17 16:23:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:23:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:23:50 | INFO | fairseq.trainer | begin training epoch 784\n",
      "2023-04-17 16:23:50 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:24:03 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:24:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:24:03 | INFO | valid | epoch 784 | valid on 'valid' subset | loss 6.105 | nll_loss 4.427 | ppl 21.52 | wps 6778.6 | wpb 290.8 | bsz 1 | num_updates 25079 | best_loss 3.979\n",
      "2023-04-17 16:24:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 784 @ 25079 updates\n",
      "2023-04-17 16:24:03 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:24:16 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:24:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 784 @ 25079 updates, score 6.105) (writing took 12.917130877962336 seconds)\n",
      "2023-04-17 16:24:16 | INFO | fairseq_cli.train | end of epoch 784 (average epoch stats below)\n",
      "2023-04-17 16:24:16 | INFO | train | epoch 784 | loss 2.341 | nll_loss 0.191 | ppl 1.14 | wps 839.5 | ups 1.24 | wpb 678.5 | bsz 2 | num_updates 25079 | lr 2.07249e-05 | gnorm 1.929 | clip 100 | loss_scale 2 | train_wall 13 | gb_free 13.9 | wall 19936\n",
      "2023-04-17 16:24:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:24:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:24:16 | INFO | fairseq.trainer | begin training epoch 785\n",
      "2023-04-17 16:24:16 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:24:24 | INFO | train_inner | epoch 785:     21 / 32 loss=2.343, nll_loss=0.195, ppl=1.14, wps=731, ups=1.08, wpb=677, bsz=2, num_updates=25100, lr=2.0717e-05, gnorm=1.96, clip=100, loss_scale=2, train_wall=38, gb_free=13.9, wall=19945\n",
      "2023-04-17 16:24:28 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:24:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:24:29 | INFO | valid | epoch 785 | valid on 'valid' subset | loss 6.059 | nll_loss 4.366 | ppl 20.62 | wps 7173 | wpb 290.8 | bsz 1 | num_updates 25111 | best_loss 3.979\n",
      "2023-04-17 16:24:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 785 @ 25111 updates\n",
      "2023-04-17 16:24:29 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 16:24:41 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:24:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 785 @ 25111 updates, score 6.059) (writing took 12.211679337080568 seconds)\n",
      "2023-04-17 16:24:41 | INFO | fairseq_cli.train | end of epoch 785 (average epoch stats below)\n",
      "2023-04-17 16:24:41 | INFO | train | epoch 785 | loss 2.344 | nll_loss 0.196 | ppl 1.15 | wps 880.1 | ups 1.3 | wpb 678.5 | bsz 2 | num_updates 25111 | lr 2.07128e-05 | gnorm 2.16 | clip 100 | loss_scale 2 | train_wall 12 | gb_free 13.9 | wall 19961\n",
      "2023-04-17 16:24:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:24:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:24:41 | INFO | fairseq.trainer | begin training epoch 786\n",
      "2023-04-17 16:24:41 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:24:53 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:24:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:24:53 | INFO | valid | epoch 786 | valid on 'valid' subset | loss 6.082 | nll_loss 4.395 | ppl 21.04 | wps 7222.1 | wpb 290.8 | bsz 1 | num_updates 25143 | best_loss 3.979\n",
      "2023-04-17 16:24:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 786 @ 25143 updates\n",
      "2023-04-17 16:24:53 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:25:06 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:25:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 786 @ 25143 updates, score 6.082) (writing took 12.551143320044503 seconds)\n",
      "2023-04-17 16:25:06 | INFO | fairseq_cli.train | end of epoch 786 (average epoch stats below)\n",
      "2023-04-17 16:25:06 | INFO | train | epoch 786 | loss 2.341 | nll_loss 0.193 | ppl 1.14 | wps 870.3 | ups 1.28 | wpb 678.5 | bsz 2 | num_updates 25143 | lr 2.07008e-05 | gnorm 2.051 | clip 100 | loss_scale 2 | train_wall 12 | gb_free 13.9 | wall 19986\n",
      "2023-04-17 16:25:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:25:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:25:06 | INFO | fairseq.trainer | begin training epoch 787\n",
      "2023-04-17 16:25:06 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:25:18 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:25:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:25:18 | INFO | valid | epoch 787 | valid on 'valid' subset | loss 6.146 | nll_loss 4.454 | ppl 21.92 | wps 7199.4 | wpb 290.8 | bsz 1 | num_updates 25175 | best_loss 3.979\n",
      "2023-04-17 16:25:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 787 @ 25175 updates\n",
      "2023-04-17 16:25:18 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:25:30 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:25:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 787 @ 25175 updates, score 6.146) (writing took 12.31612380198203 seconds)\n",
      "2023-04-17 16:25:31 | INFO | fairseq_cli.train | end of epoch 787 (average epoch stats below)\n",
      "2023-04-17 16:25:31 | INFO | train | epoch 787 | loss 2.342 | nll_loss 0.193 | ppl 1.14 | wps 877.9 | ups 1.29 | wpb 678.5 | bsz 2 | num_updates 25175 | lr 2.06887e-05 | gnorm 2.055 | clip 100 | loss_scale 2 | train_wall 12 | gb_free 13.9 | wall 20011\n",
      "2023-04-17 16:25:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:25:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:25:31 | INFO | fairseq.trainer | begin training epoch 788\n",
      "2023-04-17 16:25:31 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:25:40 | INFO | train_inner | epoch 788:     25 / 32 loss=2.342, nll_loss=0.193, ppl=1.14, wps=916, ups=1.33, wpb=691.2, bsz=2, num_updates=25200, lr=2.06792e-05, gnorm=2.093, clip=100, loss_scale=2, train_wall=37, gb_free=13.9, wall=20020\n",
      "2023-04-17 16:25:42 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:25:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:25:42 | INFO | valid | epoch 788 | valid on 'valid' subset | loss 6.139 | nll_loss 4.461 | ppl 22.03 | wps 7148.3 | wpb 290.8 | bsz 1 | num_updates 25207 | best_loss 3.979\n",
      "2023-04-17 16:25:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 788 @ 25207 updates\n",
      "2023-04-17 16:25:42 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:25:55 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:25:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 788 @ 25207 updates, score 6.139) (writing took 12.663193101994693 seconds)\n",
      "2023-04-17 16:25:55 | INFO | fairseq_cli.train | end of epoch 788 (average epoch stats below)\n",
      "2023-04-17 16:25:55 | INFO | train | epoch 788 | loss 2.342 | nll_loss 0.193 | ppl 1.14 | wps 888.5 | ups 1.31 | wpb 678.5 | bsz 2 | num_updates 25207 | lr 2.06766e-05 | gnorm 2.006 | clip 100 | loss_scale 2 | train_wall 11 | gb_free 13.9 | wall 20035\n",
      "2023-04-17 16:25:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:25:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:25:55 | INFO | fairseq.trainer | begin training epoch 789\n",
      "2023-04-17 16:25:55 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:26:06 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:26:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:26:06 | INFO | valid | epoch 789 | valid on 'valid' subset | loss 6.186 | nll_loss 4.508 | ppl 22.75 | wps 7155.2 | wpb 290.8 | bsz 1 | num_updates 25239 | best_loss 3.979\n",
      "2023-04-17 16:26:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 789 @ 25239 updates\n",
      "2023-04-17 16:26:06 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:26:18 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:26:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 789 @ 25239 updates, score 6.186) (writing took 12.191380948992446 seconds)\n",
      "2023-04-17 16:26:18 | INFO | fairseq_cli.train | end of epoch 789 (average epoch stats below)\n",
      "2023-04-17 16:26:18 | INFO | train | epoch 789 | loss 2.342 | nll_loss 0.194 | ppl 1.14 | wps 922.2 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 25239 | lr 2.06645e-05 | gnorm 2.047 | clip 100 | loss_scale 2 | train_wall 11 | gb_free 13.9 | wall 20059\n",
      "2023-04-17 16:26:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:26:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:26:18 | INFO | fairseq.trainer | begin training epoch 790\n",
      "2023-04-17 16:26:18 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:26:30 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:26:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:26:30 | INFO | valid | epoch 790 | valid on 'valid' subset | loss 6.096 | nll_loss 4.416 | ppl 21.35 | wps 6965.8 | wpb 290.8 | bsz 1 | num_updates 25271 | best_loss 3.979\n",
      "2023-04-17 16:26:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 790 @ 25271 updates\n",
      "2023-04-17 16:26:30 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:26:42 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 16:26:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 790 @ 25271 updates, score 6.096) (writing took 12.65375065500848 seconds)\n",
      "2023-04-17 16:26:43 | INFO | fairseq_cli.train | end of epoch 790 (average epoch stats below)\n",
      "2023-04-17 16:26:43 | INFO | train | epoch 790 | loss 2.343 | nll_loss 0.195 | ppl 1.14 | wps 897.8 | ups 1.32 | wpb 678.5 | bsz 2 | num_updates 25271 | lr 2.06525e-05 | gnorm 2.081 | clip 100 | loss_scale 2 | train_wall 11 | gb_free 13.9 | wall 20083\n",
      "2023-04-17 16:26:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:26:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:26:43 | INFO | fairseq.trainer | begin training epoch 791\n",
      "2023-04-17 16:26:43 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:26:53 | INFO | train_inner | epoch 791:     29 / 32 loss=2.341, nll_loss=0.193, ppl=1.14, wps=927.1, ups=1.37, wpb=678.1, bsz=2, num_updates=25300, lr=2.06415e-05, gnorm=2.03, clip=100, loss_scale=2, train_wall=35, gb_free=13.9, wall=20093\n",
      "2023-04-17 16:26:54 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:26:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:26:54 | INFO | valid | epoch 791 | valid on 'valid' subset | loss 6.13 | nll_loss 4.451 | ppl 21.86 | wps 7120.8 | wpb 290.8 | bsz 1 | num_updates 25303 | best_loss 3.979\n",
      "2023-04-17 16:26:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 791 @ 25303 updates\n",
      "2023-04-17 16:26:54 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:27:06 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:27:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 791 @ 25303 updates, score 6.13) (writing took 12.216145182959735 seconds)\n",
      "2023-04-17 16:27:06 | INFO | fairseq_cli.train | end of epoch 791 (average epoch stats below)\n",
      "2023-04-17 16:27:06 | INFO | train | epoch 791 | loss 2.341 | nll_loss 0.192 | ppl 1.14 | wps 921.3 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 25303 | lr 2.06404e-05 | gnorm 2.145 | clip 100 | loss_scale 2 | train_wall 11 | gb_free 13.9 | wall 20106\n",
      "2023-04-17 16:27:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:27:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:27:06 | INFO | fairseq.trainer | begin training epoch 792\n",
      "2023-04-17 16:27:06 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:27:17 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:27:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:27:18 | INFO | valid | epoch 792 | valid on 'valid' subset | loss 6.177 | nll_loss 4.504 | ppl 22.68 | wps 7152.7 | wpb 290.8 | bsz 1 | num_updates 25335 | best_loss 3.979\n",
      "2023-04-17 16:27:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 792 @ 25335 updates\n",
      "2023-04-17 16:27:18 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:27:30 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:27:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 792 @ 25335 updates, score 6.177) (writing took 12.770081646973267 seconds)\n",
      "2023-04-17 16:27:30 | INFO | fairseq_cli.train | end of epoch 792 (average epoch stats below)\n",
      "2023-04-17 16:27:30 | INFO | train | epoch 792 | loss 2.34 | nll_loss 0.192 | ppl 1.14 | wps 900.2 | ups 1.33 | wpb 678.5 | bsz 2 | num_updates 25335 | lr 2.06283e-05 | gnorm 1.93 | clip 100 | loss_scale 2 | train_wall 11 | gb_free 13.9 | wall 20131\n",
      "2023-04-17 16:27:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:27:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:27:30 | INFO | fairseq.trainer | begin training epoch 793\n",
      "2023-04-17 16:27:30 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:27:41 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:27:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:27:42 | INFO | valid | epoch 793 | valid on 'valid' subset | loss 6.142 | nll_loss 4.469 | ppl 22.15 | wps 7224.4 | wpb 290.8 | bsz 1 | num_updates 25367 | best_loss 3.979\n",
      "2023-04-17 16:27:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 793 @ 25367 updates\n",
      "2023-04-17 16:27:42 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:27:54 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:27:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 793 @ 25367 updates, score 6.142) (writing took 12.238605335005559 seconds)\n",
      "2023-04-17 16:27:54 | INFO | fairseq_cli.train | end of epoch 793 (average epoch stats below)\n",
      "2023-04-17 16:27:54 | INFO | train | epoch 793 | loss 2.342 | nll_loss 0.193 | ppl 1.14 | wps 920.3 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 25367 | lr 2.06162e-05 | gnorm 2.081 | clip 100 | loss_scale 2 | train_wall 11 | gb_free 13.9 | wall 20154\n",
      "2023-04-17 16:27:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:27:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:27:54 | INFO | fairseq.trainer | begin training epoch 794\n",
      "2023-04-17 16:27:54 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:28:05 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:28:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:28:05 | INFO | valid | epoch 794 | valid on 'valid' subset | loss 6.114 | nll_loss 4.426 | ppl 21.5 | wps 7158.2 | wpb 290.8 | bsz 1 | num_updates 25399 | best_loss 3.979\n",
      "2023-04-17 16:28:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 794 @ 25399 updates\n",
      "2023-04-17 16:28:05 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:28:18 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:28:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 794 @ 25399 updates, score 6.114) (writing took 12.50010099390056 seconds)\n",
      "2023-04-17 16:28:18 | INFO | fairseq_cli.train | end of epoch 794 (average epoch stats below)\n",
      "2023-04-17 16:28:18 | INFO | train | epoch 794 | loss 2.342 | nll_loss 0.194 | ppl 1.14 | wps 909.9 | ups 1.34 | wpb 678.5 | bsz 2 | num_updates 25399 | lr 2.06042e-05 | gnorm 2.296 | clip 100 | loss_scale 2 | train_wall 11 | gb_free 13.9 | wall 20178\n",
      "2023-04-17 16:28:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:28:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:28:18 | INFO | fairseq.trainer | begin training epoch 795\n",
      "2023-04-17 16:28:18 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:28:18 | INFO | train_inner | epoch 795:      1 / 32 loss=2.342, nll_loss=0.193, ppl=1.14, wps=789.9, ups=1.17, wpb=674.1, bsz=2, num_updates=25400, lr=2.06038e-05, gnorm=2.111, clip=100, loss_scale=2, train_wall=34, gb_free=13.9, wall=20178\n",
      "2023-04-17 16:28:29 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:28:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:28:29 | INFO | valid | epoch 795 | valid on 'valid' subset | loss 6.162 | nll_loss 4.479 | ppl 22.3 | wps 7277.2 | wpb 290.8 | bsz 1 | num_updates 25431 | best_loss 3.979\n",
      "2023-04-17 16:28:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 795 @ 25431 updates\n",
      "2023-04-17 16:28:29 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 16:28:41 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:28:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 795 @ 25431 updates, score 6.162) (writing took 12.271186723955907 seconds)\n",
      "2023-04-17 16:28:42 | INFO | fairseq_cli.train | end of epoch 795 (average epoch stats below)\n",
      "2023-04-17 16:28:42 | INFO | train | epoch 795 | loss 2.34 | nll_loss 0.192 | ppl 1.14 | wps 916.9 | ups 1.35 | wpb 678.5 | bsz 2 | num_updates 25431 | lr 2.05921e-05 | gnorm 2.001 | clip 100 | loss_scale 2 | train_wall 11 | gb_free 13.9 | wall 20202\n",
      "2023-04-17 16:28:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:28:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:28:42 | INFO | fairseq.trainer | begin training epoch 796\n",
      "2023-04-17 16:28:42 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:28:53 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:28:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:28:53 | INFO | valid | epoch 796 | valid on 'valid' subset | loss 6.186 | nll_loss 4.505 | ppl 22.7 | wps 7056.3 | wpb 290.8 | bsz 1 | num_updates 25463 | best_loss 3.979\n",
      "2023-04-17 16:28:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 796 @ 25463 updates\n",
      "2023-04-17 16:28:53 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:29:05 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:29:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 796 @ 25463 updates, score 6.186) (writing took 12.547164198942482 seconds)\n",
      "2023-04-17 16:29:06 | INFO | fairseq_cli.train | end of epoch 796 (average epoch stats below)\n",
      "2023-04-17 16:29:06 | INFO | train | epoch 796 | loss 2.34 | nll_loss 0.192 | ppl 1.14 | wps 903.4 | ups 1.33 | wpb 678.5 | bsz 2 | num_updates 25463 | lr 2.058e-05 | gnorm 1.998 | clip 100 | loss_scale 2 | train_wall 11 | gb_free 13.9 | wall 20226\n",
      "2023-04-17 16:29:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:29:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:29:06 | INFO | fairseq.trainer | begin training epoch 797\n",
      "2023-04-17 16:29:06 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:29:17 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:29:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:29:17 | INFO | valid | epoch 797 | valid on 'valid' subset | loss 6.169 | nll_loss 4.495 | ppl 22.54 | wps 7227.3 | wpb 290.8 | bsz 1 | num_updates 25495 | best_loss 3.979\n",
      "2023-04-17 16:29:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 797 @ 25495 updates\n",
      "2023-04-17 16:29:17 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:29:29 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:29:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 797 @ 25495 updates, score 6.169) (writing took 12.171449749032035 seconds)\n",
      "2023-04-17 16:29:29 | INFO | fairseq_cli.train | end of epoch 797 (average epoch stats below)\n",
      "2023-04-17 16:29:29 | INFO | train | epoch 797 | loss 2.34 | nll_loss 0.189 | ppl 1.14 | wps 920.7 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 25495 | lr 2.05679e-05 | gnorm 1.953 | clip 100 | loss_scale 2 | train_wall 11 | gb_free 13.9 | wall 20249\n",
      "2023-04-17 16:29:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:29:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:29:29 | INFO | fairseq.trainer | begin training epoch 798\n",
      "2023-04-17 16:29:29 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:29:31 | INFO | train_inner | epoch 798:      5 / 32 loss=2.34, nll_loss=0.192, ppl=1.14, wps=934.5, ups=1.38, wpb=679.3, bsz=2, num_updates=25500, lr=2.0566e-05, gnorm=1.988, clip=100, loss_scale=2, train_wall=35, gb_free=13.9, wall=20251\n",
      "2023-04-17 16:29:40 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:29:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:29:41 | INFO | valid | epoch 798 | valid on 'valid' subset | loss 6.131 | nll_loss 4.45 | ppl 21.86 | wps 7106.2 | wpb 290.8 | bsz 1 | num_updates 25527 | best_loss 3.979\n",
      "2023-04-17 16:29:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 798 @ 25527 updates\n",
      "2023-04-17 16:29:41 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:29:53 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:29:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 798 @ 25527 updates, score 6.131) (writing took 12.502916140016168 seconds)\n",
      "2023-04-17 16:29:53 | INFO | fairseq_cli.train | end of epoch 798 (average epoch stats below)\n",
      "2023-04-17 16:29:53 | INFO | train | epoch 798 | loss 2.343 | nll_loss 0.196 | ppl 1.15 | wps 906.7 | ups 1.34 | wpb 678.5 | bsz 2 | num_updates 25527 | lr 2.05558e-05 | gnorm 2.124 | clip 100 | loss_scale 2 | train_wall 11 | gb_free 13.9 | wall 20273\n",
      "2023-04-17 16:29:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:29:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:29:53 | INFO | fairseq.trainer | begin training epoch 799\n",
      "2023-04-17 16:29:53 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:30:04 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:30:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:30:04 | INFO | valid | epoch 799 | valid on 'valid' subset | loss 6.107 | nll_loss 4.413 | ppl 21.31 | wps 7260.7 | wpb 290.8 | bsz 1 | num_updates 25559 | best_loss 3.979\n",
      "2023-04-17 16:30:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 799 @ 25559 updates\n",
      "2023-04-17 16:30:04 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:30:17 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:30:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 799 @ 25559 updates, score 6.107) (writing took 12.54027167800814 seconds)\n",
      "2023-04-17 16:30:17 | INFO | fairseq_cli.train | end of epoch 799 (average epoch stats below)\n",
      "2023-04-17 16:30:17 | INFO | train | epoch 799 | loss 2.342 | nll_loss 0.193 | ppl 1.14 | wps 908.1 | ups 1.34 | wpb 678.5 | bsz 2 | num_updates 25559 | lr 2.05438e-05 | gnorm 2.318 | clip 100 | loss_scale 2 | train_wall 11 | gb_free 13.9 | wall 20297\n",
      "2023-04-17 16:30:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:30:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:30:17 | INFO | fairseq.trainer | begin training epoch 800\n",
      "2023-04-17 16:30:17 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:30:28 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:30:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:30:28 | INFO | valid | epoch 800 | valid on 'valid' subset | loss 6.184 | nll_loss 4.497 | ppl 22.57 | wps 7231.8 | wpb 290.8 | bsz 1 | num_updates 25591 | best_loss 3.979\n",
      "2023-04-17 16:30:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 800 @ 25591 updates\n",
      "2023-04-17 16:30:28 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:30:39 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 16:30:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 800 @ 25591 updates, score 6.184) (writing took 10.953533463994972 seconds)\n",
      "2023-04-17 16:30:39 | INFO | fairseq_cli.train | end of epoch 800 (average epoch stats below)\n",
      "2023-04-17 16:30:39 | INFO | train | epoch 800 | loss 2.34 | nll_loss 0.192 | ppl 1.14 | wps 973.4 | ups 1.43 | wpb 678.5 | bsz 2 | num_updates 25591 | lr 2.05317e-05 | gnorm 1.854 | clip 100 | loss_scale 2 | train_wall 11 | gb_free 13.9 | wall 20320\n",
      "2023-04-17 16:30:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:30:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:30:39 | INFO | fairseq.trainer | begin training epoch 801\n",
      "2023-04-17 16:30:39 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:30:43 | INFO | train_inner | epoch 801:      9 / 32 loss=2.342, nll_loss=0.194, ppl=1.14, wps=949.8, ups=1.4, wpb=680.2, bsz=2, num_updates=25600, lr=2.05283e-05, gnorm=2.097, clip=100, loss_scale=2, train_wall=35, gb_free=13.9, wall=20323\n",
      "2023-04-17 16:30:50 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:30:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:30:51 | INFO | valid | epoch 801 | valid on 'valid' subset | loss 6.122 | nll_loss 4.433 | ppl 21.59 | wps 7163.4 | wpb 290.8 | bsz 1 | num_updates 25623 | best_loss 3.979\n",
      "2023-04-17 16:30:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 801 @ 25623 updates\n",
      "2023-04-17 16:30:51 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:31:03 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:31:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 801 @ 25623 updates, score 6.122) (writing took 12.063187698018737 seconds)\n",
      "2023-04-17 16:31:03 | INFO | fairseq_cli.train | end of epoch 801 (average epoch stats below)\n",
      "2023-04-17 16:31:03 | INFO | train | epoch 801 | loss 2.34 | nll_loss 0.192 | ppl 1.14 | wps 926 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 25623 | lr 2.05196e-05 | gnorm 1.908 | clip 100 | loss_scale 2 | train_wall 11 | gb_free 13.9 | wall 20343\n",
      "2023-04-17 16:31:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:31:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:31:03 | INFO | fairseq.trainer | begin training epoch 802\n",
      "2023-04-17 16:31:03 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:31:14 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:31:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:31:14 | INFO | valid | epoch 802 | valid on 'valid' subset | loss 6.04 | nll_loss 4.326 | ppl 20.06 | wps 7271.7 | wpb 290.8 | bsz 1 | num_updates 25655 | best_loss 3.979\n",
      "2023-04-17 16:31:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 802 @ 25655 updates\n",
      "2023-04-17 16:31:14 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:31:27 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:31:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 802 @ 25655 updates, score 6.04) (writing took 12.684813741012476 seconds)\n",
      "2023-04-17 16:31:27 | INFO | fairseq_cli.train | end of epoch 802 (average epoch stats below)\n",
      "2023-04-17 16:31:27 | INFO | train | epoch 802 | loss 2.341 | nll_loss 0.193 | ppl 1.14 | wps 903 | ups 1.33 | wpb 678.5 | bsz 2 | num_updates 25655 | lr 2.05075e-05 | gnorm 2.27 | clip 100 | loss_scale 2 | train_wall 11 | gb_free 13.9 | wall 20367\n",
      "2023-04-17 16:31:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:31:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:31:27 | INFO | fairseq.trainer | begin training epoch 803\n",
      "2023-04-17 16:31:27 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:31:39 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:31:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:31:39 | INFO | valid | epoch 803 | valid on 'valid' subset | loss 6.212 | nll_loss 4.527 | ppl 23.06 | wps 6180.8 | wpb 290.8 | bsz 1 | num_updates 25687 | best_loss 3.979\n",
      "2023-04-17 16:31:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 803 @ 25687 updates\n",
      "2023-04-17 16:31:39 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:31:51 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:31:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 803 @ 25687 updates, score 6.212) (writing took 12.527342271991074 seconds)\n",
      "2023-04-17 16:31:51 | INFO | fairseq_cli.train | end of epoch 803 (average epoch stats below)\n",
      "2023-04-17 16:31:51 | INFO | train | epoch 803 | loss 2.337 | nll_loss 0.187 | ppl 1.14 | wps 880.7 | ups 1.3 | wpb 678.5 | bsz 2 | num_updates 25687 | lr 2.04955e-05 | gnorm 1.909 | clip 100 | loss_scale 2 | train_wall 12 | gb_free 13.9 | wall 20392\n",
      "2023-04-17 16:31:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:31:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:31:51 | INFO | fairseq.trainer | begin training epoch 804\n",
      "2023-04-17 16:31:51 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:31:56 | INFO | train_inner | epoch 804:     13 / 32 loss=2.339, nll_loss=0.19, ppl=1.14, wps=913.5, ups=1.36, wpb=673.9, bsz=2, num_updates=25700, lr=2.04906e-05, gnorm=2.043, clip=100, loss_scale=2, train_wall=35, gb_free=13.9, wall=20397\n",
      "2023-04-17 16:32:03 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:32:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:32:03 | INFO | valid | epoch 804 | valid on 'valid' subset | loss 6.111 | nll_loss 4.439 | ppl 21.7 | wps 6932.6 | wpb 290.8 | bsz 1 | num_updates 25719 | best_loss 3.979\n",
      "2023-04-17 16:32:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 804 @ 25719 updates\n",
      "2023-04-17 16:32:03 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:32:17 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:32:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 804 @ 25719 updates, score 6.111) (writing took 14.133008958073333 seconds)\n",
      "2023-04-17 16:32:17 | INFO | fairseq_cli.train | end of epoch 804 (average epoch stats below)\n",
      "2023-04-17 16:32:17 | INFO | train | epoch 804 | loss 2.339 | nll_loss 0.187 | ppl 1.14 | wps 835.3 | ups 1.23 | wpb 678.5 | bsz 2 | num_updates 25719 | lr 2.04834e-05 | gnorm 2.003 | clip 100 | loss_scale 2 | train_wall 11 | gb_free 13.9 | wall 20418\n",
      "2023-04-17 16:32:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:32:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:32:17 | INFO | fairseq.trainer | begin training epoch 805\n",
      "2023-04-17 16:32:17 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:32:29 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:32:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:32:29 | INFO | valid | epoch 805 | valid on 'valid' subset | loss 6.137 | nll_loss 4.459 | ppl 21.99 | wps 7093.8 | wpb 290.8 | bsz 1 | num_updates 25751 | best_loss 3.979\n",
      "2023-04-17 16:32:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 805 @ 25751 updates\n",
      "2023-04-17 16:32:29 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 16:32:41 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:32:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 805 @ 25751 updates, score 6.137) (writing took 12.398659990052693 seconds)\n",
      "2023-04-17 16:32:41 | INFO | fairseq_cli.train | end of epoch 805 (average epoch stats below)\n",
      "2023-04-17 16:32:41 | INFO | train | epoch 805 | loss 2.342 | nll_loss 0.195 | ppl 1.15 | wps 912.5 | ups 1.34 | wpb 678.5 | bsz 2 | num_updates 25751 | lr 2.04713e-05 | gnorm 2.214 | clip 100 | loss_scale 2 | train_wall 11 | gb_free 13.9 | wall 20441\n",
      "2023-04-17 16:32:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:32:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:32:41 | INFO | fairseq.trainer | begin training epoch 806\n",
      "2023-04-17 16:32:41 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:32:52 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:32:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:32:53 | INFO | valid | epoch 806 | valid on 'valid' subset | loss 6.138 | nll_loss 4.448 | ppl 21.83 | wps 6923.9 | wpb 290.8 | bsz 1 | num_updates 25783 | best_loss 3.979\n",
      "2023-04-17 16:32:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 806 @ 25783 updates\n",
      "2023-04-17 16:32:53 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:33:04 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:33:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 806 @ 25783 updates, score 6.138) (writing took 12.104204464936629 seconds)\n",
      "2023-04-17 16:33:05 | INFO | fairseq_cli.train | end of epoch 806 (average epoch stats below)\n",
      "2023-04-17 16:33:05 | INFO | train | epoch 806 | loss 2.34 | nll_loss 0.193 | ppl 1.14 | wps 925.6 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 25783 | lr 2.04592e-05 | gnorm 2.013 | clip 100 | loss_scale 2 | train_wall 11 | gb_free 13.9 | wall 20465\n",
      "2023-04-17 16:33:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:33:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:33:05 | INFO | fairseq.trainer | begin training epoch 807\n",
      "2023-04-17 16:33:05 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:33:11 | INFO | train_inner | epoch 807:     17 / 32 loss=2.34, nll_loss=0.191, ppl=1.14, wps=913.2, ups=1.34, wpb=679.6, bsz=2, num_updates=25800, lr=2.04528e-05, gnorm=2.02, clip=100, loss_scale=2, train_wall=35, gb_free=13.9, wall=20471\n",
      "2023-04-17 16:33:16 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:33:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:33:16 | INFO | valid | epoch 807 | valid on 'valid' subset | loss 6.144 | nll_loss 4.467 | ppl 22.12 | wps 7219.9 | wpb 290.8 | bsz 1 | num_updates 25815 | best_loss 3.979\n",
      "2023-04-17 16:33:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 807 @ 25815 updates\n",
      "2023-04-17 16:33:16 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:33:28 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:33:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 807 @ 25815 updates, score 6.144) (writing took 12.236815460957587 seconds)\n",
      "2023-04-17 16:33:28 | INFO | fairseq_cli.train | end of epoch 807 (average epoch stats below)\n",
      "2023-04-17 16:33:28 | INFO | train | epoch 807 | loss 2.337 | nll_loss 0.189 | ppl 1.14 | wps 919 | ups 1.35 | wpb 678.5 | bsz 2 | num_updates 25815 | lr 2.04472e-05 | gnorm 1.811 | clip 100 | loss_scale 2 | train_wall 11 | gb_free 13.9 | wall 20489\n",
      "2023-04-17 16:33:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:33:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:33:28 | INFO | fairseq.trainer | begin training epoch 808\n",
      "2023-04-17 16:33:28 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:33:40 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:33:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:33:40 | INFO | valid | epoch 808 | valid on 'valid' subset | loss 6.116 | nll_loss 4.432 | ppl 21.59 | wps 7158.7 | wpb 290.8 | bsz 1 | num_updates 25847 | best_loss 3.979\n",
      "2023-04-17 16:33:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 808 @ 25847 updates\n",
      "2023-04-17 16:33:40 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:33:52 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:33:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 808 @ 25847 updates, score 6.116) (writing took 12.20756042492576 seconds)\n",
      "2023-04-17 16:33:52 | INFO | fairseq_cli.train | end of epoch 808 (average epoch stats below)\n",
      "2023-04-17 16:33:52 | INFO | train | epoch 808 | loss 2.34 | nll_loss 0.191 | ppl 1.14 | wps 918.4 | ups 1.35 | wpb 678.5 | bsz 2 | num_updates 25847 | lr 2.04351e-05 | gnorm 2.227 | clip 100 | loss_scale 2 | train_wall 11 | gb_free 13.9 | wall 20512\n",
      "2023-04-17 16:33:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:33:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:33:52 | INFO | fairseq.trainer | begin training epoch 809\n",
      "2023-04-17 16:33:52 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:34:03 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:34:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:34:04 | INFO | valid | epoch 809 | valid on 'valid' subset | loss 6.177 | nll_loss 4.493 | ppl 22.52 | wps 7169.3 | wpb 290.8 | bsz 1 | num_updates 25879 | best_loss 3.979\n",
      "2023-04-17 16:34:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 809 @ 25879 updates\n",
      "2023-04-17 16:34:04 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:34:16 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:34:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 809 @ 25879 updates, score 6.177) (writing took 12.469743298948742 seconds)\n",
      "2023-04-17 16:34:16 | INFO | fairseq_cli.train | end of epoch 809 (average epoch stats below)\n",
      "2023-04-17 16:34:16 | INFO | train | epoch 809 | loss 2.337 | nll_loss 0.188 | ppl 1.14 | wps 901.4 | ups 1.33 | wpb 678.5 | bsz 2 | num_updates 25879 | lr 2.0423e-05 | gnorm 1.945 | clip 100 | loss_scale 2 | train_wall 11 | gb_free 13.9 | wall 20536\n",
      "2023-04-17 16:34:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:34:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:34:16 | INFO | fairseq.trainer | begin training epoch 810\n",
      "2023-04-17 16:34:16 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:34:24 | INFO | train_inner | epoch 810:     21 / 32 loss=2.339, nll_loss=0.191, ppl=1.14, wps=934, ups=1.37, wpb=682.6, bsz=2, num_updates=25900, lr=2.04151e-05, gnorm=2.026, clip=100, loss_scale=2, train_wall=35, gb_free=13.9, wall=20544\n",
      "2023-04-17 16:34:28 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:34:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:34:28 | INFO | valid | epoch 810 | valid on 'valid' subset | loss 6.102 | nll_loss 4.418 | ppl 21.37 | wps 6881.1 | wpb 290.8 | bsz 1 | num_updates 25911 | best_loss 3.979\n",
      "2023-04-17 16:34:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 810 @ 25911 updates\n",
      "2023-04-17 16:34:28 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 16:34:42 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:34:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 810 @ 25911 updates, score 6.102) (writing took 13.930665793013759 seconds)\n",
      "2023-04-17 16:34:42 | INFO | fairseq_cli.train | end of epoch 810 (average epoch stats below)\n",
      "2023-04-17 16:34:42 | INFO | train | epoch 810 | loss 2.339 | nll_loss 0.191 | ppl 1.14 | wps 841.3 | ups 1.24 | wpb 678.5 | bsz 2 | num_updates 25911 | lr 2.04109e-05 | gnorm 1.892 | clip 100 | loss_scale 2 | train_wall 11 | gb_free 13.9 | wall 20562\n",
      "2023-04-17 16:34:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:34:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:34:42 | INFO | fairseq.trainer | begin training epoch 811\n",
      "2023-04-17 16:34:42 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:34:53 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:34:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:34:53 | INFO | valid | epoch 811 | valid on 'valid' subset | loss 6.179 | nll_loss 4.499 | ppl 22.62 | wps 7184.5 | wpb 290.8 | bsz 1 | num_updates 25943 | best_loss 3.979\n",
      "2023-04-17 16:34:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 811 @ 25943 updates\n",
      "2023-04-17 16:34:53 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:35:05 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:35:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 811 @ 25943 updates, score 6.179) (writing took 12.211008297977969 seconds)\n",
      "2023-04-17 16:35:05 | INFO | fairseq_cli.train | end of epoch 811 (average epoch stats below)\n",
      "2023-04-17 16:35:05 | INFO | train | epoch 811 | loss 2.338 | nll_loss 0.189 | ppl 1.14 | wps 919.5 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 25943 | lr 2.03989e-05 | gnorm 1.89 | clip 100 | loss_scale 2 | train_wall 11 | gb_free 13.9 | wall 20586\n",
      "2023-04-17 16:35:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:35:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:35:05 | INFO | fairseq.trainer | begin training epoch 812\n",
      "2023-04-17 16:35:05 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:35:17 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:35:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:35:17 | INFO | valid | epoch 812 | valid on 'valid' subset | loss 6.123 | nll_loss 4.439 | ppl 21.69 | wps 7159.9 | wpb 290.8 | bsz 1 | num_updates 25975 | best_loss 3.979\n",
      "2023-04-17 16:35:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 812 @ 25975 updates\n",
      "2023-04-17 16:35:17 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:35:29 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:35:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 812 @ 25975 updates, score 6.123) (writing took 12.062057542032562 seconds)\n",
      "2023-04-17 16:35:29 | INFO | fairseq_cli.train | end of epoch 812 (average epoch stats below)\n",
      "2023-04-17 16:35:29 | INFO | train | epoch 812 | loss 2.34 | nll_loss 0.193 | ppl 1.14 | wps 928.3 | ups 1.37 | wpb 678.5 | bsz 2 | num_updates 25975 | lr 2.03868e-05 | gnorm 2.017 | clip 100 | loss_scale 2 | train_wall 11 | gb_free 13.9 | wall 20609\n",
      "2023-04-17 16:35:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:35:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:35:29 | INFO | fairseq.trainer | begin training epoch 813\n",
      "2023-04-17 16:35:29 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:35:38 | INFO | train_inner | epoch 813:     25 / 32 loss=2.338, nll_loss=0.19, ppl=1.14, wps=910.1, ups=1.35, wpb=672.3, bsz=2, num_updates=26000, lr=2.03774e-05, gnorm=1.886, clip=100, loss_scale=2, train_wall=35, gb_free=13.9, wall=20618\n",
      "2023-04-17 16:35:40 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:35:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:35:40 | INFO | valid | epoch 813 | valid on 'valid' subset | loss 6.188 | nll_loss 4.505 | ppl 22.71 | wps 7192.4 | wpb 290.8 | bsz 1 | num_updates 26007 | best_loss 3.979\n",
      "2023-04-17 16:35:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 813 @ 26007 updates\n",
      "2023-04-17 16:35:40 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:35:52 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:35:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 813 @ 26007 updates, score 6.188) (writing took 12.096296402974986 seconds)\n",
      "2023-04-17 16:35:52 | INFO | fairseq_cli.train | end of epoch 813 (average epoch stats below)\n",
      "2023-04-17 16:35:52 | INFO | train | epoch 813 | loss 2.337 | nll_loss 0.19 | ppl 1.14 | wps 925.5 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 26007 | lr 2.03747e-05 | gnorm 1.821 | clip 100 | loss_scale 2 | train_wall 11 | gb_free 13.9 | wall 20633\n",
      "2023-04-17 16:35:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:35:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:35:52 | INFO | fairseq.trainer | begin training epoch 814\n",
      "2023-04-17 16:35:52 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:36:04 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:36:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:36:04 | INFO | valid | epoch 814 | valid on 'valid' subset | loss 6.185 | nll_loss 4.497 | ppl 22.58 | wps 7063.3 | wpb 290.8 | bsz 1 | num_updates 26039 | best_loss 3.979\n",
      "2023-04-17 16:36:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 814 @ 26039 updates\n",
      "2023-04-17 16:36:04 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:36:16 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:36:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 814 @ 26039 updates, score 6.185) (writing took 12.132976192981005 seconds)\n",
      "2023-04-17 16:36:16 | INFO | fairseq_cli.train | end of epoch 814 (average epoch stats below)\n",
      "2023-04-17 16:36:16 | INFO | train | epoch 814 | loss 2.34 | nll_loss 0.194 | ppl 1.14 | wps 920 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 26039 | lr 2.03626e-05 | gnorm 2.007 | clip 100 | loss_scale 2 | train_wall 11 | gb_free 13.9 | wall 20656\n",
      "2023-04-17 16:36:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:36:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:36:16 | INFO | fairseq.trainer | begin training epoch 815\n",
      "2023-04-17 16:36:16 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:36:27 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:36:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:36:27 | INFO | valid | epoch 815 | valid on 'valid' subset | loss 6.179 | nll_loss 4.507 | ppl 22.74 | wps 7222.4 | wpb 290.8 | bsz 1 | num_updates 26071 | best_loss 3.979\n",
      "2023-04-17 16:36:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 815 @ 26071 updates\n",
      "2023-04-17 16:36:27 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:36:39 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 16:36:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 815 @ 26071 updates, score 6.179) (writing took 12.269256567000411 seconds)\n",
      "2023-04-17 16:36:40 | INFO | fairseq_cli.train | end of epoch 815 (average epoch stats below)\n",
      "2023-04-17 16:36:40 | INFO | train | epoch 815 | loss 2.339 | nll_loss 0.189 | ppl 1.14 | wps 918.8 | ups 1.35 | wpb 678.5 | bsz 2 | num_updates 26071 | lr 2.03506e-05 | gnorm 1.897 | clip 100 | loss_scale 2 | train_wall 11 | gb_free 13.9 | wall 20680\n",
      "2023-04-17 16:36:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:36:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:36:40 | INFO | fairseq.trainer | begin training epoch 816\n",
      "2023-04-17 16:36:40 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:36:50 | INFO | train_inner | epoch 816:     29 / 32 loss=2.339, nll_loss=0.191, ppl=1.14, wps=947.1, ups=1.39, wpb=683.1, bsz=2, num_updates=26100, lr=2.03396e-05, gnorm=1.928, clip=100, loss_scale=2, train_wall=35, gb_free=13.9, wall=20690\n",
      "2023-04-17 16:36:51 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:36:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:36:51 | INFO | valid | epoch 816 | valid on 'valid' subset | loss 6.163 | nll_loss 4.48 | ppl 22.32 | wps 7090.3 | wpb 290.8 | bsz 1 | num_updates 26103 | best_loss 3.979\n",
      "2023-04-17 16:36:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 816 @ 26103 updates\n",
      "2023-04-17 16:36:51 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:37:03 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:37:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 816 @ 26103 updates, score 6.163) (writing took 12.152220819960348 seconds)\n",
      "2023-04-17 16:37:03 | INFO | fairseq_cli.train | end of epoch 816 (average epoch stats below)\n",
      "2023-04-17 16:37:03 | INFO | train | epoch 816 | loss 2.337 | nll_loss 0.191 | ppl 1.14 | wps 921 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 26103 | lr 2.03385e-05 | gnorm 1.885 | clip 100 | loss_scale 2 | train_wall 11 | gb_free 13.9 | wall 20703\n",
      "2023-04-17 16:37:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:37:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:37:03 | INFO | fairseq.trainer | begin training epoch 817\n",
      "2023-04-17 16:37:03 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:37:14 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:37:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:37:15 | INFO | valid | epoch 817 | valid on 'valid' subset | loss 6.196 | nll_loss 4.527 | ppl 23.05 | wps 7235.3 | wpb 290.8 | bsz 1 | num_updates 26135 | best_loss 3.979\n",
      "2023-04-17 16:37:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 817 @ 26135 updates\n",
      "2023-04-17 16:37:15 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:37:27 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:37:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 817 @ 26135 updates, score 6.196) (writing took 12.224157816031948 seconds)\n",
      "2023-04-17 16:37:27 | INFO | fairseq_cli.train | end of epoch 817 (average epoch stats below)\n",
      "2023-04-17 16:37:27 | INFO | train | epoch 817 | loss 2.336 | nll_loss 0.187 | ppl 1.14 | wps 918.1 | ups 1.35 | wpb 678.5 | bsz 2 | num_updates 26135 | lr 2.03264e-05 | gnorm 1.936 | clip 100 | loss_scale 2 | train_wall 11 | gb_free 13.9 | wall 20727\n",
      "2023-04-17 16:37:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:37:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:37:27 | INFO | fairseq.trainer | begin training epoch 818\n",
      "2023-04-17 16:37:27 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:37:38 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:37:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:37:38 | INFO | valid | epoch 818 | valid on 'valid' subset | loss 6.117 | nll_loss 4.433 | ppl 21.59 | wps 7101 | wpb 290.8 | bsz 1 | num_updates 26167 | best_loss 3.979\n",
      "2023-04-17 16:37:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 818 @ 26167 updates\n",
      "2023-04-17 16:37:38 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:37:50 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:37:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 818 @ 26167 updates, score 6.117) (writing took 12.325097259017639 seconds)\n",
      "2023-04-17 16:37:51 | INFO | fairseq_cli.train | end of epoch 818 (average epoch stats below)\n",
      "2023-04-17 16:37:51 | INFO | train | epoch 818 | loss 2.34 | nll_loss 0.191 | ppl 1.14 | wps 911.1 | ups 1.34 | wpb 678.5 | bsz 2 | num_updates 26167 | lr 2.03143e-05 | gnorm 2.065 | clip 100 | loss_scale 2 | train_wall 11 | gb_free 13.9 | wall 20751\n",
      "2023-04-17 16:37:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:37:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:37:51 | INFO | fairseq.trainer | begin training epoch 819\n",
      "2023-04-17 16:37:51 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:38:02 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:38:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:38:03 | INFO | valid | epoch 819 | valid on 'valid' subset | loss 6.168 | nll_loss 4.488 | ppl 22.44 | wps 5870.9 | wpb 290.8 | bsz 1 | num_updates 26199 | best_loss 3.979\n",
      "2023-04-17 16:38:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 819 @ 26199 updates\n",
      "2023-04-17 16:38:03 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:38:15 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:38:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 819 @ 26199 updates, score 6.168) (writing took 12.283636306063272 seconds)\n",
      "2023-04-17 16:38:15 | INFO | fairseq_cli.train | end of epoch 819 (average epoch stats below)\n",
      "2023-04-17 16:38:15 | INFO | train | epoch 819 | loss 2.337 | nll_loss 0.19 | ppl 1.14 | wps 889.4 | ups 1.31 | wpb 678.5 | bsz 2 | num_updates 26199 | lr 2.03023e-05 | gnorm 2.032 | clip 100 | loss_scale 2 | train_wall 12 | gb_free 13.9 | wall 20775\n",
      "2023-04-17 16:38:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:38:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:38:15 | INFO | fairseq.trainer | begin training epoch 820\n",
      "2023-04-17 16:38:15 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:38:15 | INFO | train_inner | epoch 820:      1 / 32 loss=2.338, nll_loss=0.19, ppl=1.14, wps=786.4, ups=1.17, wpb=673.4, bsz=2, num_updates=26200, lr=2.03019e-05, gnorm=2.025, clip=100, loss_scale=2, train_wall=35, gb_free=13.9, wall=20776\n",
      "2023-04-17 16:38:27 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:38:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:38:27 | INFO | valid | epoch 820 | valid on 'valid' subset | loss 6.121 | nll_loss 4.441 | ppl 21.72 | wps 6126.9 | wpb 290.8 | bsz 1 | num_updates 26231 | best_loss 3.979\n",
      "2023-04-17 16:38:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 820 @ 26231 updates\n",
      "2023-04-17 16:38:27 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 16:38:40 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:38:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 820 @ 26231 updates, score 6.121) (writing took 12.888088098028675 seconds)\n",
      "2023-04-17 16:38:40 | INFO | fairseq_cli.train | end of epoch 820 (average epoch stats below)\n",
      "2023-04-17 16:38:40 | INFO | train | epoch 820 | loss 2.338 | nll_loss 0.191 | ppl 1.14 | wps 869 | ups 1.28 | wpb 678.5 | bsz 2 | num_updates 26231 | lr 2.02902e-05 | gnorm 2.02 | clip 100 | loss_scale 2 | train_wall 12 | gb_free 13.9 | wall 20800\n",
      "2023-04-17 16:38:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:38:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:38:40 | INFO | fairseq.trainer | begin training epoch 821\n",
      "2023-04-17 16:38:40 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:38:52 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:38:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:38:53 | INFO | valid | epoch 821 | valid on 'valid' subset | loss 6.262 | nll_loss 4.598 | ppl 24.23 | wps 5980.9 | wpb 290.8 | bsz 1 | num_updates 26263 | best_loss 3.979\n",
      "2023-04-17 16:38:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 821 @ 26263 updates\n",
      "2023-04-17 16:38:53 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:39:05 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:39:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 821 @ 26263 updates, score 6.262) (writing took 12.609366920078173 seconds)\n",
      "2023-04-17 16:39:05 | INFO | fairseq_cli.train | end of epoch 821 (average epoch stats below)\n",
      "2023-04-17 16:39:05 | INFO | train | epoch 821 | loss 2.334 | nll_loss 0.186 | ppl 1.14 | wps 864.1 | ups 1.27 | wpb 678.5 | bsz 2 | num_updates 26263 | lr 2.02781e-05 | gnorm 1.826 | clip 100 | loss_scale 2 | train_wall 12 | gb_free 13.9 | wall 20825\n",
      "2023-04-17 16:39:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:39:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:39:05 | INFO | fairseq.trainer | begin training epoch 822\n",
      "2023-04-17 16:39:05 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:39:16 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:39:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:39:16 | INFO | valid | epoch 822 | valid on 'valid' subset | loss 6.133 | nll_loss 4.447 | ppl 21.81 | wps 7196.1 | wpb 290.8 | bsz 1 | num_updates 26295 | best_loss 3.979\n",
      "2023-04-17 16:39:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 822 @ 26295 updates\n",
      "2023-04-17 16:39:16 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:39:28 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:39:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 822 @ 26295 updates, score 6.133) (writing took 12.068366819061339 seconds)\n",
      "2023-04-17 16:39:29 | INFO | fairseq_cli.train | end of epoch 822 (average epoch stats below)\n",
      "2023-04-17 16:39:29 | INFO | train | epoch 822 | loss 2.338 | nll_loss 0.191 | ppl 1.14 | wps 927.4 | ups 1.37 | wpb 678.5 | bsz 2 | num_updates 26295 | lr 2.0266e-05 | gnorm 2.01 | clip 100 | loss_scale 2 | train_wall 11 | gb_free 13.9 | wall 20849\n",
      "2023-04-17 16:39:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:39:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:39:29 | INFO | fairseq.trainer | begin training epoch 823\n",
      "2023-04-17 16:39:29 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:39:30 | INFO | train_inner | epoch 823:      5 / 32 loss=2.337, nll_loss=0.189, ppl=1.14, wps=914.5, ups=1.33, wpb=685, bsz=2, num_updates=26300, lr=2.02642e-05, gnorm=1.946, clip=100, loss_scale=2, train_wall=36, gb_free=13.9, wall=20851\n",
      "2023-04-17 16:39:40 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:39:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:39:40 | INFO | valid | epoch 823 | valid on 'valid' subset | loss 6.152 | nll_loss 4.467 | ppl 22.11 | wps 7154 | wpb 290.8 | bsz 1 | num_updates 26327 | best_loss 3.979\n",
      "2023-04-17 16:39:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 823 @ 26327 updates\n",
      "2023-04-17 16:39:40 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:39:52 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:39:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 823 @ 26327 updates, score 6.152) (writing took 12.01435377006419 seconds)\n",
      "2023-04-17 16:39:52 | INFO | fairseq_cli.train | end of epoch 823 (average epoch stats below)\n",
      "2023-04-17 16:39:52 | INFO | train | epoch 823 | loss 2.341 | nll_loss 0.193 | ppl 1.14 | wps 929.8 | ups 1.37 | wpb 678.5 | bsz 2 | num_updates 26327 | lr 2.0254e-05 | gnorm 1.951 | clip 100 | loss_scale 2 | train_wall 11 | gb_free 13.9 | wall 20872\n",
      "2023-04-17 16:39:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:39:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:39:52 | INFO | fairseq.trainer | begin training epoch 824\n",
      "2023-04-17 16:39:52 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:40:03 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:40:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:40:03 | INFO | valid | epoch 824 | valid on 'valid' subset | loss 6.202 | nll_loss 4.522 | ppl 22.97 | wps 7012.1 | wpb 290.8 | bsz 1 | num_updates 26359 | best_loss 3.979\n",
      "2023-04-17 16:40:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 824 @ 26359 updates\n",
      "2023-04-17 16:40:03 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:40:15 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:40:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 824 @ 26359 updates, score 6.202) (writing took 12.099095955956727 seconds)\n",
      "2023-04-17 16:40:15 | INFO | fairseq_cli.train | end of epoch 824 (average epoch stats below)\n",
      "2023-04-17 16:40:15 | INFO | train | epoch 824 | loss 2.337 | nll_loss 0.19 | ppl 1.14 | wps 923.1 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 26359 | lr 2.02419e-05 | gnorm 1.946 | clip 100 | loss_scale 2 | train_wall 11 | gb_free 13.9 | wall 20896\n",
      "2023-04-17 16:40:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:40:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:40:15 | INFO | fairseq.trainer | begin training epoch 825\n",
      "2023-04-17 16:40:15 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:40:27 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:40:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:40:27 | INFO | valid | epoch 825 | valid on 'valid' subset | loss 6.148 | nll_loss 4.464 | ppl 22.07 | wps 7217 | wpb 290.8 | bsz 1 | num_updates 26391 | best_loss 3.979\n",
      "2023-04-17 16:40:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 825 @ 26391 updates\n",
      "2023-04-17 16:40:27 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:40:39 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 16:40:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 825 @ 26391 updates, score 6.148) (writing took 11.96198808902409 seconds)\n",
      "2023-04-17 16:40:39 | INFO | fairseq_cli.train | end of epoch 825 (average epoch stats below)\n",
      "2023-04-17 16:40:39 | INFO | train | epoch 825 | loss 2.339 | nll_loss 0.192 | ppl 1.14 | wps 925.2 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 26391 | lr 2.02298e-05 | gnorm 2.109 | clip 100 | loss_scale 2 | train_wall 11 | gb_free 13.9 | wall 20919\n",
      "2023-04-17 16:40:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:40:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:40:39 | INFO | fairseq.trainer | begin training epoch 826\n",
      "2023-04-17 16:40:39 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:40:42 | INFO | train_inner | epoch 826:      9 / 32 loss=2.339, nll_loss=0.192, ppl=1.14, wps=933.1, ups=1.39, wpb=670, bsz=2, num_updates=26400, lr=2.02264e-05, gnorm=1.998, clip=100, loss_scale=2, train_wall=35, gb_free=13.9, wall=20922\n",
      "2023-04-17 16:40:50 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:40:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:40:50 | INFO | valid | epoch 826 | valid on 'valid' subset | loss 6.128 | nll_loss 4.45 | ppl 21.86 | wps 7017.7 | wpb 290.8 | bsz 1 | num_updates 26423 | best_loss 3.979\n",
      "2023-04-17 16:40:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 826 @ 26423 updates\n",
      "2023-04-17 16:40:50 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:41:02 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:41:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 826 @ 26423 updates, score 6.128) (writing took 12.037725338013843 seconds)\n",
      "2023-04-17 16:41:02 | INFO | fairseq_cli.train | end of epoch 826 (average epoch stats below)\n",
      "2023-04-17 16:41:02 | INFO | train | epoch 826 | loss 2.338 | nll_loss 0.19 | ppl 1.14 | wps 924.6 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 26423 | lr 2.02177e-05 | gnorm 1.869 | clip 100 | loss_scale 2 | train_wall 11 | gb_free 13.9 | wall 20943\n",
      "2023-04-17 16:41:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:41:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:41:02 | INFO | fairseq.trainer | begin training epoch 827\n",
      "2023-04-17 16:41:02 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:41:13 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:41:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:41:14 | INFO | valid | epoch 827 | valid on 'valid' subset | loss 6.182 | nll_loss 4.502 | ppl 22.65 | wps 7233 | wpb 290.8 | bsz 1 | num_updates 26455 | best_loss 3.979\n",
      "2023-04-17 16:41:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 827 @ 26455 updates\n",
      "2023-04-17 16:41:14 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:41:26 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:41:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 827 @ 26455 updates, score 6.182) (writing took 12.151692945044488 seconds)\n",
      "2023-04-17 16:41:26 | INFO | fairseq_cli.train | end of epoch 827 (average epoch stats below)\n",
      "2023-04-17 16:41:26 | INFO | train | epoch 827 | loss 2.34 | nll_loss 0.192 | ppl 1.14 | wps 925.3 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 26455 | lr 2.02057e-05 | gnorm 1.946 | clip 100 | loss_scale 2 | train_wall 11 | gb_free 13.9 | wall 20966\n",
      "2023-04-17 16:41:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:41:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:41:26 | INFO | fairseq.trainer | begin training epoch 828\n",
      "2023-04-17 16:41:26 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:41:37 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:41:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:41:37 | INFO | valid | epoch 828 | valid on 'valid' subset | loss 6.247 | nll_loss 4.573 | ppl 23.81 | wps 7190.2 | wpb 290.8 | bsz 1 | num_updates 26487 | best_loss 3.979\n",
      "2023-04-17 16:41:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 828 @ 26487 updates\n",
      "2023-04-17 16:41:37 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:41:49 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:41:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 828 @ 26487 updates, score 6.247) (writing took 12.11332368501462 seconds)\n",
      "2023-04-17 16:41:49 | INFO | fairseq_cli.train | end of epoch 828 (average epoch stats below)\n",
      "2023-04-17 16:41:49 | INFO | train | epoch 828 | loss 2.338 | nll_loss 0.191 | ppl 1.14 | wps 917.9 | ups 1.35 | wpb 678.5 | bsz 2 | num_updates 26487 | lr 2.01936e-05 | gnorm 1.906 | clip 100 | loss_scale 2 | train_wall 11 | gb_free 13.9 | wall 20990\n",
      "2023-04-17 16:41:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:41:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:41:49 | INFO | fairseq.trainer | begin training epoch 829\n",
      "2023-04-17 16:41:49 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:41:54 | INFO | train_inner | epoch 829:     13 / 32 loss=2.338, nll_loss=0.19, ppl=1.14, wps=947.1, ups=1.39, wpb=681.2, bsz=2, num_updates=26500, lr=2.01887e-05, gnorm=1.917, clip=100, loss_scale=2, train_wall=34, gb_free=13.9, wall=20994\n",
      "2023-04-17 16:42:01 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:42:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:42:01 | INFO | valid | epoch 829 | valid on 'valid' subset | loss 6.113 | nll_loss 4.43 | ppl 21.56 | wps 7173.2 | wpb 290.8 | bsz 1 | num_updates 26519 | best_loss 3.979\n",
      "2023-04-17 16:42:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 829 @ 26519 updates\n",
      "2023-04-17 16:42:01 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:42:13 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:42:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 829 @ 26519 updates, score 6.113) (writing took 12.200832750997506 seconds)\n",
      "2023-04-17 16:42:13 | INFO | fairseq_cli.train | end of epoch 829 (average epoch stats below)\n",
      "2023-04-17 16:42:13 | INFO | train | epoch 829 | loss 2.337 | nll_loss 0.189 | ppl 1.14 | wps 923.1 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 26519 | lr 2.01815e-05 | gnorm 1.975 | clip 100 | loss_scale 2 | train_wall 11 | gb_free 13.9 | wall 21013\n",
      "2023-04-17 16:42:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:42:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:42:13 | INFO | fairseq.trainer | begin training epoch 830\n",
      "2023-04-17 16:42:13 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:42:24 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:42:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:42:24 | INFO | valid | epoch 830 | valid on 'valid' subset | loss 6.219 | nll_loss 4.553 | ppl 23.48 | wps 7047.5 | wpb 290.8 | bsz 1 | num_updates 26551 | best_loss 3.979\n",
      "2023-04-17 16:42:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 830 @ 26551 updates\n",
      "2023-04-17 16:42:24 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 16:42:36 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:42:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 830 @ 26551 updates, score 6.219) (writing took 12.14427742396947 seconds)\n",
      "2023-04-17 16:42:37 | INFO | fairseq_cli.train | end of epoch 830 (average epoch stats below)\n",
      "2023-04-17 16:42:37 | INFO | train | epoch 830 | loss 2.338 | nll_loss 0.19 | ppl 1.14 | wps 924.4 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 26551 | lr 2.01694e-05 | gnorm 1.864 | clip 100 | loss_scale 2 | train_wall 11 | gb_free 13.9 | wall 21037\n",
      "2023-04-17 16:42:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:42:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:42:37 | INFO | fairseq.trainer | begin training epoch 831\n",
      "2023-04-17 16:42:37 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:42:48 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:42:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:42:48 | INFO | valid | epoch 831 | valid on 'valid' subset | loss 6.221 | nll_loss 4.555 | ppl 23.51 | wps 7252.1 | wpb 290.8 | bsz 1 | num_updates 26583 | best_loss 3.979\n",
      "2023-04-17 16:42:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 831 @ 26583 updates\n",
      "2023-04-17 16:42:48 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:43:00 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:43:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 831 @ 26583 updates, score 6.221) (writing took 12.207775818998925 seconds)\n",
      "2023-04-17 16:43:00 | INFO | fairseq_cli.train | end of epoch 831 (average epoch stats below)\n",
      "2023-04-17 16:43:00 | INFO | train | epoch 831 | loss 2.337 | nll_loss 0.191 | ppl 1.14 | wps 922.4 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 26583 | lr 2.01574e-05 | gnorm 1.818 | clip 100 | loss_scale 2 | train_wall 11 | gb_free 13.9 | wall 21060\n",
      "2023-04-17 16:43:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:43:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:43:00 | INFO | fairseq.trainer | begin training epoch 832\n",
      "2023-04-17 16:43:00 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:43:06 | INFO | train_inner | epoch 832:     17 / 32 loss=2.337, nll_loss=0.19, ppl=1.14, wps=960.4, ups=1.39, wpb=691.3, bsz=2, num_updates=26600, lr=2.01509e-05, gnorm=1.842, clip=100, loss_scale=2, train_wall=34, gb_free=13.9, wall=21066\n",
      "2023-04-17 16:43:11 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:43:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:43:11 | INFO | valid | epoch 832 | valid on 'valid' subset | loss 6.248 | nll_loss 4.581 | ppl 23.93 | wps 7080.9 | wpb 290.8 | bsz 1 | num_updates 26615 | best_loss 3.979\n",
      "2023-04-17 16:43:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 832 @ 26615 updates\n",
      "2023-04-17 16:43:11 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:43:23 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:43:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 832 @ 26615 updates, score 6.248) (writing took 12.119153940002434 seconds)\n",
      "2023-04-17 16:43:24 | INFO | fairseq_cli.train | end of epoch 832 (average epoch stats below)\n",
      "2023-04-17 16:43:24 | INFO | train | epoch 832 | loss 2.337 | nll_loss 0.19 | ppl 1.14 | wps 924 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 26615 | lr 2.01453e-05 | gnorm 1.77 | clip 100 | loss_scale 2 | train_wall 11 | gb_free 13.9 | wall 21084\n",
      "2023-04-17 16:43:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:43:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:43:24 | INFO | fairseq.trainer | begin training epoch 833\n",
      "2023-04-17 16:43:24 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:43:35 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:43:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:43:35 | INFO | valid | epoch 833 | valid on 'valid' subset | loss 6.252 | nll_loss 4.595 | ppl 24.16 | wps 7233.7 | wpb 290.8 | bsz 1 | num_updates 26647 | best_loss 3.979\n",
      "2023-04-17 16:43:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 833 @ 26647 updates\n",
      "2023-04-17 16:43:35 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:43:47 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:43:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 833 @ 26647 updates, score 6.252) (writing took 12.211582628078759 seconds)\n",
      "2023-04-17 16:43:47 | INFO | fairseq_cli.train | end of epoch 833 (average epoch stats below)\n",
      "2023-04-17 16:43:47 | INFO | train | epoch 833 | loss 2.337 | nll_loss 0.19 | ppl 1.14 | wps 921.2 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 26647 | lr 2.01332e-05 | gnorm 1.894 | clip 100 | loss_scale 2 | train_wall 11 | gb_free 13.9 | wall 21107\n",
      "2023-04-17 16:43:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:43:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:43:47 | INFO | fairseq.trainer | begin training epoch 834\n",
      "2023-04-17 16:43:47 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:43:58 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:43:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:43:58 | INFO | valid | epoch 834 | valid on 'valid' subset | loss 6.161 | nll_loss 4.485 | ppl 22.39 | wps 7146.7 | wpb 290.8 | bsz 1 | num_updates 26679 | best_loss 3.979\n",
      "2023-04-17 16:43:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 834 @ 26679 updates\n",
      "2023-04-17 16:43:58 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:44:10 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:44:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 834 @ 26679 updates, score 6.161) (writing took 12.167582493042573 seconds)\n",
      "2023-04-17 16:44:11 | INFO | fairseq_cli.train | end of epoch 834 (average epoch stats below)\n",
      "2023-04-17 16:44:11 | INFO | train | epoch 834 | loss 2.34 | nll_loss 0.194 | ppl 1.14 | wps 922.1 | ups 1.36 | wpb 678.5 | bsz 2 | num_updates 26679 | lr 2.01211e-05 | gnorm 1.96 | clip 100 | loss_scale 2 | train_wall 11 | gb_free 13.9 | wall 21131\n",
      "2023-04-17 16:44:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:44:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:44:11 | INFO | fairseq.trainer | begin training epoch 835\n",
      "2023-04-17 16:44:11 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:44:18 | INFO | train_inner | epoch 835:     21 / 32 loss=2.339, nll_loss=0.192, ppl=1.14, wps=927, ups=1.39, wpb=668.5, bsz=2, num_updates=26700, lr=2.01132e-05, gnorm=1.947, clip=100, loss_scale=2, train_wall=34, gb_free=13.9, wall=21138\n",
      "2023-04-17 16:44:22 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:44:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:44:22 | INFO | valid | epoch 835 | valid on 'valid' subset | loss 6.135 | nll_loss 4.457 | ppl 21.96 | wps 7102.3 | wpb 290.8 | bsz 1 | num_updates 26711 | best_loss 3.979\n",
      "2023-04-17 16:44:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 835 @ 26711 updates\n",
      "2023-04-17 16:44:22 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 16:44:34 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:44:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 835 @ 26711 updates, score 6.135) (writing took 12.17313062993344 seconds)\n",
      "2023-04-17 16:44:34 | INFO | fairseq_cli.train | end of epoch 835 (average epoch stats below)\n",
      "2023-04-17 16:44:34 | INFO | train | epoch 835 | loss 2.338 | nll_loss 0.19 | ppl 1.14 | wps 918.3 | ups 1.35 | wpb 678.5 | bsz 2 | num_updates 26711 | lr 2.01091e-05 | gnorm 2.125 | clip 100 | loss_scale 2 | train_wall 11 | gb_free 13.9 | wall 21155\n",
      "2023-04-17 16:44:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:44:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:44:34 | INFO | fairseq.trainer | begin training epoch 836\n",
      "2023-04-17 16:44:34 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:44:46 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:44:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:44:46 | INFO | valid | epoch 836 | valid on 'valid' subset | loss 6.12 | nll_loss 4.441 | ppl 21.72 | wps 6903.6 | wpb 290.8 | bsz 1 | num_updates 26743 | best_loss 3.979\n",
      "2023-04-17 16:44:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 836 @ 26743 updates\n",
      "2023-04-17 16:44:46 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:44:58 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:44:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 836 @ 26743 updates, score 6.12) (writing took 12.013467513024807 seconds)\n",
      "2023-04-17 16:44:58 | INFO | fairseq_cli.train | end of epoch 836 (average epoch stats below)\n",
      "2023-04-17 16:44:58 | INFO | train | epoch 836 | loss 2.339 | nll_loss 0.191 | ppl 1.14 | wps 898.6 | ups 1.32 | wpb 678.5 | bsz 2 | num_updates 26743 | lr 2.0097e-05 | gnorm 2.007 | clip 100 | loss_scale 2 | train_wall 12 | gb_free 13.9 | wall 21179\n",
      "2023-04-17 16:44:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:44:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:44:58 | INFO | fairseq.trainer | begin training epoch 837\n",
      "2023-04-17 16:44:58 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:45:10 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:45:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:45:10 | INFO | valid | epoch 837 | valid on 'valid' subset | loss 6.066 | nll_loss 4.374 | ppl 20.73 | wps 7207.4 | wpb 290.8 | bsz 1 | num_updates 26775 | best_loss 3.979\n",
      "2023-04-17 16:45:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 837 @ 26775 updates\n",
      "2023-04-17 16:45:10 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:45:22 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:45:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 837 @ 26775 updates, score 6.066) (writing took 12.356167525984347 seconds)\n",
      "2023-04-17 16:45:22 | INFO | fairseq_cli.train | end of epoch 837 (average epoch stats below)\n",
      "2023-04-17 16:45:22 | INFO | train | epoch 837 | loss 2.338 | nll_loss 0.191 | ppl 1.14 | wps 913.5 | ups 1.35 | wpb 678.5 | bsz 2 | num_updates 26775 | lr 2.00849e-05 | gnorm 1.722 | clip 100 | loss_scale 2 | train_wall 11 | gb_free 13.9 | wall 21202\n",
      "2023-04-17 16:45:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:45:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:45:22 | INFO | fairseq.trainer | begin training epoch 838\n",
      "2023-04-17 16:45:22 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:45:31 | INFO | train_inner | epoch 838:     25 / 32 loss=2.337, nll_loss=0.19, ppl=1.14, wps=931.1, ups=1.37, wpb=679.3, bsz=2, num_updates=26800, lr=2.00755e-05, gnorm=1.818, clip=100, loss_scale=2, train_wall=35, gb_free=13.9, wall=21211\n",
      "2023-04-17 16:45:33 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:45:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:45:34 | INFO | valid | epoch 838 | valid on 'valid' subset | loss 6.203 | nll_loss 4.534 | ppl 23.17 | wps 7142.6 | wpb 290.8 | bsz 1 | num_updates 26807 | best_loss 3.979\n",
      "2023-04-17 16:45:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 838 @ 26807 updates\n",
      "2023-04-17 16:45:34 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:45:46 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n",
      "2023-04-17 16:45:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 838 @ 26807 updates, score 6.203) (writing took 12.958833336015232 seconds)\n",
      "2023-04-17 16:45:47 | INFO | fairseq_cli.train | end of epoch 838 (average epoch stats below)\n",
      "2023-04-17 16:45:47 | INFO | train | epoch 838 | loss 2.334 | nll_loss 0.186 | ppl 1.14 | wps 891.4 | ups 1.31 | wpb 678.5 | bsz 2 | num_updates 26807 | lr 2.00728e-05 | gnorm 1.582 | clip 100 | loss_scale 2 | train_wall 11 | gb_free 13.9 | wall 21227\n",
      "2023-04-17 16:45:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:45:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
      "2023-04-17 16:45:47 | INFO | fairseq.trainer | begin training epoch 839\n",
      "2023-04-17 16:45:47 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-04-17 16:45:58 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-04-17 16:45:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-04-17 16:45:58 | INFO | valid | epoch 839 | valid on 'valid' subset | loss 6.124 | nll_loss 4.45 | ppl 21.86 | wps 7203.5 | wpb 290.8 | bsz 1 | num_updates 26839 | best_loss 3.979\n",
      "2023-04-17 16:45:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 839 @ 26839 updates\n",
      "2023-04-17 16:45:58 | INFO | fairseq.trainer | Saving checkpoint to /home/ichuviliaeva/is_project/checkpoints/checkpoint_last.pt\n"
     ]
    }
   ],
   "source": [
    "!bash summarus/external/bart_scripts/train.sh \\\n",
    "/DATA/ichuviliaeva/project_data/gazeta_bart/checkpoint12.pt \\\n",
    "/DATA/ichuviliaeva/project_data/bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-03T01:25:06.371583Z",
     "iopub.status.busy": "2023-04-03T01:25:06.371227Z",
     "iopub.status.idle": "2023-04-03T01:25:07.403184Z",
     "shell.execute_reply": "2023-04-03T01:25:07.401871Z",
     "shell.execute_reply.started": "2023-04-03T01:25:06.371544Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bart_preprocess.sh  \u001b[0m\u001b[01;34mru_articles_summarization\u001b[0m/             \u001b[01;34msummarus\u001b[0m/\r\n",
      "\u001b[01;34mfairseq\u001b[0m/            ru-articles-summarization-mbart.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ichuviliaeva/is_project/summarus\n"
     ]
    }
   ],
   "source": [
    "%cd summarus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-03T01:25:07.405940Z",
     "iopub.status.busy": "2023-04-03T01:25:07.405170Z",
     "iopub.status.idle": "2023-04-03T01:25:11.491195Z",
     "shell.execute_reply": "2023-04-03T01:25:11.489949Z",
     "shell.execute_reply.started": "2023-04-03T01:25:07.405892Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import MBartTokenizer, MBartForConditionalGeneration\n",
    "\n",
    "\n",
    "def gen_batch(inputs, batch_size):\n",
    "    batch_start = 0\n",
    "    while batch_start < len(inputs):\n",
    "        yield inputs[batch_start: batch_start + batch_size]\n",
    "        batch_start += batch_size\n",
    "\n",
    "\n",
    "def predict(\n",
    "    model_name,\n",
    "    test_file,\n",
    "    predictions_file,\n",
    "    targets_file,\n",
    "    max_source_tokens_count=600,\n",
    "    max_target_tokens_count=160,\n",
    "    use_cuda=True,\n",
    "    batch_size=4\n",
    "):\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    \n",
    "    with open(test_file, \"r\") as r:\n",
    "        for line in r:\n",
    "            record = json.loads(line)\n",
    "            inputs.append(record[\"article\"])\n",
    "            targets.append(record[\"abstract\"].replace(\"\\n\", \" \"))\n",
    "        \n",
    "    tokenizer = MBartTokenizer.from_pretrained(model_name)\n",
    "    device = torch.device(\"cuda:0\") if use_cuda else torch.device(\"cpu\")\n",
    "    model = MBartForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "    # print('model = ', model)\n",
    "    # print('test_file = ', test_file)\n",
    "    # print('predictions_file = ', predictions_file)\n",
    "    # print('targets_file = ', targets_file)\n",
    "    \n",
    "    predictions = []\n",
    "    batches = gen_batch(inputs, batch_size)\n",
    "    for batch in tqdm(batches):\n",
    "        input_ids = tokenizer.prepare_seq2seq_batch(\n",
    "            batch,\n",
    "            src_lang=\"ru_RU\",\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=max_source_tokens_count\n",
    "        )[\"input_ids\"].to(device)\n",
    "        output_ids = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_length=max_target_tokens_count + 2,\n",
    "            no_repeat_ngram_size=3,\n",
    "            num_beams=5,\n",
    "            top_k=0\n",
    "        )\n",
    "        summaries = tokenizer.batch_decode(output_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "        predictions.extend(summaries)\n",
    "        \n",
    "    with open(predictions_file, \"w\") as w:\n",
    "        for p in predictions:\n",
    "            w.write(p.strip() + \"\\n\")\n",
    "    with open(targets_file, \"w\") as w:\n",
    "        for t in targets:\n",
    "            w.write(t.strip() + \"\\n\")\n",
    "        \n",
    "    return predictions, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-03T01:25:11.493705Z",
     "iopub.status.busy": "2023-04-03T01:25:11.493363Z",
     "iopub.status.idle": "2023-04-03T01:34:34.604533Z",
     "shell.execute_reply": "2023-04-03T01:34:34.602750Z",
     "shell.execute_reply.started": "2023-04-03T01:25:11.493671Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a10e4b05b5441139d59d57a78e71a83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ichuviliaeva/miniconda3/envs/is_project/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3712: FutureWarning: \n",
      "`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular\n",
      "`__call__` method to prepare your inputs and targets.\n",
      "\n",
      "Here is a short example:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, text_target=tgt_texts, ...)\n",
      "\n",
      "If you either need to use different keyword arguments for the source and target texts, you should do two calls like\n",
      "this:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, ...)\n",
      "labels = tokenizer(text_target=tgt_texts, ...)\n",
      "model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
      "\n",
      "See the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.\n",
      "For a more complete example, see the implementation of `prepare_seq2seq_batch`.\n",
      "\n",
      "  warnings.warn(formatted_warning, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "predictions, targets = predict(\n",
    "    \"IlyaGusev/mbart_ru_sum_gazeta\", \n",
    "    \"/DATA/ichuviliaeva/project_data/test_data.json\",\n",
    "    \"/DATA/ichuviliaeva/project_data/predictions.txt\", \n",
    "    \"/DATA/ichuviliaeva/project_data/targets.txt\", \n",
    "    batch_size = 16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# работает около часа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ichuviliaeva/is_project\n"
     ]
    }
   ],
   "source": [
    "%cd is_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bart_preprocess.sh  \u001b[0m\u001b[01;34mru_articles_summarization\u001b[0m/             \u001b[01;34msummarus\u001b[0m/\r\n",
      "\u001b[01;34mfairseq\u001b[0m/            ru-articles-summarization-mbart.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd fairseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import MBartTokenizer, MBartForConditionalGeneration\n",
    "\n",
    "def train(\n",
    "    model_name,\n",
    "    train_file,\n",
    "    val_file,\n",
    "    max_source_tokens_count=600,\n",
    "    max_target_tokens_count=160,\n",
    "    use_cuda=True,\n",
    "    batch_size=4\n",
    "):\n",
    "    inputs_train = []\n",
    "    targets_train = []\n",
    "    inputs_val = []\n",
    "    targets_val = []\n",
    "\n",
    "    with open(train_file, \"r\") as r:\n",
    "        for line in tqdm(r):\n",
    "            record = json.loads(line)\n",
    "            inputs_train.append(record[\"article\"])\n",
    "            targets_train.append(record[\"abstract\"].replace(\"\\n\", \" \"))\n",
    "    \n",
    "    with open(val_file, \"r\") as r:\n",
    "        for line in tqdm(r):\n",
    "            record = json.loads(line)\n",
    "            inputs_val.append(record[\"article\"])\n",
    "            targets_val.append(record[\"abstract\"].replace(\"\\n\", \" \"))\n",
    "    \n",
    "    tokenizer = MBartTokenizer.from_pretrained(model_name)\n",
    "    device = torch.device(\"cuda:0\") if use_cuda else torch.device(\"cpu\")\n",
    "    model = MBartForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "    model.train()\n",
    "    \n",
    "    predictions = []\n",
    "    train_batches = gen_batch(inputs_train, batch_size)\n",
    "    \n",
    "    for batch in tqdm(train_batches):\n",
    "        input_ids = tokenizer.prepare_seq2seq_batch(\n",
    "            batch,\n",
    "            src_lang=\"ru_RU\",\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=max_source_tokens_count\n",
    "        )[\"input_ids\"].to(device)\n",
    "        output_ids = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_length=max_target_tokens_count + 2,\n",
    "            no_repeat_ngram_size=3,\n",
    "            num_beams=5,\n",
    "            top_k=0\n",
    "        )\n",
    "        summaries = tokenizer.batch_decode(output_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "        predictions.extend(summaries)\n",
    "        \n",
    "    # можно попытаться вставить лосс и оптимизатор вот сюда\n",
    "    \n",
    "    val_batches = gen_batch(inputs_val, batch_size)\n",
    "    \n",
    "    for batch in tqdm(val_batches):\n",
    "        input_ids = tokenizer.prepare_seq2seq_batch(\n",
    "            batch,\n",
    "            src_lang=\"ru_RU\",\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=max_source_tokens_count\n",
    "        )[\"input_ids\"].to(device)\n",
    "        output_ids = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_length=max_target_tokens_count + 2,\n",
    "            no_repeat_ngram_size=3,\n",
    "            num_beams=5,\n",
    "            top_k=0\n",
    "        )\n",
    "        summaries = tokenizer.batch_decode(output_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "        predictions.extend(summaries)\n",
    "        \n",
    "    return predictions, targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c8ac567b099429985fc9a172fa7ae1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m predictions, targets \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mIlyaGusev/mbart_ru_sum_gazeta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/DATA/ichuviliaeva/project_data/train_data.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/DATA/ichuviliaeva/project_data/val_data.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                            \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 47\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model_name, train_file, val_file, max_source_tokens_count, max_target_tokens_count, use_cuda, batch_size)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(gen_batch(inputs_train, batch_size)):\n\u001b[1;32m     39\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mprepare_seq2seq_batch(\n\u001b[1;32m     40\u001b[0m         batch,\n\u001b[1;32m     41\u001b[0m         src_lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mru_RU\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     45\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mmax_source_tokens_count\n\u001b[1;32m     46\u001b[0m     )[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 47\u001b[0m     output_ids \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_target_tokens_count\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[43mno_repeat_ngram_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     summaries \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(output_ids, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     55\u001b[0m     predictions\u001b[38;5;241m.\u001b[39mextend(summaries)\n",
      "File \u001b[0;32m~/miniconda3/envs/is_project/lib/python3.9/site-packages/torch/autograd/grad_mode.py:28\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m():\n\u001b[0;32m---> 28\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/is_project/lib/python3.9/site-packages/transformers/generation/utils.py:1490\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, **kwargs)\u001b[0m\n\u001b[1;32m   1483\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1484\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1485\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   1486\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1487\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1488\u001b[0m     )\n\u001b[1;32m   1489\u001b[0m     \u001b[38;5;66;03m# 13. run beam search\u001b[39;00m\n\u001b[0;32m-> 1490\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbeam_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1491\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeam_scorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1496\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1497\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1499\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1501\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_beam_sample_gen_mode:\n\u001b[1;32m   1504\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m     logits_warper \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config)\n",
      "File \u001b[0;32m~/miniconda3/envs/is_project/lib/python3.9/site-packages/transformers/generation/utils.py:2749\u001b[0m, in \u001b[0;36mGenerationMixin.beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   2745\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   2747\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[0;32m-> 2749\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2750\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2751\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2752\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2753\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2754\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2756\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2757\u001b[0m     cur_len \u001b[38;5;241m=\u001b[39m cur_len \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/is_project/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/is_project/lib/python3.9/site-packages/transformers/models/mbart/modeling_mbart.py:1351\u001b[0m, in \u001b[0;36mMBartForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m decoder_input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m decoder_inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1349\u001b[0m         decoder_input_ids \u001b[38;5;241m=\u001b[39m shift_tokens_right(labels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id)\n\u001b[0;32m-> 1351\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1361\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1362\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1363\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1364\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1365\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1366\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1367\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1368\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(outputs[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_logits_bias\n\u001b[1;32m   1370\u001b[0m masked_lm_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/is_project/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/is_project/lib/python3.9/site-packages/transformers/models/mbart/modeling_mbart.py:1232\u001b[0m, in \u001b[0;36mMBartModel.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1225\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m BaseModelOutput(\n\u001b[1;32m   1226\u001b[0m         last_hidden_state\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m   1227\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1228\u001b[0m         attentions\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1229\u001b[0m     )\n\u001b[1;32m   1231\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1232\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1233\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1234\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1238\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1240\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1241\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1243\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1244\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1245\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[1;32m   1248\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m decoder_outputs \u001b[38;5;241m+\u001b[39m encoder_outputs\n",
      "File \u001b[0;32m~/miniconda3/envs/is_project/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/is_project/lib/python3.9/site-packages/transformers/models/mbart/modeling_mbart.py:1097\u001b[0m, in \u001b[0;36mMBartDecoder.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1086\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m   1087\u001b[0m         create_custom_forward(decoder_layer),\n\u001b[1;32m   1088\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1094\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1095\u001b[0m     )\n\u001b[1;32m   1096\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1097\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1098\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1104\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m   1105\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1106\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1107\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1110\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/is_project/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/is_project/lib/python3.9/site-packages/transformers/models/mbart/modeling_mbart.py:423\u001b[0m, in \u001b[0;36mMBartDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    421\u001b[0m self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;66;03m# add present self-attn cache to positions 1,2 of present_key_value tuple\u001b[39;00m\n\u001b[0;32m--> 423\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    430\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(hidden_states, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[1;32m    431\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/miniconda3/envs/is_project/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/is_project/lib/python3.9/site-packages/transformers/models/mbart/modeling_mbart.py:208\u001b[0m, in \u001b[0;36mMBartAttention.forward\u001b[0;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;66;03m# reuse k, v, self_attention\u001b[39;00m\n\u001b[1;32m    207\u001b[0m     key_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj(hidden_states), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, bsz)\n\u001b[0;32m--> 208\u001b[0m     value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbsz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     key_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([past_key_value[\u001b[38;5;241m0\u001b[39m], key_states], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    210\u001b[0m     value_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([past_key_value[\u001b[38;5;241m1\u001b[39m], value_states], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/is_project/lib/python3.9/site-packages/transformers/models/mbart/modeling_mbart.py:168\u001b[0m, in \u001b[0;36mMBartAttention._shape\u001b[0;34m(self, tensor, seq_len, bsz)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_shape\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor: torch\u001b[38;5;241m.\u001b[39mTensor, seq_len: \u001b[38;5;28mint\u001b[39m, bsz: \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m--> 168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbsz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "predictions, targets = train(\"IlyaGusev/mbart_ru_sum_gazeta\", \n",
    "                             \"/DATA/ichuviliaeva/project_data/train_data.json\",\n",
    "                             \"/DATA/ichuviliaeva/project_data/val_data.json\"\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
